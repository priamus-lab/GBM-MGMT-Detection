{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../rsnaresnet10/working/create_folds.py --n_folds 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../rsnaresnet10/working/create_folds.py --in_csv_file upenn_train_labels.csv --out_csv_file upenn_train_fold.csv --n_folds 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_T1wCE_0\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 468/468 [00:54<00:00,  8.64it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 117/117 [00:13<00:00,  8.68it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.647, loss=0.719]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.96it/s, batch_loss=0.317, loss=0.681]\n",
      "EPOCH 0/15: Validation average loss: 0.6807579507430395 + AUC SCORE = 0.5521994134897361 + AUC SCORE THRESH 0.5102040816326531 = 0.5488269794721408\n",
      "Saving the model...\n",
      "100%|███████████| 117/117 [01:01<00:00,  1.90it/s, batch_loss=0.633, loss=0.702]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.95it/s, batch_loss=0.137, loss=0.704]\n",
      "EPOCH 1/15: Validation average loss: 0.7043568228681882 + AUC SCORE = 0.5592375366568915 + AUC SCORE THRESH 0.4897959183673469 = 0.5351906158357771\n",
      "Saving the model...\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.88it/s, batch_loss=0.722, loss=0.701]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.99it/s, batch_loss=0.509, loss=0.687]\n",
      "EPOCH 2/15: Validation average loss: 0.6874499261379242 + AUC SCORE = 0.5548387096774194 + AUC SCORE THRESH 0.5510204081632653 = 0.5552785923753666\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.833, loss=0.695]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.94it/s, batch_loss=0.297, loss=0.701]\n",
      "EPOCH 3/15: Validation average loss: 0.7011617511510849 + AUC SCORE = 0.46510263929618767 + AUC SCORE THRESH 0.36734693877551017 = 0.5243401759530792\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.88it/s, batch_loss=0.597, loss=0.689]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.96it/s, batch_loss=0.889, loss=0.704]\n",
      "EPOCH 4/15: Validation average loss: 0.7043877849976222 + AUC SCORE = 0.5149560117302052 + AUC SCORE THRESH 0.6326530612244897 = 0.5502932551319648\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.88it/s, batch_loss=0.691, loss=0.687]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 30/30 [00:07<00:00,  4.02it/s, batch_loss=1.07, loss=0.698]\n",
      "EPOCH 5/15: Validation average loss: 0.6975088556607564 + AUC SCORE = 0.5744868035190616 + AUC SCORE THRESH 0.5918367346938775 = 0.5772727272727273\n",
      "Saving the model...\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.629, loss=0.696]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 30/30 [00:07<00:00,  3.85it/s, batch_loss=0.8, loss=0.692]\n",
      "EPOCH 6/15: Validation average loss: 0.6916147689024608 + AUC SCORE = 0.5486803519061585 + AUC SCORE THRESH 0.5306122448979591 = 0.5548387096774194\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.757, loss=0.686]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.98it/s, batch_loss=0.719, loss=0.688]\n",
      "EPOCH 7/15: Validation average loss: 0.6877283076445262 + AUC SCORE = 0.5759530791788856 + AUC SCORE THRESH 0.5306122448979591 = 0.5683284457478006\n",
      "Saving the model...\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.706, loss=0.693]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 30/30 [00:07<00:00,  3.97it/s, batch_loss=0.781, loss=0.69]\n",
      "EPOCH 8/15: Validation average loss: 0.6897437930107116 + AUC SCORE = 0.5498533724340176 + AUC SCORE THRESH 0.5306122448979591 = 0.5709677419354838\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.744, loss=0.687]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.01it/s, batch_loss=0.662, loss=0.686]\n",
      "EPOCH 9/15: Validation average loss: 0.6855356434981028 + AUC SCORE = 0.5598240469208211 + AUC SCORE THRESH 0.5918367346938775 = 0.5664222873900293\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.88it/s, batch_loss=0.702, loss=0.686]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.95it/s, batch_loss=0.606, loss=0.685]\n",
      "EPOCH 10/15: Validation average loss: 0.6854314804077148 + AUC SCORE = 0.5519061583577712 + AUC SCORE THRESH 0.4897959183673469 = 0.5478005865102639\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.721, loss=0.682]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.92it/s, batch_loss=0.587, loss=0.688]\n",
      "EPOCH 11/15: Validation average loss: 0.6879564424355825 + AUC SCORE = 0.5428152492668622 + AUC SCORE THRESH 0.5714285714285714 = 0.543108504398827\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.663, loss=0.679]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 30/30 [00:07<00:00,  4.02it/s, batch_loss=0.62, loss=0.693]\n",
      "EPOCH 12/15: Validation average loss: 0.6934803545475006 + AUC SCORE = 0.5363636363636364 + AUC SCORE THRESH 0.4897959183673469 = 0.5224340175953078\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.678, loss=0.687]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.03it/s, batch_loss=0.564, loss=0.687]\n",
      "EPOCH 13/15: Validation average loss: 0.6870432992776235 + AUC SCORE = 0.5472140762463343 + AUC SCORE THRESH 0.5510204081632653 = 0.546041055718475\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.765, loss=0.678]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.99it/s, batch_loss=0.573, loss=0.686]\n",
      "EPOCH 14/15: Validation average loss: 0.6861029108365376 + AUC SCORE = 0.5612903225806452 + AUC SCORE THRESH 0.4693877551020408 = 0.5558651026392962\n",
      "0.5759530791788856\n",
      "train_T1wCE_1\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 468/468 [00:53<00:00,  8.74it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 117/117 [00:12<00:00,  9.12it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████| 117/117 [01:03<00:00,  1.84it/s, batch_loss=0.707, loss=0.719]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.15it/s, batch_loss=0.687, loss=0.681]\n",
      "EPOCH 0/15: Validation average loss: 0.6805633485317231 + AUC SCORE = 0.5967741935483871 + AUC SCORE THRESH 0.5510204081632653 = 0.5812316715542521\n",
      "Saving the model...\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.665, loss=0.704]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 30/30 [00:07<00:00,  4.06it/s, batch_loss=0.773, loss=0.69]\n",
      "EPOCH 1/15: Validation average loss: 0.6898946603139241 + AUC SCORE = 0.5897360703812317 + AUC SCORE THRESH 0.4693877551020408 = 0.5841642228739002\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.755, loss=0.701]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.15it/s, batch_loss=0.706, loss=0.687]\n",
      "EPOCH 2/15: Validation average loss: 0.687260091304779 + AUC SCORE = 0.5882697947214076 + AUC SCORE THRESH 0.4897959183673469 = 0.6083577712609971\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.619, loss=0.696]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.10it/s, batch_loss=0.602, loss=0.675]\n",
      "EPOCH 3/15: Validation average loss: 0.6747873097658157 + AUC SCORE = 0.6117302052785923 + AUC SCORE THRESH 0.5714285714285714 = 0.5892961876832844\n",
      "Saving the model...\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.678, loss=0.694]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.09it/s, batch_loss=0.689, loss=0.685]\n",
      "EPOCH 4/15: Validation average loss: 0.6854230304559071 + AUC SCORE = 0.5844574780058651 + AUC SCORE THRESH 0.5306122448979591 = 0.5743401759530792\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.662, loss=0.691]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 30/30 [00:07<00:00,  4.05it/s, batch_loss=0.583, loss=0.68]\n",
      "EPOCH 5/15: Validation average loss: 0.6803188780943553 + AUC SCORE = 0.5926686217008797 + AUC SCORE THRESH 0.5918367346938775 = 0.5853372434017595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.769, loss=0.693]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.13it/s, batch_loss=0.877, loss=0.704]\n",
      "EPOCH 6/15: Validation average loss: 0.7037815848986307 + AUC SCORE = 0.5782991202346041 + AUC SCORE THRESH 0.44897959183673464 = 0.5923753665689149\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.678, loss=0.695]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.01it/s, batch_loss=0.612, loss=0.687]\n",
      "EPOCH 7/15: Validation average loss: 0.6874104936917623 + AUC SCORE = 0.5360703812316716 + AUC SCORE THRESH 0.5510204081632653 = 0.5548387096774194\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.637, loss=0.691]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 30/30 [00:07<00:00,  4.08it/s, batch_loss=0.803, loss=0.69]\n",
      "EPOCH 8/15: Validation average loss: 0.6901079495747884 + AUC SCORE = 0.5442815249266862 + AUC SCORE THRESH 0.4693877551020408 = 0.5860703812316715\n",
      "100%|████████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.651, loss=0.69]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.00it/s, batch_loss=0.663, loss=0.682]\n",
      "EPOCH 9/15: Validation average loss: 0.6817322532335918 + AUC SCORE = 0.5747800586510263 + AUC SCORE THRESH 0.5306122448979591 = 0.6043988269794722\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.731, loss=0.677]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.07it/s, batch_loss=0.699, loss=0.686]\n",
      "EPOCH 10/15: Validation average loss: 0.68574564854304 + AUC SCORE = 0.5671554252199414 + AUC SCORE THRESH 0.5102040816326531 = 0.5759530791788856\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.753, loss=0.684]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.01it/s, batch_loss=0.746, loss=0.689]\n",
      "EPOCH 11/15: Validation average loss: 0.688677970568339 + AUC SCORE = 0.5563049853372434 + AUC SCORE THRESH 0.4693877551020408 = 0.5728739002932551\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.601, loss=0.682]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.96it/s, batch_loss=0.725, loss=0.685]\n",
      "EPOCH 12/15: Validation average loss: 0.6849791904290518 + AUC SCORE = 0.5747800586510264 + AUC SCORE THRESH 0.5102040816326531 = 0.5903225806451613\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.644, loss=0.679]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.09it/s, batch_loss=0.674, loss=0.685]\n",
      "EPOCH 13/15: Validation average loss: 0.6850854575634002 + AUC SCORE = 0.5741935483870968 + AUC SCORE THRESH 0.5306122448979591 = 0.5973607038123167\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.629, loss=0.673]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.05it/s, batch_loss=0.663, loss=0.688]\n",
      "EPOCH 14/15: Validation average loss: 0.6881601572036743 + AUC SCORE = 0.5565982404692082 + AUC SCORE THRESH 0.4897959183673469 = 0.5912023460410558\n",
      "0.6117302052785923\n",
      "train_T1wCE_2\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 468/468 [00:52<00:00,  8.87it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 117/117 [00:12<00:00,  9.64it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████| 117/117 [01:03<00:00,  1.84it/s, batch_loss=0.605, loss=0.721]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.99it/s, batch_loss=0.465, loss=0.782]\n",
      "EPOCH 0/15: Validation average loss: 0.7816911280155182 + AUC SCORE = 0.49619437939110067 + AUC SCORE THRESH 0.4693877551020408 = 0.5686475409836066\n",
      "Saving the model...\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.639, loss=0.691]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 30/30 [00:07<00:00,  4.03it/s, batch_loss=0.72, loss=0.711]\n",
      "EPOCH 1/15: Validation average loss: 0.7108102202415466 + AUC SCORE = 0.5011709601873536 + AUC SCORE THRESH 0.6326530612244897 = 0.5298594847775175\n",
      "Saving the model...\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.763, loss=0.682]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 30/30 [00:07<00:00,  3.99it/s, batch_loss=1.11, loss=0.766]\n",
      "EPOCH 2/15: Validation average loss: 0.766120841105779 + AUC SCORE = 0.48272833723653397 + AUC SCORE THRESH 0.7142857142857142 = 0.5322014051522248\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.746, loss=0.691]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.99it/s, batch_loss=0.452, loss=0.791]\n",
      "EPOCH 3/15: Validation average loss: 0.7910146335760753 + AUC SCORE = 0.49677985948477754 + AUC SCORE THRESH 0.42857142857142855 = 0.5604508196721312\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.686, loss=0.682]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 30/30 [00:07<00:00,  4.02it/s, batch_loss=0.67, loss=0.728]\n",
      "EPOCH 4/15: Validation average loss: 0.7275115231672923 + AUC SCORE = 0.4903395784543325 + AUC SCORE THRESH 0.5714285714285714 = 0.5336651053864169\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.743, loss=0.693]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.01it/s, batch_loss=0.821, loss=0.722]\n",
      "EPOCH 5/15: Validation average loss: 0.7219211101531983 + AUC SCORE = 0.5029274004683841 + AUC SCORE THRESH 0.673469387755102 = 0.5611826697892272\n",
      "Saving the model...\n",
      "100%|█████████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.7, loss=0.698]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.07it/s, batch_loss=0.743, loss=0.701]\n",
      "EPOCH 6/15: Validation average loss: 0.7006970743338267 + AUC SCORE = 0.5081967213114754 + AUC SCORE THRESH 0.5714285714285714 = 0.5300058548009368\n",
      "Saving the model...\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.756, loss=0.688]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.92it/s, batch_loss=0.704, loss=0.712]\n",
      "EPOCH 7/15: Validation average loss: 0.7119942148526509 + AUC SCORE = 0.5278103044496487 + AUC SCORE THRESH 0.5510204081632653 = 0.5463992974238876\n",
      "Saving the model...\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.721, loss=0.684]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.94it/s, batch_loss=0.833, loss=0.715]\n",
      "EPOCH 8/15: Validation average loss: 0.715344292918841 + AUC SCORE = 0.5283957845433255 + AUC SCORE THRESH 0.6326530612244897 = 0.5702576112412178\n",
      "Saving the model...\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.552, loss=0.693]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.98it/s, batch_loss=0.923, loss=0.735]\n",
      "EPOCH 9/15: Validation average loss: 0.7345034837722778 + AUC SCORE = 0.5207845433255269 + AUC SCORE THRESH 0.6530612244897959 = 0.551668618266979\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.661, loss=0.684]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.95it/s, batch_loss=0.556, loss=0.736]\n",
      "EPOCH 10/15: Validation average loss: 0.735768053929011 + AUC SCORE = 0.47833723653395777 + AUC SCORE THRESH 0.5306122448979591 = 0.5589871194379392\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.693, loss=0.675]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.02it/s, batch_loss=0.714, loss=0.725]\n",
      "EPOCH 11/15: Validation average loss: 0.7253533621629079 + AUC SCORE = 0.4733606557377049 + AUC SCORE THRESH 0.5918367346938775 = 0.5194672131147541\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.832, loss=0.679]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.91it/s, batch_loss=0.619, loss=0.728]\n",
      "EPOCH 12/15: Validation average loss: 0.7278670450051625 + AUC SCORE = 0.4563817330210772 + AUC SCORE THRESH 0.6122448979591836 = 0.5209309133489461\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.667, loss=0.676]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.94it/s, batch_loss=0.694, loss=0.758]\n",
      "EPOCH 13/15: Validation average loss: 0.7581385493278503 + AUC SCORE = 0.4757025761124122 + AUC SCORE THRESH 0.6122448979591836 = 0.5351288056206089\n",
      "100%|████████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.628, loss=0.67]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.94it/s, batch_loss=0.813, loss=0.764]\n",
      "EPOCH 14/15: Validation average loss: 0.7640074511369069 + AUC SCORE = 0.4552107728337237 + AUC SCORE THRESH 0.6530612244897959 = 0.5343969555035128\n",
      "0.5283957845433255\n",
      "train_T1wCE_3\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 468/468 [00:51<00:00,  9.01it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 117/117 [00:13<00:00,  8.45it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████| 117/117 [01:03<00:00,  1.84it/s, batch_loss=0.692, loss=0.719]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 30/30 [00:07<00:00,  3.85it/s, batch_loss=1.9, loss=0.791]\n",
      "EPOCH 0/15: Validation average loss: 0.7907988304893175 + AUC SCORE = 0.6513466042154566 + AUC SCORE THRESH 0.32653061224489793 = 0.6678864168618267\n",
      "Saving the model...\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.559, loss=0.704]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.98it/s, batch_loss=0.522, loss=0.695]\n",
      "EPOCH 1/15: Validation average loss: 0.6945286065340042 + AUC SCORE = 0.5843091334894613 + AUC SCORE THRESH 0.6122448979591836 = 0.5993852459016393\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.694, loss=0.699]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.84it/s, batch_loss=0.632, loss=0.681]\n",
      "EPOCH 2/15: Validation average loss: 0.6808882693449656 + AUC SCORE = 0.5802107728337236 + AUC SCORE THRESH 0.5306122448979591 = 0.6233899297423887\n",
      "100%|████████████| 117/117 [01:03<00:00,  1.86it/s, batch_loss=0.71, loss=0.693]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.97it/s, batch_loss=0.651, loss=0.683]\n",
      "EPOCH 3/15: Validation average loss: 0.683222716053327 + AUC SCORE = 0.5989461358313817 + AUC SCORE THRESH 0.5918367346938775 = 0.5895784543325527\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.671, loss=0.697]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 30/30 [00:07<00:00,  3.90it/s, batch_loss=1.01, loss=0.693]\n",
      "EPOCH 4/15: Validation average loss: 0.693060443798701 + AUC SCORE = 0.6214871194379391 + AUC SCORE THRESH 0.44897959183673464 = 0.6500292740046838\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.629, loss=0.694]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:08<00:00,  3.69it/s, batch_loss=0.881, loss=0.685]\n",
      "EPOCH 5/15: Validation average loss: 0.6853659212589264 + AUC SCORE = 0.6173887587822015 + AUC SCORE THRESH 0.4693877551020408 = 0.6210480093676815\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.685, loss=0.692]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.82it/s, batch_loss=0.638, loss=0.692]\n",
      "EPOCH 6/15: Validation average loss: 0.6917058289051056 + AUC SCORE = 0.4973653395784543 + AUC SCORE THRESH 0.4693877551020408 = 0.5475702576112412\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.661, loss=0.688]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.79it/s, batch_loss=0.765, loss=0.692]\n",
      "EPOCH 7/15: Validation average loss: 0.6919353147347768 + AUC SCORE = 0.5579625292740047 + AUC SCORE THRESH 0.5102040816326531 = 0.549473067915691\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.653, loss=0.696]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.96it/s, batch_loss=0.748, loss=0.681]\n",
      "EPOCH 8/15: Validation average loss: 0.6812058965365092 + AUC SCORE = 0.6308548009367682 + AUC SCORE THRESH 0.5306122448979591 = 0.6299765807962528\n",
      "100%|████████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.73, loss=0.692]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 30/30 [00:07<00:00,  3.92it/s, batch_loss=0.705, loss=0.69]\n",
      "EPOCH 9/15: Validation average loss: 0.6898070275783539 + AUC SCORE = 0.5401053864168618 + AUC SCORE THRESH 0.4693877551020408 = 0.5519613583138174\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.709, loss=0.683]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.94it/s, batch_loss=0.692, loss=0.688]\n",
      "EPOCH 10/15: Validation average loss: 0.6877014617125193 + AUC SCORE = 0.5714285714285714 + AUC SCORE THRESH 0.5306122448979591 = 0.5472775175644028\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.629, loss=0.685]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.99it/s, batch_loss=0.602, loss=0.688]\n",
      "EPOCH 11/15: Validation average loss: 0.6884221494197845 + AUC SCORE = 0.5626463700234192 + AUC SCORE THRESH 0.5102040816326531 = 0.5712822014051522\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.729, loss=0.682]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.83it/s, batch_loss=0.727, loss=0.699]\n",
      "EPOCH 12/15: Validation average loss: 0.699192879597346 + AUC SCORE = 0.5234192037470726 + AUC SCORE THRESH 0.5306122448979591 = 0.5360070257611241\n",
      "100%|████████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.546, loss=0.68]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.90it/s, batch_loss=0.591, loss=0.688]\n",
      "EPOCH 13/15: Validation average loss: 0.6880923688411713 + AUC SCORE = 0.563231850117096 + AUC SCORE THRESH 0.5102040816326531 = 0.5891393442622951\n",
      "100%|███████████| 117/117 [01:03<00:00,  1.86it/s, batch_loss=0.665, loss=0.681]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.95it/s, batch_loss=0.569, loss=0.694]\n",
      "EPOCH 14/15: Validation average loss: 0.6941166619459788 + AUC SCORE = 0.5532786885245902 + AUC SCORE THRESH 0.5510204081632653 = 0.5860655737704918\n",
      "0.6513466042154566\n",
      "train_T1wCE_4\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 468/468 [00:51<00:00,  9.07it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 117/117 [00:13<00:00,  8.95it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████| 117/117 [01:03<00:00,  1.83it/s, batch_loss=0.732, loss=0.714]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.98it/s, batch_loss=0.477, loss=0.701]\n",
      "EPOCH 0/15: Validation average loss: 0.701173988978068 + AUC SCORE = 0.5711358313817331 + AUC SCORE THRESH 0.4693877551020408 = 0.5904566744730679\n",
      "Saving the model...\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.761, loss=0.696]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.05it/s, batch_loss=0.695, loss=0.691]\n",
      "EPOCH 1/15: Validation average loss: 0.6909732321898142 + AUC SCORE = 0.5403981264637002 + AUC SCORE THRESH 0.5918367346938775 = 0.5620608899297423\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.732, loss=0.692]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.02it/s, batch_loss=0.469, loss=0.693]\n",
      "EPOCH 2/15: Validation average loss: 0.6929829736550649 + AUC SCORE = 0.5421545667447307 + AUC SCORE THRESH 0.5102040816326531 = 0.5799180327868853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.678, loss=0.69]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.92it/s, batch_loss=0.581, loss=0.695]\n",
      "EPOCH 3/15: Validation average loss: 0.6952042112747828 + AUC SCORE = 0.5187353629976581 + AUC SCORE THRESH 0.4693877551020408 = 0.5450819672131147\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.594, loss=0.699]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 30/30 [00:07<00:00,  3.92it/s, batch_loss=0.56, loss=0.695]\n",
      "EPOCH 4/15: Validation average loss: 0.6953412393728892 + AUC SCORE = 0.5436182669789227 + AUC SCORE THRESH 0.5918367346938775 = 0.5762587822014051\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.659, loss=0.691]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.80it/s, batch_loss=0.575, loss=0.704]\n",
      "EPOCH 5/15: Validation average loss: 0.7035524437824885 + AUC SCORE = 0.5310304449648713 + AUC SCORE THRESH 0.5918367346938775 = 0.573331381733021\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.738, loss=0.681]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.83it/s, batch_loss=0.598, loss=0.707]\n",
      "EPOCH 6/15: Validation average loss: 0.7065611243247986 + AUC SCORE = 0.5216627634660422 + AUC SCORE THRESH 0.6326530612244897 = 0.5620608899297423\n",
      "100%|███████████| 117/117 [01:03<00:00,  1.86it/s, batch_loss=0.714, loss=0.683]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:08<00:00,  3.69it/s, batch_loss=0.884, loss=0.728]\n",
      "EPOCH 7/15: Validation average loss: 0.7278978874286016 + AUC SCORE = 0.5067330210772834 + AUC SCORE THRESH 0.6122448979591836 = 0.5488875878220141\n",
      "100%|████████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.704, loss=0.69]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.10it/s, batch_loss=0.298, loss=0.803]\n",
      "EPOCH 8/15: Validation average loss: 0.8025638371706009 + AUC SCORE = 0.5134660421545667 + AUC SCORE THRESH 0.2857142857142857 = 0.5368852459016393\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.749, loss=0.687]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.91it/s, batch_loss=0.811, loss=0.702]\n",
      "EPOCH 9/15: Validation average loss: 0.7019705275694529 + AUC SCORE = 0.5199063231850116 + AUC SCORE THRESH 0.5102040816326531 = 0.5355679156908666\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.715, loss=0.681]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  4.04it/s, batch_loss=0.451, loss=0.704]\n",
      "EPOCH 10/15: Validation average loss: 0.7036413729190827 + AUC SCORE = 0.5245901639344261 + AUC SCORE THRESH 0.5102040816326531 = 0.5620608899297423\n",
      "100%|███████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.618, loss=0.679]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.97it/s, batch_loss=0.501, loss=0.705]\n",
      "EPOCH 11/15: Validation average loss: 0.7049087305863698 + AUC SCORE = 0.49999999999999994 + AUC SCORE THRESH 0.5510204081632653 = 0.5345433255269321\n",
      "100%|████████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.575, loss=0.68]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 30/30 [00:07<00:00,  3.91it/s, batch_loss=0.53, loss=0.705]\n",
      "EPOCH 12/15: Validation average loss: 0.7052992125352223 + AUC SCORE = 0.4938524590163934 + AUC SCORE THRESH 0.5918367346938775 = 0.5352751756440282\n",
      "100%|████████████| 117/117 [01:02<00:00,  1.87it/s, batch_loss=0.69, loss=0.674]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.92it/s, batch_loss=0.641, loss=0.725]\n",
      "EPOCH 13/15: Validation average loss: 0.7246145655711492 + AUC SCORE = 0.47131147540983603 + AUC SCORE THRESH 0.5714285714285714 = 0.5168325526932085\n",
      "100%|████████████| 117/117 [01:02<00:00,  1.86it/s, batch_loss=0.761, loss=0.68]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 30/30 [00:07<00:00,  3.79it/s, batch_loss=0.384, loss=0.714]\n",
      "EPOCH 14/15: Validation average loss: 0.7142457693815232 + AUC SCORE = 0.4830210772833724 + AUC SCORE THRESH 0.5102040816326531 = 0.5487412177985949\n",
      "0.5711358313817331\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/train.py --fold 0 --type T1wCE --model_name resnet10\n",
    "!python3 ../rsnaresnet10/working/train.py --fold 1 --type T1wCE --model_name resnet10\n",
    "!python3 ../rsnaresnet10/working/train.py --fold 2 --type T1wCE --model_name resnet10\n",
    "!python3 ../rsnaresnet10/working/train.py --fold 3 --type T1wCE --model_name resnet10\n",
    "!python3 ../rsnaresnet10/working/train.py --fold 4 --type T1wCE --model_name resnet10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_T1wCE_0\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 145/145 [00:39<00:00,  3.64it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 40/40 [00:10<00:00,  3.67it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 36/36 [00:22<00:00,  1.63it/s, batch_loss=0.921, loss=0.713]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.79it/s, batch_loss=0.849, loss=0.839]\n",
      "EPOCH 0/15: Validation average loss: 0.8392882823944092 + AUC SCORE = 0.5386666666666666 + AUC SCORE THRESH 0.6326530612244897 = 0.6000000000000001\n",
      "Saving the model...\n",
      "100%|█████████████| 36/36 [00:19<00:00,  1.81it/s, batch_loss=0.742, loss=0.686]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.77it/s, batch_loss=0.805, loss=0.688]\n",
      "EPOCH 1/15: Validation average loss: 0.6877386093139648 + AUC SCORE = 0.5093333333333333 + AUC SCORE THRESH 0.4081632653061224 = 0.6333333333333333\n",
      "100%|█████████████| 36/36 [00:19<00:00,  1.80it/s, batch_loss=0.599, loss=0.664]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.60it/s, batch_loss=0.847, loss=0.688]\n",
      "EPOCH 2/15: Validation average loss: 0.6879557192325592 + AUC SCORE = 0.4426666666666667 + AUC SCORE THRESH 0.36734693877551017 = 0.52\n",
      "100%|██████████████| 36/36 [00:20<00:00,  1.79it/s, batch_loss=0.479, loss=0.66]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.59it/s, batch_loss=1.03, loss=0.711]\n",
      "EPOCH 3/15: Validation average loss: 0.7112545371055603 + AUC SCORE = 0.44799999999999995 + AUC SCORE THRESH 0.4897959183673469 = 0.5066666666666666\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.79it/s, batch_loss=0.791, loss=0.657]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.57it/s, batch_loss=1.28, loss=0.831]\n",
      "EPOCH 4/15: Validation average loss: 0.8308742105960846 + AUC SCORE = 0.42133333333333334 + AUC SCORE THRESH 0.44897959183673464 = 0.5333333333333333\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.79it/s, batch_loss=0.781, loss=0.661]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.78it/s, batch_loss=0.803, loss=0.708]\n",
      "EPOCH 5/15: Validation average loss: 0.7078317642211914 + AUC SCORE = 0.4746666666666666 + AUC SCORE THRESH 0.42857142857142855 = 0.5666666666666667\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.77it/s, batch_loss=0.732, loss=0.636]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.73it/s, batch_loss=1.07, loss=0.802]\n",
      "EPOCH 6/15: Validation average loss: 0.8019157811999321 + AUC SCORE = 0.46933333333333327 + AUC SCORE THRESH 0.18367346938775508 = 0.5333333333333333\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.78it/s, batch_loss=0.541, loss=0.661]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.70it/s, batch_loss=0.99, loss=0.706]\n",
      "EPOCH 7/15: Validation average loss: 0.7063378661870956 + AUC SCORE = 0.45600000000000007 + AUC SCORE THRESH 0.5306122448979591 = 0.54\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.78it/s, batch_loss=0.454, loss=0.616]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.75it/s, batch_loss=1.47, loss=0.745]\n",
      "EPOCH 8/15: Validation average loss: 0.7448680192232132 + AUC SCORE = 0.49066666666666664 + AUC SCORE THRESH 0.36734693877551017 = 0.5533333333333333\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.76it/s, batch_loss=0.472, loss=0.628]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.73it/s, batch_loss=0.967, loss=0.692]\n",
      "EPOCH 9/15: Validation average loss: 0.6920107662677765 + AUC SCORE = 0.5653333333333334 + AUC SCORE THRESH 0.24489795918367346 = 0.6\n",
      "Saving the model...\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.76it/s, batch_loss=0.739, loss=0.609]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.72it/s, batch_loss=0.935, loss=0.71]\n",
      "EPOCH 10/15: Validation average loss: 0.7103274822235107 + AUC SCORE = 0.384 + AUC SCORE THRESH 0.26530612244897955 = 0.5266666666666666\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.76it/s, batch_loss=0.344, loss=0.586]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|████████████████| 10/10 [00:03<00:00,  2.72it/s, batch_loss=1.53, loss=0.8]\n",
      "EPOCH 11/15: Validation average loss: 0.7995644509792328 + AUC SCORE = 0.4613333333333334 + AUC SCORE THRESH 0.36734693877551017 = 0.52\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.77it/s, batch_loss=0.629, loss=0.598]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.77it/s, batch_loss=1.39, loss=0.947]\n",
      "EPOCH 12/15: Validation average loss: 0.9470589637756348 + AUC SCORE = 0.4693333333333334 + AUC SCORE THRESH 0.08163265306122448 = 0.56\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.75it/s, batch_loss=0.475, loss=0.579]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.67it/s, batch_loss=1.39, loss=0.777]\n",
      "EPOCH 13/15: Validation average loss: 0.7770869135856628 + AUC SCORE = 0.416 + AUC SCORE THRESH 0.12244897959183673 = 0.5066666666666666\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.77it/s, batch_loss=0.584, loss=0.522]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.64it/s, batch_loss=1.52, loss=0.807]\n",
      "EPOCH 14/15: Validation average loss: 0.8074419736862183 + AUC SCORE = 0.44533333333333336 + AUC SCORE THRESH 0.2857142857142857 = 0.5733333333333333\n",
      "0.5653333333333334\n",
      "train_T1wCE_1\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 149/149 [00:41<00:00,  3.58it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 36/36 [00:10<00:00,  3.53it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 37/37 [00:22<00:00,  1.63it/s, batch_loss=0.691, loss=0.735]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████████| 9/9 [00:03<00:00,  2.73it/s, batch_loss=1.42, loss=1.01]\n",
      "EPOCH 0/15: Validation average loss: 1.0108487870958116 + AUC SCORE = 0.23411371237458195 + AUC SCORE THRESH 0.0 = 0.5\n",
      "Saving the model...\n",
      "100%|█████████████| 37/37 [00:20<00:00,  1.80it/s, batch_loss=0.361, loss=0.652]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 9/9 [00:03<00:00,  2.62it/s, batch_loss=0.743, loss=0.696]\n",
      "EPOCH 1/15: Validation average loss: 0.6958742075496249 + AUC SCORE = 0.5551839464882943 + AUC SCORE THRESH 0.4693877551020408 = 0.6237458193979933\n",
      "Saving the model...\n",
      "100%|█████████████| 37/37 [00:20<00:00,  1.79it/s, batch_loss=0.525, loss=0.677]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 9/9 [00:03<00:00,  2.60it/s, batch_loss=0.211, loss=0.925]\n",
      "EPOCH 2/15: Validation average loss: 0.9246162374814352 + AUC SCORE = 0.3076923076923077 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|█████████████| 37/37 [00:20<00:00,  1.79it/s, batch_loss=0.434, loss=0.643]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 9/9 [00:03<00:00,  2.61it/s, batch_loss=0.531, loss=0.905]\n",
      "EPOCH 3/15: Validation average loss: 0.9047477841377258 + AUC SCORE = 0.2408026755852843 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|█████████████| 37/37 [00:20<00:00,  1.77it/s, batch_loss=0.796, loss=0.683]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 9/9 [00:03<00:00,  2.64it/s, batch_loss=0.594, loss=0.712]\n",
      "EPOCH 4/15: Validation average loss: 0.7123024662335714 + AUC SCORE = 0.431438127090301 + AUC SCORE THRESH 0.4693877551020408 = 0.5351170568561874\n",
      "100%|█████████████| 37/37 [00:20<00:00,  1.78it/s, batch_loss=0.987, loss=0.653]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 9/9 [00:03<00:00,  2.60it/s, batch_loss=0.517, loss=0.747]\n",
      "EPOCH 5/15: Validation average loss: 0.7468857301606072 + AUC SCORE = 0.32441471571906355 + AUC SCORE THRESH 0.5306122448979591 = 0.5384615384615384\n",
      "100%|██████████████| 37/37 [00:20<00:00,  1.77it/s, batch_loss=0.558, loss=0.64]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 9/9 [00:03<00:00,  2.72it/s, batch_loss=0.574, loss=0.781]\n",
      "EPOCH 6/15: Validation average loss: 0.781433085600535 + AUC SCORE = 0.2976588628762542 + AUC SCORE THRESH 0.5102040816326531 = 0.5066889632107024\n",
      "100%|█████████████| 37/37 [00:21<00:00,  1.76it/s, batch_loss=0.505, loss=0.609]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 9/9 [00:03<00:00,  2.60it/s, batch_loss=0.588, loss=0.796]\n",
      "EPOCH 7/15: Validation average loss: 0.7960739533106486 + AUC SCORE = 0.2642140468227425 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|█████████████| 37/37 [00:20<00:00,  1.77it/s, batch_loss=0.389, loss=0.622]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████████| 9/9 [00:03<00:00,  2.61it/s, batch_loss=3.22, loss=2.61]\n",
      "EPOCH 8/15: Validation average loss: 2.6098545690377555 + AUC SCORE = 0.7123745819397993 + AUC SCORE THRESH 0.9591836734693877 = 0.5919732441471572\n",
      "Saving the model...\n",
      "100%|█████████████| 37/37 [00:20<00:00,  1.77it/s, batch_loss=0.822, loss=0.619]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|███████████████| 9/9 [00:03<00:00,  2.69it/s, batch_loss=0.713, loss=0.838]\n",
      "EPOCH 9/15: Validation average loss: 0.8376650876469083 + AUC SCORE = 0.274247491638796 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|█████████████| 37/37 [00:20<00:00,  1.77it/s, batch_loss=0.704, loss=0.621]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|████████████████| 9/9 [00:03<00:00,  2.61it/s, batch_loss=0.609, loss=0.91]\n",
      "EPOCH 10/15: Validation average loss: 0.9101535611682467 + AUC SCORE = 0.20401337792642144 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|█████████████| 37/37 [00:20<00:00,  1.78it/s, batch_loss=0.572, loss=0.567]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|███████████████| 9/9 [00:03<00:00,  2.68it/s, batch_loss=0.799, loss=0.986]\n",
      "EPOCH 11/15: Validation average loss: 0.9861386153433058 + AUC SCORE = 0.22408026755852845 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|█████████████| 37/37 [00:20<00:00,  1.77it/s, batch_loss=0.525, loss=0.556]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|████████████████| 9/9 [00:03<00:00,  2.58it/s, batch_loss=0.246, loss=1.11]\n",
      "EPOCH 12/15: Validation average loss: 1.1138025091754065 + AUC SCORE = 0.21070234113712377 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|█████████████| 37/37 [00:20<00:00,  1.77it/s, batch_loss=0.402, loss=0.551]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|███████████████████| 9/9 [00:03<00:00,  2.67it/s, batch_loss=0.564, loss=1]\n",
      "EPOCH 13/15: Validation average loss: 1.0013260708914862 + AUC SCORE = 0.2508361204013378 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|█████████████| 37/37 [00:20<00:00,  1.76it/s, batch_loss=0.753, loss=0.528]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|███████████████| 9/9 [00:03<00:00,  2.63it/s, batch_loss=0.406, loss=0.966]\n",
      "EPOCH 14/15: Validation average loss: 0.9656794567902883 + AUC SCORE = 0.3010033444816054 + AUC SCORE THRESH 0.0 = 0.5\n",
      "0.7123745819397993\n",
      "train_T1wCE_2\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 153/153 [00:42<00:00,  3.61it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 32/32 [00:09<00:00,  3.54it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 38/38 [00:23<00:00,  1.64it/s, batch_loss=0.488, loss=0.708]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|████████████████| 8/8 [00:03<00:00,  2.56it/s, batch_loss=0.83, loss=0.723]\n",
      "EPOCH 0/15: Validation average loss: 0.7225031480193138 + AUC SCORE = 0.7316017316017317 + AUC SCORE THRESH 0.5510204081632653 = 0.6948051948051949\n",
      "Saving the model...\n",
      "100%|█████████████| 38/38 [00:21<00:00,  1.78it/s, batch_loss=0.547, loss=0.698]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 8/8 [00:03<00:00,  2.36it/s, batch_loss=0.445, loss=0.767]\n",
      "EPOCH 1/15: Validation average loss: 0.7674286961555481 + AUC SCORE = 0.2683982683982684 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|█████████████| 38/38 [00:21<00:00,  1.80it/s, batch_loss=0.387, loss=0.684]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 8/8 [00:03<00:00,  2.32it/s, batch_loss=0.684, loss=0.809]\n",
      "EPOCH 2/15: Validation average loss: 0.809374675154686 + AUC SCORE = 0.33333333333333337 + AUC SCORE THRESH 0.4081632653061224 = 0.525974025974026\n",
      "100%|██████████████| 38/38 [00:21<00:00,  1.77it/s, batch_loss=0.81, loss=0.677]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████████| 8/8 [00:03<00:00,  2.45it/s, batch_loss=0.5, loss=0.772]\n",
      "EPOCH 3/15: Validation average loss: 0.7715091109275818 + AUC SCORE = 0.22510822510822512 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|██████████████| 38/38 [00:21<00:00,  1.76it/s, batch_loss=0.634, loss=0.65]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|████████████████| 8/8 [00:03<00:00,  2.49it/s, batch_loss=0.46, loss=0.757]\n",
      "EPOCH 4/15: Validation average loss: 0.7569082006812096 + AUC SCORE = 0.2554112554112554 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|█████████████| 38/38 [00:21<00:00,  1.77it/s, batch_loss=0.592, loss=0.644]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 8/8 [00:03<00:00,  2.39it/s, batch_loss=0.427, loss=0.775]\n",
      "EPOCH 5/15: Validation average loss: 0.7747558057308197 + AUC SCORE = 0.3333333333333333 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|█████████████| 38/38 [00:21<00:00,  1.76it/s, batch_loss=0.992, loss=0.642]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 8/8 [00:03<00:00,  2.49it/s, batch_loss=0.498, loss=0.789]\n",
      "EPOCH 6/15: Validation average loss: 0.789206001907587 + AUC SCORE = 0.367965367965368 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|█████████████| 38/38 [00:21<00:00,  1.77it/s, batch_loss=0.678, loss=0.628]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 8/8 [00:03<00:00,  2.43it/s, batch_loss=0.476, loss=0.832]\n",
      "EPOCH 7/15: Validation average loss: 0.8324737884104252 + AUC SCORE = 0.3766233766233766 + AUC SCORE THRESH 0.5918367346938775 = 0.5216450216450216\n",
      "100%|██████████████████| 38/38 [00:21<00:00,  1.77it/s, batch_loss=1, loss=0.64]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 8/8 [00:03<00:00,  2.44it/s, batch_loss=0.608, loss=0.751]\n",
      "EPOCH 8/15: Validation average loss: 0.7513475995510817 + AUC SCORE = 0.5670995670995671 + AUC SCORE THRESH 0.16326530612244897 = 0.6255411255411255\n",
      "100%|█████████████| 38/38 [00:21<00:00,  1.78it/s, batch_loss=0.604, loss=0.658]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|████████████████| 8/8 [00:03<00:00,  2.48it/s, batch_loss=0.539, loss=0.78]\n",
      "EPOCH 9/15: Validation average loss: 0.7800274789333344 + AUC SCORE = 0.34199134199134196 + AUC SCORE THRESH 0.6122448979591836 = 0.5216450216450216\n",
      "100%|██████████████| 38/38 [00:21<00:00,  1.76it/s, batch_loss=0.834, loss=0.62]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|███████████████| 8/8 [00:03<00:00,  2.51it/s, batch_loss=0.415, loss=0.734]\n",
      "EPOCH 10/15: Validation average loss: 0.7339811585843563 + AUC SCORE = 0.4675324675324675 + AUC SCORE THRESH 0.4897959183673469 = 0.5454545454545454\n",
      "100%|█████████████| 38/38 [00:21<00:00,  1.77it/s, batch_loss=0.692, loss=0.605]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|████████████████| 8/8 [00:03<00:00,  2.42it/s, batch_loss=0.43, loss=0.823]\n",
      "EPOCH 11/15: Validation average loss: 0.8234618417918682 + AUC SCORE = 0.40259740259740256 + AUC SCORE THRESH 0.6530612244897959 = 0.5454545454545454\n",
      "100%|█████████████| 38/38 [00:21<00:00,  1.75it/s, batch_loss=0.518, loss=0.586]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|███████████████| 8/8 [00:03<00:00,  2.40it/s, batch_loss=0.457, loss=0.847]\n",
      "EPOCH 12/15: Validation average loss: 0.8468718566000462 + AUC SCORE = 0.4805194805194805 + AUC SCORE THRESH 0.5510204081632653 = 0.5865800865800866\n",
      "100%|█████████████| 38/38 [00:21<00:00,  1.75it/s, batch_loss=0.489, loss=0.597]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████████| 8/8 [00:03<00:00,  2.42it/s, batch_loss=0.44, loss=0.79]\n",
      "EPOCH 13/15: Validation average loss: 0.7897539772093296 + AUC SCORE = 0.5194805194805194 + AUC SCORE THRESH 0.5102040816326531 = 0.5865800865800866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 38/38 [00:21<00:00,  1.75it/s, batch_loss=0.598, loss=0.586]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|███████████████| 8/8 [00:03<00:00,  2.25it/s, batch_loss=0.286, loss=0.852]\n",
      "EPOCH 14/15: Validation average loss: 0.8522918745875359 + AUC SCORE = 0.4848484848484849 + AUC SCORE THRESH 0.4693877551020408 = 0.564935064935065\n",
      "0.7316017316017317\n",
      "train_T1wCE_3\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 147/147 [00:40<00:00,  3.65it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 38/38 [00:10<00:00,  3.69it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 36/36 [00:22<00:00,  1.62it/s, batch_loss=0.642, loss=0.714]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.67it/s, batch_loss=0.754, loss=0.667]\n",
      "EPOCH 0/15: Validation average loss: 0.6671989351511002 + AUC SCORE = 0.4255952380952381 + AUC SCORE THRESH 0.0 = 0.5\n",
      "Saving the model...\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.78it/s, batch_loss=0.786, loss=0.683]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.86it/s, batch_loss=0.695, loss=0.661]\n",
      "EPOCH 1/15: Validation average loss: 0.6609523952007293 + AUC SCORE = 0.6160714285714285 + AUC SCORE THRESH 0.3877551020408163 = 0.6607142857142857\n",
      "Saving the model...\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.77it/s, batch_loss=0.881, loss=0.669]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.84it/s, batch_loss=0.803, loss=0.786]\n",
      "EPOCH 2/15: Validation average loss: 0.7856854498386383 + AUC SCORE = 0.4821428571428571 + AUC SCORE THRESH 0.3877551020408163 = 0.5803571428571429\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.78it/s, batch_loss=0.801, loss=0.659]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.77it/s, batch_loss=0.72, loss=0.715]\n",
      "EPOCH 3/15: Validation average loss: 0.7147512435913086 + AUC SCORE = 0.5386904761904763 + AUC SCORE THRESH 0.32653061224489793 = 0.5714285714285714\n",
      "100%|██████████████| 36/36 [00:20<00:00,  1.77it/s, batch_loss=1.01, loss=0.654]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.85it/s, batch_loss=0.621, loss=0.751]\n",
      "EPOCH 4/15: Validation average loss: 0.750955930352211 + AUC SCORE = 0.550595238095238 + AUC SCORE THRESH 0.24489795918367346 = 0.5654761904761904\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.78it/s, batch_loss=0.664, loss=0.653]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 10/10 [00:03<00:00,  2.86it/s, batch_loss=1.31, loss=1.66]\n",
      "EPOCH 5/15: Validation average loss: 1.6561170041561126 + AUC SCORE = 0.49404761904761907 + AUC SCORE THRESH 0.7346938775510203 = 0.5565476190476191\n",
      "100%|██████████████| 36/36 [00:20<00:00,  1.76it/s, batch_loss=1.27, loss=0.662]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.84it/s, batch_loss=0.783, loss=0.715]\n",
      "EPOCH 6/15: Validation average loss: 0.7148108780384064 + AUC SCORE = 0.5744047619047619 + AUC SCORE THRESH 0.3469387755102041 = 0.601190476190476\n",
      "100%|███████████████| 36/36 [00:20<00:00,  1.76it/s, batch_loss=1.04, loss=0.63]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.81it/s, batch_loss=0.71, loss=0.645]\n",
      "EPOCH 7/15: Validation average loss: 0.6453233897686005 + AUC SCORE = 0.6279761904761905 + AUC SCORE THRESH 0.3877551020408163 = 0.6071428571428572\n",
      "Saving the model...\n",
      "100%|██████████████| 36/36 [00:20<00:00,  1.76it/s, batch_loss=0.68, loss=0.641]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.92it/s, batch_loss=0.768, loss=0.912]\n",
      "EPOCH 8/15: Validation average loss: 0.9118208467960358 + AUC SCORE = 0.443452380952381 + AUC SCORE THRESH 0.6326530612244897 = 0.5386904761904762\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.75it/s, batch_loss=0.368, loss=0.624]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.91it/s, batch_loss=0.726, loss=0.788]\n",
      "EPOCH 9/15: Validation average loss: 0.7883012056350708 + AUC SCORE = 0.5089285714285714 + AUC SCORE THRESH 0.673469387755102 = 0.5654761904761904\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.77it/s, batch_loss=0.584, loss=0.591]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.86it/s, batch_loss=0.738, loss=0.682]\n",
      "EPOCH 10/15: Validation average loss: 0.6820251405239105 + AUC SCORE = 0.5416666666666667 + AUC SCORE THRESH 0.2857142857142857 = 0.5595238095238095\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.76it/s, batch_loss=0.795, loss=0.571]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.72it/s, batch_loss=0.716, loss=0.713]\n",
      "EPOCH 11/15: Validation average loss: 0.7134470164775848 + AUC SCORE = 0.5892857142857143 + AUC SCORE THRESH 0.12244897959183673 = 0.5892857142857143\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.77it/s, batch_loss=0.305, loss=0.524]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.95it/s, batch_loss=0.656, loss=0.667]\n",
      "EPOCH 12/15: Validation average loss: 0.6674204200506211 + AUC SCORE = 0.550595238095238 + AUC SCORE THRESH 0.3469387755102041 = 0.6190476190476191\n",
      "100%|██████████████| 36/36 [00:20<00:00,  1.75it/s, batch_loss=0.29, loss=0.448]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.61it/s, batch_loss=0.991, loss=0.795]\n",
      "EPOCH 13/15: Validation average loss: 0.7945451140403748 + AUC SCORE = 0.4494047619047619 + AUC SCORE THRESH 0.36734693877551017 = 0.5803571428571429\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.74it/s, batch_loss=0.553, loss=0.389]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|███████████████| 10/10 [00:03<00:00,  2.60it/s, batch_loss=2.46, loss=1.81]\n",
      "EPOCH 14/15: Validation average loss: 1.8081124067306518 + AUC SCORE = 0.4285714285714286 + AUC SCORE THRESH 0.9183673469387754 = 0.5476190476190477\n",
      "0.6279761904761905\n",
      "train_T1wCE_4\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 146/146 [00:41<00:00,  3.55it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 39/39 [00:10<00:00,  3.69it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 36/36 [00:22<00:00,  1.62it/s, batch_loss=0.646, loss=0.695]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.74it/s, batch_loss=0.601, loss=0.852]\n",
      "EPOCH 0/15: Validation average loss: 0.8517483949661255 + AUC SCORE = 0.4918478260869565 + AUC SCORE THRESH 0.5510204081632653 = 0.5679347826086957\n",
      "Saving the model...\n",
      "100%|██████████████| 36/36 [00:20<00:00,  1.78it/s, batch_loss=0.695, loss=0.68]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.81it/s, batch_loss=0.803, loss=0.784]\n",
      "EPOCH 1/15: Validation average loss: 0.7842570602893829 + AUC SCORE = 0.3967391304347826 + AUC SCORE THRESH 0.4081632653061224 = 0.53125\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.79it/s, batch_loss=0.621, loss=0.639]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.80it/s, batch_loss=0.59, loss=0.733]\n",
      "EPOCH 2/15: Validation average loss: 0.7330329656600952 + AUC SCORE = 0.49184782608695654 + AUC SCORE THRESH 0.5102040816326531 = 0.5720108695652174\n",
      "Saving the model...\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.76it/s, batch_loss=0.391, loss=0.664]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.66it/s, batch_loss=0.529, loss=0.704]\n",
      "EPOCH 3/15: Validation average loss: 0.7044474005699157 + AUC SCORE = 0.5027173913043479 + AUC SCORE THRESH 0.32653061224489793 = 0.5597826086956521\n",
      "Saving the model...\n",
      "100%|██████████████| 36/36 [00:20<00:00,  1.77it/s, batch_loss=0.63, loss=0.623]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.77it/s, batch_loss=0.545, loss=0.693]\n",
      "EPOCH 4/15: Validation average loss: 0.692909300327301 + AUC SCORE = 0.5271739130434783 + AUC SCORE THRESH 0.36734693877551017 = 0.5896739130434783\n",
      "Saving the model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 36/36 [00:20<00:00,  1.74it/s, batch_loss=0.808, loss=0.641]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.75it/s, batch_loss=0.676, loss=0.709]\n",
      "EPOCH 5/15: Validation average loss: 0.7091762900352478 + AUC SCORE = 0.47282608695652173 + AUC SCORE THRESH 0.4693877551020408 = 0.5720108695652174\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.75it/s, batch_loss=0.459, loss=0.626]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.72it/s, batch_loss=0.637, loss=0.877]\n",
      "EPOCH 6/15: Validation average loss: 0.8773143053054809 + AUC SCORE = 0.4592391304347826 + AUC SCORE THRESH 0.32653061224489793 = 0.53125\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.76it/s, batch_loss=0.642, loss=0.629]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.64it/s, batch_loss=0.577, loss=0.715]\n",
      "EPOCH 7/15: Validation average loss: 0.7154207468032837 + AUC SCORE = 0.43478260869565216 + AUC SCORE THRESH 0.2857142857142857 = 0.5461956521739131\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.77it/s, batch_loss=0.629, loss=0.646]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.69it/s, batch_loss=0.822, loss=0.746]\n",
      "EPOCH 8/15: Validation average loss: 0.7462977528572082 + AUC SCORE = 0.5271739130434783 + AUC SCORE THRESH 0.4693877551020408 = 0.5774456521739131\n",
      "100%|██████████████| 36/36 [00:20<00:00,  1.76it/s, batch_loss=0.706, loss=0.61]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.78it/s, batch_loss=1.08, loss=0.831]\n",
      "EPOCH 9/15: Validation average loss: 0.8308557629585266 + AUC SCORE = 0.43478260869565216 + AUC SCORE THRESH 0.24489795918367346 = 0.5394021739130435\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.76it/s, batch_loss=0.825, loss=0.594]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.75it/s, batch_loss=0.84, loss=0.752]\n",
      "EPOCH 10/15: Validation average loss: 0.7522152304649353 + AUC SCORE = 0.41032608695652173 + AUC SCORE THRESH 0.3877551020408163 = 0.5801630434782609\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.75it/s, batch_loss=0.601, loss=0.588]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 10/10 [00:03<00:00,  2.69it/s, batch_loss=0.428, loss=0.83]\n",
      "EPOCH 11/15: Validation average loss: 0.8303121089935303 + AUC SCORE = 0.5190217391304348 + AUC SCORE THRESH 0.2040816326530612 = 0.5896739130434783\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.75it/s, batch_loss=0.877, loss=0.549]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 10/10 [00:03<00:00,  2.69it/s, batch_loss=0.609, loss=0.831]\n",
      "EPOCH 12/15: Validation average loss: 0.8314446806907654 + AUC SCORE = 0.47010869565217395 + AUC SCORE THRESH 0.2040816326530612 = 0.5896739130434783\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.75it/s, batch_loss=0.699, loss=0.539]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|████████████████| 10/10 [00:03<00:00,  2.69it/s, batch_loss=2.52, loss=2.4]\n",
      "EPOCH 13/15: Validation average loss: 2.399423801898956 + AUC SCORE = 0.4076086956521739 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|█████████████| 36/36 [00:20<00:00,  1.78it/s, batch_loss=0.483, loss=0.577]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|███████████████| 10/10 [00:03<00:00,  2.70it/s, batch_loss=1.4, loss=0.855]\n",
      "EPOCH 14/15: Validation average loss: 0.8546772539615631 + AUC SCORE = 0.3777173913043479 + AUC SCORE THRESH 0.4081632653061224 = 0.5176630434782609\n",
      "0.5271739130434783\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/train.py --fold 0 --type T1wCE --model_name resnet10 --csv_file upenn_train_fold_t1wce.csv\n",
    "!python3 ../rsnaresnet10/working/train.py --fold 1 --type T1wCE --model_name resnet10 --csv_file upenn_train_fold_t1wce.csv\n",
    "!python3 ../rsnaresnet10/working/train.py --fold 2 --type T1wCE --model_name resnet10 --csv_file upenn_train_fold_t1wce.csv\n",
    "!python3 ../rsnaresnet10/working/train.py --fold 3 --type T1wCE --model_name resnet10 --csv_file upenn_train_fold_t1wce.csv\n",
    "!python3 ../rsnaresnet10/working/train.py --fold 4 --type T1wCE --model_name resnet10 --csv_file upenn_train_fold_t1wce.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kfm-admin/GBM-MGMT-Detection/notebooks/../rsnaresnet10/working/train.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df1.append(train_df2, ignore_index=True)\n",
      "/home/kfm-admin/GBM-MGMT-Detection/notebooks/../rsnaresnet10/working/train.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_df = val_df1.append(val_df2, ignore_index=True)\n",
      "train_T1wCE_0\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 613/613 [01:32<00:00,  6.63it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 157/157 [00:24<00:00,  6.54it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.902, loss=0.708]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|████████████| 40/40 [00:10<00:00,  3.76it/s, batch_loss=0.0815, loss=0.675]\n",
      "EPOCH 0/15: Validation average loss: 0.6746988771483302 + AUC SCORE = 0.5824675324675325 + AUC SCORE THRESH 0.5510204081632653 = 0.5642857142857143\n",
      "Saving the model...\n",
      "100%|███████████| 153/153 [01:20<00:00,  1.90it/s, batch_loss=0.713, loss=0.699]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 40/40 [00:10<00:00,  3.76it/s, batch_loss=1.61, loss=0.715]\n",
      "EPOCH 1/15: Validation average loss: 0.7153555750846863 + AUC SCORE = 0.5560064935064936 + AUC SCORE THRESH 0.5510204081632653 = 0.5496753246753247\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.88it/s, batch_loss=0.763, loss=0.693]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 40/40 [00:10<00:00,  3.72it/s, batch_loss=0.444, loss=0.679]\n",
      "EPOCH 2/15: Validation average loss: 0.6792399942874908 + AUC SCORE = 0.5631493506493507 + AUC SCORE THRESH 0.4081632653061224 = 0.5840909090909091\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.88it/s, batch_loss=0.809, loss=0.683]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 40/40 [00:10<00:00,  3.72it/s, batch_loss=0.914, loss=0.702]\n",
      "EPOCH 3/15: Validation average loss: 0.7024692550301552 + AUC SCORE = 0.4928571428571428 + AUC SCORE THRESH 0.42857142857142855 = 0.5223214285714286\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.88it/s, batch_loss=0.747, loss=0.694]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 40/40 [00:10<00:00,  3.73it/s, batch_loss=0.859, loss=0.69]\n",
      "EPOCH 4/15: Validation average loss: 0.6904782854020596 + AUC SCORE = 0.5964285714285715 + AUC SCORE THRESH 0.5510204081632653 = 0.5672077922077923\n",
      "Saving the model...\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.88it/s, batch_loss=0.643, loss=0.693]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 40/40 [00:10<00:00,  3.78it/s, batch_loss=0.479, loss=0.681]\n",
      "EPOCH 5/15: Validation average loss: 0.6814784467220306 + AUC SCORE = 0.5881493506493506 + AUC SCORE THRESH 0.44897959183673464 = 0.5741883116883117\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.88it/s, batch_loss=0.836, loss=0.678]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 40/40 [00:11<00:00,  3.64it/s, batch_loss=1.33, loss=0.709]\n",
      "EPOCH 6/15: Validation average loss: 0.7093467593193055 + AUC SCORE = 0.5832792207792208 + AUC SCORE THRESH 0.5102040816326531 = 0.578814935064935\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.639, loss=0.688]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|████████████████| 40/40 [00:10<00:00,  3.70it/s, batch_loss=1.12, loss=0.7]\n",
      "EPOCH 7/15: Validation average loss: 0.6996312998235226 + AUC SCORE = 0.5737012987012987 + AUC SCORE THRESH 0.4897959183673469 = 0.5587662337662338\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.88it/s, batch_loss=0.589, loss=0.679]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 40/40 [00:10<00:00,  3.68it/s, batch_loss=1.38, loss=0.706]\n",
      "EPOCH 8/15: Validation average loss: 0.7059944257140159 + AUC SCORE = 0.5678571428571428 + AUC SCORE THRESH 0.4693877551020408 = 0.5806818181818181\n",
      "100%|████████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.629, loss=0.68]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 40/40 [00:10<00:00,  3.64it/s, batch_loss=1.34, loss=0.706]\n",
      "EPOCH 9/15: Validation average loss: 0.7064084522426128 + AUC SCORE = 0.5628246753246753 + AUC SCORE THRESH 0.44897959183673464 = 0.5516233766233767\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.725, loss=0.682]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 40/40 [00:10<00:00,  3.67it/s, batch_loss=1.02, loss=0.698]\n",
      "EPOCH 10/15: Validation average loss: 0.6982634454965592 + AUC SCORE = 0.5805194805194807 + AUC SCORE THRESH 0.4081632653061224 = 0.5607954545454545\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.712, loss=0.683]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 40/40 [00:10<00:00,  3.71it/s, batch_loss=0.91, loss=0.697]\n",
      "EPOCH 11/15: Validation average loss: 0.6974470883607864 + AUC SCORE = 0.5771103896103897 + AUC SCORE THRESH 0.5714285714285714 = 0.5594967532467533\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.651, loss=0.678]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 40/40 [00:10<00:00,  3.69it/s, batch_loss=1.09, loss=0.706]\n",
      "EPOCH 12/15: Validation average loss: 0.7060810923576355 + AUC SCORE = 0.5556818181818182 + AUC SCORE THRESH 0.3877551020408163 = 0.5473214285714285\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.633, loss=0.668]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 40/40 [00:10<00:00,  3.71it/s, batch_loss=1.85, loss=0.724]\n",
      "EPOCH 13/15: Validation average loss: 0.7237786665558815 + AUC SCORE = 0.5556818181818183 + AUC SCORE THRESH 0.5510204081632653 = 0.5537337662337661\n",
      "100%|████████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.615, loss=0.68]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 40/40 [00:12<00:00,  3.27it/s, batch_loss=0.62, loss=0.692]\n",
      "EPOCH 14/15: Validation average loss: 0.6923097752034664 + AUC SCORE = 0.5628246753246753 + AUC SCORE THRESH 0.4693877551020408 = 0.5607142857142857\n",
      "0.5964285714285715\n",
      "/home/kfm-admin/GBM-MGMT-Detection/notebooks/../rsnaresnet10/working/train.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df1.append(train_df2, ignore_index=True)\n",
      "/home/kfm-admin/GBM-MGMT-Detection/notebooks/../rsnaresnet10/working/train.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_df = val_df1.append(val_df2, ignore_index=True)\n",
      "train_T1wCE_1\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 617/617 [01:36<00:00,  6.41it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 153/153 [00:23<00:00,  6.56it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████| 154/154 [01:23<00:00,  1.85it/s, batch_loss=0.705, loss=0.714]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.85it/s, batch_loss=0.633, loss=0.692]\n",
      "EPOCH 0/15: Validation average loss: 0.6918288912528601 + AUC SCORE = 0.6025641025641025 + AUC SCORE THRESH 0.5714285714285714 = 0.5797435897435899\n",
      "Saving the model...\n",
      "100%|███████████| 154/154 [01:22<00:00,  1.88it/s, batch_loss=0.618, loss=0.701]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████████| 39/39 [00:09<00:00,  3.94it/s, batch_loss=0.76, loss=0.71]\n",
      "EPOCH 1/15: Validation average loss: 0.7102616841976459 + AUC SCORE = 0.5994871794871794 + AUC SCORE THRESH 0.5918367346938775 = 0.588974358974359\n",
      "100%|███████████| 154/154 [01:22<00:00,  1.87it/s, batch_loss=0.602, loss=0.687]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:09<00:00,  3.91it/s, batch_loss=0.629, loss=0.678]\n",
      "EPOCH 2/15: Validation average loss: 0.6781283280788324 + AUC SCORE = 0.6083760683760683 + AUC SCORE THRESH 0.5510204081632653 = 0.5876923076923077\n",
      "Saving the model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████| 154/154 [01:22<00:00,  1.87it/s, batch_loss=0.615, loss=0.693]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:09<00:00,  3.91it/s, batch_loss=0.446, loss=0.697]\n",
      "EPOCH 3/15: Validation average loss: 0.6970853408177694 + AUC SCORE = 0.5917948717948718 + AUC SCORE THRESH 0.5306122448979591 = 0.5851282051282052\n",
      "100%|███████████| 154/154 [01:22<00:00,  1.87it/s, batch_loss=0.696, loss=0.694]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:09<00:00,  3.91it/s, batch_loss=0.574, loss=0.683]\n",
      "EPOCH 4/15: Validation average loss: 0.6828566927176255 + AUC SCORE = 0.596923076923077 + AUC SCORE THRESH 0.5306122448979591 = 0.5843589743589744\n",
      "100%|████████████| 154/154 [01:22<00:00,  1.87it/s, batch_loss=0.65, loss=0.681]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:09<00:00,  4.22it/s, batch_loss=0.501, loss=0.675]\n",
      "EPOCH 5/15: Validation average loss: 0.6753351260454227 + AUC SCORE = 0.6061538461538462 + AUC SCORE THRESH 0.5510204081632653 = 0.6051282051282052\n",
      "100%|███████████| 154/154 [01:22<00:00,  1.87it/s, batch_loss=0.638, loss=0.685]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:09<00:00,  3.93it/s, batch_loss=0.708, loss=0.692]\n",
      "EPOCH 6/15: Validation average loss: 0.6916734239993951 + AUC SCORE = 0.6047863247863249 + AUC SCORE THRESH 0.6122448979591836 = 0.6002564102564103\n",
      "100%|███████████| 154/154 [01:22<00:00,  1.87it/s, batch_loss=0.714, loss=0.685]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.85it/s, batch_loss=0.595, loss=0.681]\n",
      "EPOCH 7/15: Validation average loss: 0.6806367406478295 + AUC SCORE = 0.5805128205128205 + AUC SCORE THRESH 0.5306122448979591 = 0.5774358974358975\n",
      "100%|███████████| 154/154 [01:22<00:00,  1.87it/s, batch_loss=0.693, loss=0.683]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 39/39 [00:09<00:00,  3.92it/s, batch_loss=0.37, loss=0.686]\n",
      "EPOCH 8/15: Validation average loss: 0.6861396103333204 + AUC SCORE = 0.5914529914529915 + AUC SCORE THRESH 0.5306122448979591 = 0.5851282051282052\n",
      "100%|███████████| 154/154 [01:22<00:00,  1.87it/s, batch_loss=0.593, loss=0.683]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 39/39 [00:09<00:00,  3.95it/s, batch_loss=0.645, loss=0.68]\n",
      "EPOCH 9/15: Validation average loss: 0.6802927431387779 + AUC SCORE = 0.591111111111111 + AUC SCORE THRESH 0.5510204081632653 = 0.5861538461538461\n",
      "100%|███████████| 154/154 [01:22<00:00,  1.87it/s, batch_loss=0.672, loss=0.676]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 39/39 [00:09<00:00,  3.91it/s, batch_loss=0.582, loss=0.686]\n",
      "EPOCH 10/15: Validation average loss: 0.6862343022456536 + AUC SCORE = 0.5482051282051282 + AUC SCORE THRESH 0.5102040816326531 = 0.5351282051282051\n",
      "100%|███████████| 154/154 [01:22<00:00,  1.86it/s, batch_loss=0.673, loss=0.676]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 39/39 [00:10<00:00,  3.85it/s, batch_loss=0.54, loss=0.686]\n",
      "EPOCH 11/15: Validation average loss: 0.6864848526624533 + AUC SCORE = 0.5929914529914531 + AUC SCORE THRESH 0.5306122448979591 = 0.5853846153846154\n",
      "100%|████████████| 154/154 [01:22<00:00,  1.87it/s, batch_loss=0.646, loss=0.67]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.85it/s, batch_loss=0.386, loss=0.692]\n",
      "EPOCH 12/15: Validation average loss: 0.6915691089935792 + AUC SCORE = 0.5960683760683761 + AUC SCORE THRESH 0.5510204081632653 = 0.5866666666666668\n",
      "100%|███████████| 154/154 [01:22<00:00,  1.86it/s, batch_loss=0.633, loss=0.671]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.86it/s, batch_loss=0.443, loss=0.695]\n",
      "EPOCH 13/15: Validation average loss: 0.6946922234999828 + AUC SCORE = 0.5517948717948717 + AUC SCORE THRESH 0.5306122448979591 = 0.5700000000000001\n",
      "100%|███████████| 154/154 [01:22<00:00,  1.87it/s, batch_loss=0.717, loss=0.668]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.75it/s, batch_loss=0.452, loss=0.688]\n",
      "EPOCH 14/15: Validation average loss: 0.6875146367611029 + AUC SCORE = 0.5842735042735042 + AUC SCORE THRESH 0.5306122448979591 = 0.5805128205128205\n",
      "0.6083760683760683\n",
      "/home/kfm-admin/GBM-MGMT-Detection/notebooks/../rsnaresnet10/working/train.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df1.append(train_df2, ignore_index=True)\n",
      "/home/kfm-admin/GBM-MGMT-Detection/notebooks/../rsnaresnet10/working/train.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_df = val_df1.append(val_df2, ignore_index=True)\n",
      "train_T1wCE_2\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 621/621 [01:34<00:00,  6.54it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 149/149 [00:20<00:00,  7.12it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████| 155/155 [01:23<00:00,  1.85it/s, batch_loss=0.674, loss=0.717]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 38/38 [00:09<00:00,  3.92it/s, batch_loss=0.617, loss=0.691]\n",
      "EPOCH 0/15: Validation average loss: 0.6910666823387146 + AUC SCORE = 0.5849567099567099 + AUC SCORE THRESH 0.5306122448979591 = 0.6035353535353535\n",
      "Saving the model...\n",
      "100%|█████████████| 155/155 [01:22<00:00,  1.87it/s, batch_loss=0.884, loss=0.7]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 38/38 [00:09<00:00,  3.93it/s, batch_loss=0.633, loss=0.698]\n",
      "EPOCH 1/15: Validation average loss: 0.6984048438699622 + AUC SCORE = 0.582972582972583 + AUC SCORE THRESH 0.5306122448979591 = 0.5740440115440115\n",
      "100%|███████████| 155/155 [01:22<00:00,  1.87it/s, batch_loss=0.577, loss=0.696]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 38/38 [00:10<00:00,  3.56it/s, batch_loss=0.816, loss=0.696]\n",
      "EPOCH 2/15: Validation average loss: 0.695794760396606 + AUC SCORE = 0.5625901875901875 + AUC SCORE THRESH 0.4081632653061224 = 0.5646645021645021\n",
      "100%|███████████| 155/155 [01:23<00:00,  1.86it/s, batch_loss=0.666, loss=0.692]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 38/38 [00:11<00:00,  3.32it/s, batch_loss=0.917, loss=0.69]\n",
      "EPOCH 3/15: Validation average loss: 0.6900523493164464 + AUC SCORE = 0.5818903318903319 + AUC SCORE THRESH 0.4897959183673469 = 0.558531746031746\n",
      "100%|███████████| 155/155 [01:23<00:00,  1.86it/s, batch_loss=0.672, loss=0.689]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 38/38 [00:09<00:00,  3.87it/s, batch_loss=0.735, loss=0.686]\n",
      "EPOCH 4/15: Validation average loss: 0.6860581777597728 + AUC SCORE = 0.5755772005772005 + AUC SCORE THRESH 0.5102040816326531 = 0.5598845598845599\n",
      "100%|███████████| 155/155 [01:23<00:00,  1.86it/s, batch_loss=0.859, loss=0.683]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 38/38 [00:09<00:00,  3.90it/s, batch_loss=0.695, loss=0.732]\n",
      "EPOCH 5/15: Validation average loss: 0.7322311205299277 + AUC SCORE = 0.5683621933621934 + AUC SCORE THRESH 0.6122448979591836 = 0.6107503607503608\n",
      "100%|████████████| 155/155 [01:23<00:00,  1.86it/s, batch_loss=0.683, loss=0.69]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 38/38 [00:09<00:00,  3.84it/s, batch_loss=0.51, loss=0.713]\n",
      "EPOCH 6/15: Validation average loss: 0.7134599717039811 + AUC SCORE = 0.5539321789321789 + AUC SCORE THRESH 0.5918367346938775 = 0.5946067821067821\n",
      "100%|███████████| 155/155 [01:22<00:00,  1.87it/s, batch_loss=0.799, loss=0.687]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 38/38 [00:09<00:00,  3.92it/s, batch_loss=0.943, loss=0.716]\n",
      "EPOCH 7/15: Validation average loss: 0.7160824356894744 + AUC SCORE = 0.5339105339105339 + AUC SCORE THRESH 0.36734693877551017 = 0.5521284271284271\n",
      "100%|███████████| 155/155 [01:22<00:00,  1.87it/s, batch_loss=0.739, loss=0.682]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 38/38 [00:09<00:00,  3.88it/s, batch_loss=0.855, loss=0.701]\n",
      "EPOCH 8/15: Validation average loss: 0.7008530881844068 + AUC SCORE = 0.5625901875901875 + AUC SCORE THRESH 0.4897959183673469 = 0.5634920634920635\n",
      "100%|███████████| 155/155 [01:23<00:00,  1.86it/s, batch_loss=0.763, loss=0.684]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 38/38 [00:09<00:00,  3.82it/s, batch_loss=0.815, loss=0.692]\n",
      "EPOCH 9/15: Validation average loss: 0.6922523175415239 + AUC SCORE = 0.5689033189033188 + AUC SCORE THRESH 0.4693877551020408 = 0.5733225108225107\n",
      "100%|███████████| 155/155 [01:23<00:00,  1.86it/s, batch_loss=0.747, loss=0.685]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 38/38 [00:09<00:00,  3.90it/s, batch_loss=0.725, loss=0.707]\n",
      "EPOCH 10/15: Validation average loss: 0.7071723467425296 + AUC SCORE = 0.5396825396825397 + AUC SCORE THRESH 0.5102040816326531 = 0.5733225108225107\n",
      "100%|███████████| 155/155 [01:23<00:00,  1.86it/s, batch_loss=0.581, loss=0.676]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 38/38 [00:09<00:00,  3.87it/s, batch_loss=0.691, loss=0.719]\n",
      "EPOCH 11/15: Validation average loss: 0.7189795970916748 + AUC SCORE = 0.525974025974026 + AUC SCORE THRESH 0.5102040816326531 = 0.5728715728715728\n",
      "100%|████████████| 155/155 [01:23<00:00,  1.86it/s, batch_loss=0.636, loss=0.68]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 38/38 [00:09<00:00,  3.90it/s, batch_loss=0.61, loss=0.705]\n",
      "EPOCH 12/15: Validation average loss: 0.7054365936078524 + AUC SCORE = 0.5319264069264069 + AUC SCORE THRESH 0.5510204081632653 = 0.5923520923520924\n",
      "100%|███████████| 155/155 [01:23<00:00,  1.86it/s, batch_loss=0.665, loss=0.675]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 38/38 [00:10<00:00,  3.77it/s, batch_loss=0.597, loss=0.717]\n",
      "EPOCH 13/15: Validation average loss: 0.71676094751609 + AUC SCORE = 0.516955266955267 + AUC SCORE THRESH 0.5510204081632653 = 0.5488816738816739\n",
      "100%|███████████| 155/155 [01:22<00:00,  1.87it/s, batch_loss=0.671, loss=0.675]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 38/38 [00:10<00:00,  3.72it/s, batch_loss=0.658, loss=0.702]\n",
      "EPOCH 14/15: Validation average loss: 0.7016991800383517 + AUC SCORE = 0.5386002886002885 + AUC SCORE THRESH 0.5306122448979591 = 0.6048881673881674\n",
      "0.5849567099567099\n",
      "/home/kfm-admin/GBM-MGMT-Detection/notebooks/../rsnaresnet10/working/train.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df1.append(train_df2, ignore_index=True)\n",
      "/home/kfm-admin/GBM-MGMT-Detection/notebooks/../rsnaresnet10/working/train.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_df = val_df1.append(val_df2, ignore_index=True)\n",
      "train_T1wCE_3\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 615/615 [01:46<00:00,  5.76it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 155/155 [00:27<00:00,  5.57it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.85it/s, batch_loss=0.737, loss=0.714]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.72it/s, batch_loss=0.746, loss=0.669]\n",
      "EPOCH 0/15: Validation average loss: 0.6693804202935635 + AUC SCORE = 0.6371666666666667 + AUC SCORE THRESH 0.5714285714285714 = 0.6320833333333333\n",
      "Saving the model...\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.88it/s, batch_loss=0.709, loss=0.697]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 39/39 [00:10<00:00,  3.82it/s, batch_loss=0.74, loss=0.695]\n",
      "EPOCH 1/15: Validation average loss: 0.6950447070292938 + AUC SCORE = 0.6268333333333334 + AUC SCORE THRESH 0.5714285714285714 = 0.6383333333333334\n",
      "100%|█████████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.82, loss=0.69]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 39/39 [00:11<00:00,  3.39it/s, batch_loss=1.12, loss=0.733]\n",
      "EPOCH 2/15: Validation average loss: 0.7327103171593103 + AUC SCORE = 0.6606666666666666 + AUC SCORE THRESH 0.42857142857142855 = 0.6504166666666666\n",
      "Saving the model...\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.688, loss=0.695]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 39/39 [00:10<00:00,  3.77it/s, batch_loss=0.76, loss=0.685]\n",
      "EPOCH 3/15: Validation average loss: 0.6845306509580368 + AUC SCORE = 0.6435000000000001 + AUC SCORE THRESH 0.5510204081632653 = 0.6112500000000001\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.699, loss=0.691]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.81it/s, batch_loss=0.795, loss=0.677]\n",
      "EPOCH 4/15: Validation average loss: 0.6767909511541709 + AUC SCORE = 0.6100000000000001 + AUC SCORE THRESH 0.4897959183673469 = 0.5983333333333334\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.706, loss=0.684]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 39/39 [00:10<00:00,  3.80it/s, batch_loss=0.76, loss=0.714]\n",
      "EPOCH 5/15: Validation average loss: 0.7141471994228852 + AUC SCORE = 0.48533333333333334 + AUC SCORE THRESH 0.4693877551020408 = 0.5425\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.599, loss=0.683]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.77it/s, batch_loss=0.849, loss=0.673]\n",
      "EPOCH 6/15: Validation average loss: 0.6732620092538687 + AUC SCORE = 0.6278333333333332 + AUC SCORE THRESH 0.5714285714285714 = 0.61625\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.623, loss=0.678]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.85it/s, batch_loss=0.876, loss=0.669]\n",
      "EPOCH 7/15: Validation average loss: 0.6694546830959809 + AUC SCORE = 0.6505000000000001 + AUC SCORE THRESH 0.5102040816326531 = 0.6345833333333334\n",
      "100%|█████████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.62, loss=0.68]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.76it/s, batch_loss=0.836, loss=0.681]\n",
      "EPOCH 8/15: Validation average loss: 0.681303967268039 + AUC SCORE = 0.5891666666666667 + AUC SCORE THRESH 0.44897959183673464 = 0.5966666666666667\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.723, loss=0.684]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.63it/s, batch_loss=0.759, loss=0.736]\n",
      "EPOCH 9/15: Validation average loss: 0.736338659738883 + AUC SCORE = 0.5258333333333333 + AUC SCORE THRESH 0.5102040816326531 = 0.5475000000000001\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.623, loss=0.676]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.66it/s, batch_loss=0.897, loss=0.684]\n",
      "EPOCH 10/15: Validation average loss: 0.6841613910137079 + AUC SCORE = 0.6096666666666667 + AUC SCORE THRESH 0.4897959183673469 = 0.5845833333333333\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.651, loss=0.671]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.62it/s, batch_loss=0.873, loss=0.693]\n",
      "EPOCH 11/15: Validation average loss: 0.6927160146908883 + AUC SCORE = 0.5896666666666667 + AUC SCORE THRESH 0.5306122448979591 = 0.6\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.766, loss=0.672]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.65it/s, batch_loss=0.866, loss=0.672]\n",
      "EPOCH 12/15: Validation average loss: 0.672354132701189 + AUC SCORE = 0.6263333333333333 + AUC SCORE THRESH 0.5306122448979591 = 0.6433333333333333\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.737, loss=0.674]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 39/39 [00:10<00:00,  3.56it/s, batch_loss=0.81, loss=0.695]\n",
      "EPOCH 13/15: Validation average loss: 0.6945775976547828 + AUC SCORE = 0.5801666666666666 + AUC SCORE THRESH 0.5306122448979591 = 0.5970833333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.568, loss=0.665]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 39/39 [00:10<00:00,  3.64it/s, batch_loss=0.77, loss=0.701]\n",
      "EPOCH 14/15: Validation average loss: 0.7014941863524609 + AUC SCORE = 0.5543333333333333 + AUC SCORE THRESH 0.5102040816326531 = 0.5662499999999999\n",
      "0.6606666666666666\n",
      "/home/kfm-admin/GBM-MGMT-Detection/notebooks/../rsnaresnet10/working/train.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_df = train_df1.append(train_df2, ignore_index=True)\n",
      "/home/kfm-admin/GBM-MGMT-Detection/notebooks/../rsnaresnet10/working/train.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_df = val_df1.append(val_df2, ignore_index=True)\n",
      "train_T1wCE_4\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 614/614 [01:32<00:00,  6.63it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 156/156 [00:23<00:00,  6.61it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.85it/s, batch_loss=0.751, loss=0.725]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.77it/s, batch_loss=0.624, loss=0.694]\n",
      "EPOCH 0/15: Validation average loss: 0.6938700798230294 + AUC SCORE = 0.5724149268453066 + AUC SCORE THRESH 0.5510204081632653 = 0.5833470327141212\n",
      "Saving the model...\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.652, loss=0.698]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.72it/s, batch_loss=0.485, loss=0.797]\n",
      "EPOCH 1/15: Validation average loss: 0.7971640263612454 + AUC SCORE = 0.5556468847608088 + AUC SCORE THRESH 0.2857142857142857 = 0.5573730067400953\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.623, loss=0.694]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 39/39 [00:10<00:00,  3.71it/s, batch_loss=0.579, loss=0.73]\n",
      "EPOCH 2/15: Validation average loss: 0.7298625157429621 + AUC SCORE = 0.554167351635706 + AUC SCORE THRESH 0.42857142857142855 = 0.5549071181982574\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.87it/s, batch_loss=0.697, loss=0.686]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.75it/s, batch_loss=0.588, loss=0.727]\n",
      "EPOCH 3/15: Validation average loss: 0.7273703186939924 + AUC SCORE = 0.5327963176064442 + AUC SCORE THRESH 0.42857142857142855 = 0.550715107677133\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.543, loss=0.683]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 39/39 [00:10<00:00,  3.74it/s, batch_loss=0.56, loss=0.716]\n",
      "EPOCH 4/15: Validation average loss: 0.71634002832266 + AUC SCORE = 0.5308236067729738 + AUC SCORE THRESH 0.5714285714285714 = 0.5570442216011837\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.87it/s, batch_loss=0.645, loss=0.679]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.74it/s, batch_loss=0.493, loss=0.742]\n",
      "EPOCH 5/15: Validation average loss: 0.7421457500029833 + AUC SCORE = 0.5291796810784152 + AUC SCORE THRESH 0.3469387755102041 = 0.538878842676311\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.688, loss=0.677]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.75it/s, batch_loss=0.622, loss=0.717]\n",
      "EPOCH 6/15: Validation average loss: 0.7174820410899627 + AUC SCORE = 0.5337826730231794 + AUC SCORE THRESH 0.5918367346938775 = 0.5497287522603979\n",
      "100%|███████████| 153/153 [01:21<00:00,  1.87it/s, batch_loss=0.595, loss=0.679]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 39/39 [00:10<00:00,  3.71it/s, batch_loss=0.664, loss=0.73]\n",
      "EPOCH 7/15: Validation average loss: 0.7300462990235059 + AUC SCORE = 0.5512082853855006 + AUC SCORE THRESH 0.673469387755102 = 0.5677297386158144\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.515, loss=0.683]\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "100%|██████████████| 39/39 [00:13<00:00,  2.80it/s, batch_loss=0.591, loss=0.71]\n",
      "EPOCH 8/15: Validation average loss: 0.7098307961072677 + AUC SCORE = 0.5370705244122965 + AUC SCORE THRESH 0.5510204081632653 = 0.5532631925036988\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.638, loss=0.674]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 39/39 [00:13<00:00,  2.80it/s, batch_loss=0.638, loss=0.726]\n",
      "EPOCH 9/15: Validation average loss: 0.7262392640113831 + AUC SCORE = 0.5352622061482821 + AUC SCORE THRESH 0.5306122448979591 = 0.5448791714614499\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.85it/s, batch_loss=0.752, loss=0.672]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.68it/s, batch_loss=0.556, loss=0.719]\n",
      "EPOCH 10/15: Validation average loss: 0.7194118553247207 + AUC SCORE = 0.505835936215683 + AUC SCORE THRESH 0.5306122448979591 = 0.5442216011836265\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.453, loss=0.669]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 39/39 [00:10<00:00,  3.69it/s, batch_loss=0.604, loss=0.732]\n",
      "EPOCH 11/15: Validation average loss: 0.7315942247708639 + AUC SCORE = 0.522275193161269 + AUC SCORE THRESH 0.5510204081632653 = 0.5493999671214861\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.582, loss=0.667]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 39/39 [00:11<00:00,  3.37it/s, batch_loss=0.521, loss=0.728]\n",
      "EPOCH 12/15: Validation average loss: 0.72785317668548 + AUC SCORE = 0.5119184612855499 + AUC SCORE THRESH 0.6122448979591836 = 0.5433996383363472\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.667, loss=0.666]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|██████████████| 39/39 [00:13<00:00,  2.89it/s, batch_loss=0.49, loss=0.712]\n",
      "EPOCH 13/15: Validation average loss: 0.7123174025462224 + AUC SCORE = 0.4944928489232286 + AUC SCORE THRESH 0.5102040816326531 = 0.5249054742725628\n",
      "100%|███████████| 153/153 [01:22<00:00,  1.86it/s, batch_loss=0.542, loss=0.661]\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "100%|█████████████| 39/39 [00:11<00:00,  3.51it/s, batch_loss=0.539, loss=0.751]\n",
      "EPOCH 14/15: Validation average loss: 0.750649190101868 + AUC SCORE = 0.530659214203518 + AUC SCORE THRESH 0.5102040816326531 = 0.5505507151076771\n",
      "0.5724149268453066\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/train.py --fold 0 --type T1wCE --model_name resnet10 --csv_file all\n",
    "!python3 ../rsnaresnet10/working/train.py --fold 1 --type T1wCE --model_name resnet10 --csv_file all\n",
    "!python3 ../rsnaresnet10/working/train.py --fold 2 --type T1wCE --model_name resnet10 --csv_file all\n",
    "!python3 ../rsnaresnet10/working/train.py --fold 3 --type T1wCE --model_name resnet10 --csv_file all\n",
    "!python3 ../rsnaresnet10/working/train.py --fold 4 --type T1wCE --model_name resnet10 --csv_file all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 117/117 [00:13<00:00,  8.69it/s]\n",
      "Fold 0:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            3           0     1  0.578185\n",
      "1           12           1     1  0.580087\n",
      "2           21           0     1  0.551007\n",
      "3           22           0     1  0.535661\n",
      "4           25           1     1  0.531831\n",
      "5           28           1     1  0.568403\n",
      "6           32           0     1  0.536821\n",
      "7           33           1     1  0.515511\n",
      "8           46           1     1  0.545175\n",
      "9           48           1     1  0.531933\n",
      "10          49           0     1  0.526710\n",
      "11          58           1     1  0.565037\n",
      "12          72           0     0  0.490792\n",
      "13          84           0     1  0.566028\n",
      "14          87           1     1  0.578662\n",
      "15          90           0     1  0.566209\n",
      "16          94           1     1  0.513592\n",
      "17          96           1     1  0.544165\n",
      "18          97           0     1  0.569968\n",
      "19         124           0     1  0.520298\n",
      "20         133           0     0  0.472914\n",
      "21         137           0     1  0.547426\n",
      "22         140           1     1  0.524466\n",
      "23         148           0     0  0.489850\n",
      "24         156           1     1  0.507196\n",
      "25         162           0     0  0.419667\n",
      "26         172           0     0  0.376039\n",
      "27         178           1     0  0.396785\n",
      "28         183           0     0  0.388896\n",
      "29         211           0     0  0.421519\n",
      "30         214           0     0  0.455193\n",
      "31         243           0     0  0.469055\n",
      "32         249           0     0  0.378771\n",
      "33         260           1     0  0.367893\n",
      "34         266           0     1  0.508694\n",
      "35         270           1     0  0.491935\n",
      "36         283           0     0  0.351897\n",
      "37         285           1     0  0.370327\n",
      "38         299           1     0  0.398131\n",
      "39         304           1     0  0.388280\n",
      "40         306           1     0  0.427921\n",
      "41         310           0     0  0.424756\n",
      "42         312           0     1  0.504106\n",
      "43         317           1     1  0.523822\n",
      "44         332           1     0  0.471995\n",
      "45         339           0     0  0.384145\n",
      "46         340           1     0  0.384429\n",
      "47         344           1     0  0.462357\n",
      "48         347           0     0  0.409601\n",
      "49         351           0     1  0.507749\n",
      "50         359           1     1  0.544025\n",
      "51         377           0     0  0.487888\n",
      "52         379           0     0  0.424150\n",
      "53         390           0     0  0.425253\n",
      "54         401           0     0  0.396478\n",
      "55         414           0     0  0.362869\n",
      "56         416           1     0  0.365740\n",
      "57         419           0     0  0.364588\n",
      "58         425           1     0  0.355252\n",
      "59         432           0     0  0.414069\n",
      "60         442           1     1  0.517159\n",
      "61         446           0     0  0.493071\n",
      "62         454           0     1  0.515868\n",
      "63         456           1     0  0.499974\n",
      "64         466           1     1  0.582044\n",
      "65         472           1     0  0.497668\n",
      "66         493           1     1  0.567143\n",
      "67         494           1     1  0.556856\n",
      "68         499           1     0  0.484857\n",
      "69         505           1     1  0.549754\n",
      "70         511           1     1  0.528609\n",
      "71         519           0     1  0.503594\n",
      "72         524           1     1  0.526508\n",
      "73         526           1     0  0.464179\n",
      "74         538           0     1  0.579181\n",
      "75         540           0     1  0.532763\n",
      "76         547           0     1  0.532709\n",
      "77         549           1     1  0.521154\n",
      "78         552           1     1  0.587221\n",
      "79         558           1     1  0.545671\n",
      "80         575           0     0  0.442366\n",
      "81         577           1     0  0.498117\n",
      "82         582           1     0  0.410751\n",
      "83         583           1     1  0.553829\n",
      "84         586           1     1  0.522551\n",
      "85         594           1     0  0.469831\n",
      "86         596           0     0  0.496327\n",
      "87         597           1     0  0.467152\n",
      "88         598           1     0  0.483321\n",
      "89         616           0     1  0.508224\n",
      "90         641           0     1  0.509028\n",
      "91         642           0     1  0.534719\n",
      "92         651           0     1  0.527000\n",
      "93         652           1     0  0.480717\n",
      "94         656           1     0  0.469821\n",
      "95         657           0     1  0.520271\n",
      "96         667           0     1  0.527457\n",
      "97         690           1     1  0.531820\n",
      "98         704           1     1  0.592022\n",
      "99         706           0     1  0.518217\n",
      "100        714           1     1  0.569302\n",
      "101        716           1     1  0.541015\n",
      "102        744           0     0  0.497737\n",
      "103        747           0     1  0.512767\n",
      "104        750           1     1  0.535025\n",
      "105        753           0     1  0.505347\n",
      "106        759           0     0  0.483382\n",
      "107        773           1     1  0.523951\n",
      "108        777           1     1  0.555523\n",
      "109        784           1     0  0.489265\n",
      "110        789           1     1  0.501490\n",
      "111        794           1     0  0.471321\n",
      "112        837           0     0  0.434715\n",
      "113        838           1     0  0.425551\n",
      "114        840           1     0  0.394358\n",
      "115       1005           1     0  0.478806\n",
      "116       1009           0     1  0.512889\n",
      "Prediction AUC: 0.5760\n",
      "Prediction Accuracy: 0.5214\n",
      "Prediction Specificity: 0.4909\n",
      "Prediction Sensitivity: 0.5484\n",
      "Prediction Precision: 0.5484\n",
      "the score of the fold number 0 and the type T1wCE: 0.5759530791788856\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     0  0.575953  0.521368  0.490909  0.548387  0.548387\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 117/117 [00:12<00:00,  9.21it/s]\n",
      "Fold 1:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            0           1     1  0.705193\n",
      "1            5           1     1  0.696102\n",
      "2            8           1     1  0.690189\n",
      "3           17           0     1  0.625012\n",
      "4           19           0     1  0.693636\n",
      "5           20           1     1  0.660859\n",
      "6           24           0     1  0.650625\n",
      "7           36           0     1  0.676110\n",
      "8           62           1     1  0.689332\n",
      "9           66           1     1  0.670309\n",
      "10          85           1     1  0.609311\n",
      "11         102           0     1  0.548104\n",
      "12         111           0     1  0.579378\n",
      "13         116           0     1  0.523847\n",
      "14         117           1     1  0.686440\n",
      "15         130           0     1  0.585025\n",
      "16         132           0     1  0.649624\n",
      "17         136           1     1  0.630596\n",
      "18         144           1     1  0.757913\n",
      "19         146           1     1  0.648230\n",
      "20         154           0     1  0.595352\n",
      "21         165           0     1  0.539412\n",
      "22         166           1     1  0.639406\n",
      "23         167           0     1  0.557531\n",
      "24         188           1     1  0.582589\n",
      "25         192           0     1  0.502969\n",
      "26         206           0     1  0.552332\n",
      "27         222           1     1  0.547839\n",
      "28         234           1     1  0.640538\n",
      "29         236           0     1  0.591030\n",
      "30         237           0     1  0.546239\n",
      "31         245           1     1  0.618824\n",
      "32         247           0     1  0.607007\n",
      "33         251           0     1  0.570475\n",
      "34         258           0     1  0.551249\n",
      "35         259           0     1  0.569325\n",
      "36         261           0     1  0.566790\n",
      "37         269           0     1  0.613123\n",
      "38         275           0     0  0.484402\n",
      "39         281           1     1  0.695873\n",
      "40         286           0     1  0.546195\n",
      "41         289           0     1  0.595716\n",
      "42         293           1     1  0.557692\n",
      "43         294           1     0  0.491688\n",
      "44         311           1     1  0.564373\n",
      "45         331           1     1  0.564926\n",
      "46         338           1     1  0.567843\n",
      "47         349           0     0  0.492705\n",
      "48         366           1     1  0.706786\n",
      "49         370           1     1  0.569736\n",
      "50         371           1     1  0.589368\n",
      "51         373           0     1  0.555146\n",
      "52         388           0     1  0.553740\n",
      "53         389           0     1  0.676028\n",
      "54         402           0     1  0.550105\n",
      "55         403           1     1  0.791057\n",
      "56         417           0     1  0.592909\n",
      "57         418           0     1  0.622877\n",
      "58         426           1     1  0.596293\n",
      "59         430           0     1  0.519297\n",
      "60         451           1     1  0.685112\n",
      "61         469           0     1  0.678305\n",
      "62         470           1     1  0.652356\n",
      "63         479           1     1  0.608246\n",
      "64         491           1     1  0.629453\n",
      "65         516           1     1  0.620035\n",
      "66         520           1     1  0.708210\n",
      "67         532           1     1  0.697711\n",
      "68         548           1     1  0.692879\n",
      "69         551           1     1  0.635600\n",
      "70         557           1     1  0.717302\n",
      "71         565           0     1  0.658312\n",
      "72         572           0     1  0.708836\n",
      "73         576           1     1  0.594811\n",
      "74         590           1     1  0.627217\n",
      "75         599           1     1  0.621766\n",
      "76         601           0     1  0.637167\n",
      "77         604           1     1  0.652904\n",
      "78         608           1     1  0.653428\n",
      "79         621           1     1  0.630687\n",
      "80         645           0     1  0.631100\n",
      "81         646           1     1  0.610414\n",
      "82         649           0     1  0.630619\n",
      "83         650           1     1  0.651515\n",
      "84         654           0     1  0.710179\n",
      "85         655           1     1  0.590933\n",
      "86         658           1     1  0.628937\n",
      "87         661           1     1  0.668300\n",
      "88         674           1     1  0.717563\n",
      "89         676           1     1  0.657848\n",
      "90         683           0     1  0.677309\n",
      "91         684           0     1  0.628898\n",
      "92         687           0     1  0.692091\n",
      "93         698           1     1  0.656290\n",
      "94         705           1     1  0.672312\n",
      "95         724           0     1  0.686613\n",
      "96         729           0     1  0.616134\n",
      "97         732           1     1  0.698702\n",
      "98         734           0     1  0.612755\n",
      "99         736           1     1  0.618325\n",
      "100        742           0     1  0.645917\n",
      "101        756           0     1  0.651244\n",
      "102        757           1     1  0.679729\n",
      "103        758           1     1  0.617679\n",
      "104        764           0     1  0.688799\n",
      "105        791           1     1  0.513992\n",
      "106        800           0     0  0.498208\n",
      "107        801           1     1  0.527790\n",
      "108        803           0     1  0.535371\n",
      "109        804           0     1  0.571729\n",
      "110        811           1     1  0.569581\n",
      "111        823           1     0  0.498143\n",
      "112        824           0     1  0.553421\n",
      "113        834           0     1  0.668059\n",
      "114        839           0     1  0.572784\n",
      "115        999           1     1  0.551333\n",
      "116       1008           1     1  0.595452\n",
      "Prediction AUC: 0.6531\n",
      "Prediction Accuracy: 0.5385\n",
      "Prediction Specificity: 0.0545\n",
      "Prediction Sensitivity: 0.9677\n",
      "Prediction Precision: 0.5357\n",
      "the score of the fold number 1 and the type T1wCE: 0.6530791788856305\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     1  0.653079  0.538462  0.054545  0.967742  0.535714\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 117/117 [00:12<00:00,  9.57it/s]\n",
      "Fold 2:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            2           1     1  0.612273\n",
      "1            6           1     1  0.625027\n",
      "2           11           1     0  0.410529\n",
      "3           30           0     1  0.523880\n",
      "4           59           1     0  0.495441\n",
      "5           61           0     1  0.591775\n",
      "6           64           0     1  0.640633\n",
      "7           77           1     1  0.571820\n",
      "8           78           1     1  0.592387\n",
      "9          105           1     0  0.286855\n",
      "10         107           1     0  0.252142\n",
      "11         108           0     0  0.195402\n",
      "12         110           0     0  0.421910\n",
      "13         112           0     0  0.202504\n",
      "14         121           0     0  0.242352\n",
      "15         122           0     0  0.378502\n",
      "16         123           0     0  0.137043\n",
      "17         138           1     0  0.285427\n",
      "18         142           0     0  0.385459\n",
      "19         157           0     1  0.532795\n",
      "20         159           1     0  0.427841\n",
      "21         169           0     0  0.243308\n",
      "22         185           1     0  0.496752\n",
      "23         186           1     0  0.426277\n",
      "24         187           1     0  0.399098\n",
      "25         194           0     0  0.465503\n",
      "26         199           1     0  0.487547\n",
      "27         201           0     0  0.437838\n",
      "28         218           0     0  0.399903\n",
      "29         227           0     1  0.517204\n",
      "30         230           1     0  0.299416\n",
      "31         239           0     0  0.411000\n",
      "32         241           0     0  0.405766\n",
      "33         262           0     0  0.337927\n",
      "34         271           1     0  0.476993\n",
      "35         282           1     0  0.425622\n",
      "36         284           1     0  0.370500\n",
      "37         291           1     0  0.263473\n",
      "38         298           0     0  0.367846\n",
      "39         300           0     0  0.443178\n",
      "40         303           1     0  0.393524\n",
      "41         308           0     0  0.371508\n",
      "42         321           1     0  0.467822\n",
      "43         327           0     0  0.358310\n",
      "44         334           1     0  0.424382\n",
      "45         336           0     0  0.466299\n",
      "46         346           0     0  0.479908\n",
      "47         350           1     0  0.131463\n",
      "48         352           1     0  0.408979\n",
      "49         356           0     0  0.356779\n",
      "50         367           1     0  0.238305\n",
      "51         369           1     0  0.338361\n",
      "52         383           1     0  0.340047\n",
      "53         386           1     0  0.360876\n",
      "54         392           0     0  0.475850\n",
      "55         397           0     0  0.407780\n",
      "56         400           1     0  0.475999\n",
      "57         406           1     0  0.455680\n",
      "58         412           0     0  0.376902\n",
      "59         423           0     0  0.425312\n",
      "60         433           0     0  0.493626\n",
      "61         436           1     0  0.151505\n",
      "62         441           0     0  0.258770\n",
      "63         449           1     0  0.434205\n",
      "64         452           0     0  0.462175\n",
      "65         459           0     0  0.391080\n",
      "66         468           1     1  0.559632\n",
      "67         480           1     1  0.568026\n",
      "68         481           0     1  0.586761\n",
      "69         483           1     1  0.590105\n",
      "70         495           0     0  0.491063\n",
      "71         510           0     1  0.549844\n",
      "72         525           1     1  0.589148\n",
      "73         528           1     1  0.505221\n",
      "74         530           0     1  0.548143\n",
      "75         533           0     0  0.464581\n",
      "76         537           1     0  0.331806\n",
      "77         542           1     1  0.552776\n",
      "78         544           1     1  0.588921\n",
      "79         556           1     0  0.444252\n",
      "80         567           0     0  0.467109\n",
      "81         569           0     0  0.378087\n",
      "82         571           0     0  0.233570\n",
      "83         588           0     0  0.425203\n",
      "84         589           0     0  0.411046\n",
      "85         593           1     0  0.476882\n",
      "86         602           1     1  0.554334\n",
      "87         610           1     1  0.525633\n",
      "88         613           1     1  0.578538\n",
      "89         618           1     0  0.447785\n",
      "90         623           0     1  0.559836\n",
      "91         624           0     1  0.616930\n",
      "92         679           1     0  0.254238\n",
      "93         688           0     1  0.510821\n",
      "94         691           1     0  0.400703\n",
      "95         692           1     0  0.460314\n",
      "96         694           1     0  0.462419\n",
      "97         703           0     1  0.571518\n",
      "98         709           0     1  0.567503\n",
      "99         731           1     1  0.552659\n",
      "100        735           0     0  0.490697\n",
      "101        751           0     0  0.434748\n",
      "102        765           1     1  0.612421\n",
      "103        772           1     1  0.566490\n",
      "104        781           1     0  0.308066\n",
      "105        782           1     0  0.364184\n",
      "106        787           1     0  0.314047\n",
      "107        795           1     0  0.281636\n",
      "108        796           0     0  0.416541\n",
      "109        799           0     0  0.291699\n",
      "110        802           0     0  0.419962\n",
      "111        816           1     0  0.252386\n",
      "112        818           0     0  0.349447\n",
      "113        998           1     0  0.018931\n",
      "114       1002           1     0  0.272732\n",
      "115       1003           1     0  0.191653\n",
      "116       1010           0     0  0.474224\n",
      "Prediction AUC: 0.4971\n",
      "Prediction Accuracy: 0.5128\n",
      "Prediction Specificity: 0.7679\n",
      "Prediction Sensitivity: 0.2787\n",
      "Prediction Precision: 0.5667\n",
      "the score of the fold number 2 and the type T1wCE: 0.497072599531616\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     2  0.497073  0.512821  0.767857  0.278689  0.566667\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 117/117 [00:13<00:00,  8.46it/s]\n",
      "Fold 3:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0           26           1     0  0.393890\n",
      "1           31           1     1  0.501990\n",
      "2           43           1     1  0.514937\n",
      "3           45           0     0  0.382004\n",
      "4           54           1     0  0.488947\n",
      "5           56           1     0  0.440049\n",
      "6           63           1     0  0.492415\n",
      "7           68           1     0  0.289394\n",
      "8           70           1     0  0.351083\n",
      "9           71           1     0  0.424353\n",
      "10          74           1     0  0.482404\n",
      "11          88           0     0  0.413402\n",
      "12          89           1     1  0.501703\n",
      "13          99           0     0  0.443553\n",
      "14         106           1     0  0.070632\n",
      "15         109           1     0  0.157875\n",
      "16         113           0     0  0.187670\n",
      "17         139           1     0  0.153971\n",
      "18         147           0     0  0.202236\n",
      "19         149           0     0  0.182871\n",
      "20         150           0     0  0.274508\n",
      "21         151           0     0  0.087255\n",
      "22         155           1     0  0.243585\n",
      "23         170           0     0  0.186169\n",
      "24         171           1     0  0.240088\n",
      "25         176           0     0  0.430867\n",
      "26         191           0     0  0.260901\n",
      "27         196           1     0  0.214065\n",
      "28         203           1     0  0.300390\n",
      "29         209           0     0  0.294865\n",
      "30         216           0     0  0.162695\n",
      "31         221           0     0  0.258887\n",
      "32         228           0     0  0.203701\n",
      "33         231           0     0  0.325070\n",
      "34         235           1     0  0.367529\n",
      "35         238           0     0  0.217962\n",
      "36         240           1     0  0.353144\n",
      "37         242           0     0  0.188305\n",
      "38         253           1     0  0.281420\n",
      "39         263           1     0  0.177420\n",
      "40         267           0     0  0.296569\n",
      "41         288           0     0  0.085919\n",
      "42         290           0     0  0.263248\n",
      "43         297           0     0  0.216263\n",
      "44         301           0     0  0.221859\n",
      "45         309           0     0  0.264636\n",
      "46         313           1     0  0.289855\n",
      "47         316           0     0  0.240873\n",
      "48         320           0     0  0.279295\n",
      "49         324           0     0  0.326834\n",
      "50         328           1     0  0.171184\n",
      "51         343           0     0  0.296677\n",
      "52         348           0     0  0.251380\n",
      "53         376           0     0  0.029547\n",
      "54         387           0     0  0.359074\n",
      "55         395           0     0  0.279804\n",
      "56         404           1     0  0.340193\n",
      "57         409           1     0  0.291493\n",
      "58         431           1     0  0.297192\n",
      "59         440           1     0  0.336253\n",
      "60         444           0     0  0.224943\n",
      "61         455           0     0  0.218980\n",
      "62         457           1     0  0.096120\n",
      "63         478           1     0  0.327717\n",
      "64         496           0     0  0.296914\n",
      "65         498           0     0  0.423320\n",
      "66         501           1     0  0.425213\n",
      "67         504           1     0  0.405686\n",
      "68         512           0     0  0.488946\n",
      "69         513           1     0  0.473728\n",
      "70         529           1     0  0.350275\n",
      "71         539           1     0  0.450705\n",
      "72         543           1     0  0.389523\n",
      "73         545           0     0  0.498074\n",
      "74         555           0     0  0.362949\n",
      "75         559           1     0  0.384602\n",
      "76         574           0     0  0.295691\n",
      "77         578           0     0  0.140789\n",
      "78         581           0     0  0.236787\n",
      "79         587           0     0  0.211106\n",
      "80         606           1     0  0.287392\n",
      "81         611           1     0  0.480094\n",
      "82         620           0     0  0.394851\n",
      "83         625           1     0  0.350247\n",
      "84         626           1     0  0.455442\n",
      "85         631           1     0  0.367123\n",
      "86         640           1     0  0.471566\n",
      "87         675           1     0  0.459438\n",
      "88         677           1     0  0.245407\n",
      "89         680           1     0  0.359881\n",
      "90         685           0     0  0.410071\n",
      "91         686           0     0  0.497782\n",
      "92         693           1     0  0.429868\n",
      "93         707           1     0  0.335114\n",
      "94         708           1     0  0.492122\n",
      "95         715           1     0  0.468231\n",
      "96         718           1     0  0.292453\n",
      "97         723           0     0  0.435255\n",
      "98         728           0     0  0.477193\n",
      "99         730           0     1  0.512430\n",
      "100        737           1     0  0.379514\n",
      "101        746           1     0  0.449077\n",
      "102        760           1     0  0.326680\n",
      "103        767           0     0  0.350598\n",
      "104        768           1     0  0.366450\n",
      "105        775           1     0  0.336957\n",
      "106        780           0     0  0.167249\n",
      "107        788           0     0  0.170793\n",
      "108        793           1     0  0.132330\n",
      "109        805           0     0  0.172405\n",
      "110        806           0     0  0.311101\n",
      "111        809           0     0  0.196249\n",
      "112        819           1     0  0.203348\n",
      "113       1000           1     0  0.349293\n",
      "114       1001           1     0  0.247117\n",
      "115       1004           0     0  0.310807\n",
      "116       1007           1     0  0.150174\n",
      "Prediction AUC: 0.6513\n",
      "Prediction Accuracy: 0.4957\n",
      "Prediction Specificity: 0.9821\n",
      "Prediction Sensitivity: 0.0492\n",
      "Prediction Precision: 0.7500\n",
      "the score of the fold number 3 and the type T1wCE: 0.6513466042154566\n",
      "  model       AUC       acc      spec     sens  prec\n",
      "0     3  0.651347  0.495726  0.982143  0.04918  0.75\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 117/117 [00:13<00:00,  8.78it/s]\n",
      "Fold 4:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            9           0     1  0.652635\n",
      "1           14           1     1  0.640172\n",
      "2           18           0     1  0.644432\n",
      "3           35           1     1  0.655470\n",
      "4           44           0     1  0.653089\n",
      "5           52           1     1  0.629109\n",
      "6           53           0     1  0.834944\n",
      "7           60           1     1  0.585243\n",
      "8           81           0     1  0.637737\n",
      "9           95           0     1  0.774231\n",
      "10          98           1     1  0.806121\n",
      "11         100           1     0  0.413707\n",
      "12         104           0     0  0.481362\n",
      "13         120           1     1  0.540661\n",
      "14         128           1     0  0.386363\n",
      "15         134           1     0  0.371252\n",
      "16         143           1     1  0.542263\n",
      "17         158           0     1  0.563857\n",
      "18         160           1     1  0.529754\n",
      "19         177           1     1  0.533624\n",
      "20         184           0     1  0.516942\n",
      "21         193           0     0  0.397154\n",
      "22         195           0     1  0.564775\n",
      "23         197           1     1  0.537092\n",
      "24         204           1     1  0.534969\n",
      "25         210           1     1  0.557479\n",
      "26         212           1     1  0.521028\n",
      "27         217           0     0  0.435965\n",
      "28         219           0     1  0.558838\n",
      "29         220           1     1  0.530529\n",
      "30         233           1     1  0.525778\n",
      "31         246           1     1  0.542534\n",
      "32         250           1     1  0.596607\n",
      "33         254           1     1  0.537358\n",
      "34         273           1     1  0.532813\n",
      "35         274           0     1  0.564082\n",
      "36         280           0     1  0.540529\n",
      "37         296           1     1  0.560699\n",
      "38         305           1     1  0.606260\n",
      "39         314           0     1  0.506660\n",
      "40         318           0     1  0.521649\n",
      "41         322           1     1  0.555115\n",
      "42         325           0     0  0.494430\n",
      "43         329           1     1  0.539431\n",
      "44         341           0     1  0.532051\n",
      "45         353           0     1  0.524047\n",
      "46         360           1     1  0.573665\n",
      "47         364           1     1  0.562308\n",
      "48         378           0     1  0.550577\n",
      "49         380           0     1  0.555968\n",
      "50         382           0     1  0.538578\n",
      "51         391           0     0  0.496715\n",
      "52         399           0     1  0.552963\n",
      "53         405           0     0  0.465741\n",
      "54         407           0     1  0.508612\n",
      "55         408           1     1  0.562665\n",
      "56         410           0     1  0.566760\n",
      "57         413           1     1  0.575333\n",
      "58         421           0     1  0.549755\n",
      "59         429           1     0  0.333508\n",
      "60         443           1     1  0.615857\n",
      "61         445           0     1  0.569181\n",
      "62         464           0     1  0.601265\n",
      "63         477           0     1  0.626092\n",
      "64         485           1     1  0.653530\n",
      "65         488           1     1  0.624358\n",
      "66         500           1     1  0.648048\n",
      "67         502           1     1  0.615176\n",
      "68         506           1     1  0.637254\n",
      "69         507           0     1  0.666005\n",
      "70         514           0     1  0.658569\n",
      "71         517           1     1  0.624014\n",
      "72         518           0     1  0.650951\n",
      "73         523           1     1  0.652765\n",
      "74         550           1     1  0.662881\n",
      "75         554           1     1  0.821847\n",
      "76         561           1     1  0.623254\n",
      "77         563           0     1  0.608080\n",
      "78         564           1     1  0.654906\n",
      "79         568           0     1  0.514649\n",
      "80         570           1     1  0.522952\n",
      "81         579           1     1  0.633200\n",
      "82         584           1     1  0.510001\n",
      "83         591           0     1  0.635520\n",
      "84         605           0     1  0.618630\n",
      "85         607           1     1  0.610458\n",
      "86         612           1     1  0.651905\n",
      "87         615           1     1  0.677073\n",
      "88         619           0     1  0.613073\n",
      "89         622           1     1  0.657247\n",
      "90         628           1     1  0.651368\n",
      "91         630           0     1  0.682798\n",
      "92         636           0     1  0.613189\n",
      "93         638           1     1  0.669677\n",
      "94         639           1     1  0.649925\n",
      "95         659           1     1  0.659660\n",
      "96         663           0     1  0.612850\n",
      "97         668           0     1  0.734670\n",
      "98         682           0     1  0.646250\n",
      "99         697           1     1  0.639853\n",
      "100        725           1     1  0.641405\n",
      "101        727           0     1  0.609204\n",
      "102        733           0     1  0.615667\n",
      "103        739           1     1  0.588890\n",
      "104        740           1     1  0.666540\n",
      "105        774           0     1  0.609739\n",
      "106        778           0     1  0.673546\n",
      "107        792           0     0  0.387407\n",
      "108        797           0     0  0.429546\n",
      "109        807           1     0  0.342624\n",
      "110        808           1     0  0.336783\n",
      "111        810           0     0  0.438668\n",
      "112        814           0     0  0.415325\n",
      "113        820           0     1  0.521105\n",
      "114        828           1     1  0.579398\n",
      "115        830           0     0  0.414072\n",
      "116        836           0     0  0.370688\n",
      "Prediction AUC: 0.5694\n",
      "Prediction Accuracy: 0.5726\n",
      "Prediction Specificity: 0.2143\n",
      "Prediction Sensitivity: 0.9016\n",
      "Prediction Precision: 0.5556\n",
      "the score of the fold number 4 and the type T1wCE: 0.5693793911007026\n",
      "  model       AUC      acc      spec      sens      prec\n",
      "0     4  0.569379  0.57265  0.214286  0.901639  0.555556\n",
      "Prediction AUC: 0.5498\n",
      "Prediction Accuracy: 0.5282\n",
      "Prediction Specificity: 0.5036\n",
      "Prediction Sensitivity: 0.5505\n",
      "Prediction Precision: 0.5505\n",
      "Prediction AUC: 0.5498\n",
      "Prediction Accuracy: 0.5282\n",
      "Prediction Specificity: 0.5036\n",
      "Prediction Sensitivity: 0.5505\n",
      "Prediction Precision: 0.5505\n",
      "the final socre of the type T1wCE\n",
      "0.5498207297354298\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0   all  0.549821  0.528205  0.503597  0.550489  0.550489\n",
      "\n",
      "\n",
      "\n",
      "the final score is\n",
      "0.5498207297354298\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/validation.py --models_folder tunisiaai_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 117/117 [00:17<00:00,  6.61it/s]\n",
      "Fold 0:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            3           0     1  0.578185\n",
      "1           12           1     1  0.580087\n",
      "2           21           0     1  0.551007\n",
      "3           22           0     1  0.535661\n",
      "4           25           1     1  0.531831\n",
      "5           28           1     1  0.568403\n",
      "6           32           0     1  0.536821\n",
      "7           33           1     1  0.515511\n",
      "8           46           1     1  0.545175\n",
      "9           48           1     1  0.531933\n",
      "10          49           0     1  0.526710\n",
      "11          58           1     1  0.565037\n",
      "12          72           0     0  0.490792\n",
      "13          84           0     1  0.566028\n",
      "14          87           1     1  0.578662\n",
      "15          90           0     1  0.566209\n",
      "16          94           1     1  0.513592\n",
      "17          96           1     1  0.544165\n",
      "18          97           0     1  0.569968\n",
      "19         124           0     1  0.520298\n",
      "20         133           0     0  0.472914\n",
      "21         137           0     1  0.547426\n",
      "22         140           1     1  0.524466\n",
      "23         148           0     0  0.489850\n",
      "24         156           1     1  0.507196\n",
      "25         162           0     0  0.419667\n",
      "26         172           0     0  0.376039\n",
      "27         178           1     0  0.396785\n",
      "28         183           0     0  0.388896\n",
      "29         211           0     0  0.421519\n",
      "30         214           0     0  0.455193\n",
      "31         243           0     0  0.469055\n",
      "32         249           0     0  0.378771\n",
      "33         260           1     0  0.367893\n",
      "34         266           0     1  0.508694\n",
      "35         270           1     0  0.491935\n",
      "36         283           0     0  0.351897\n",
      "37         285           1     0  0.370327\n",
      "38         299           1     0  0.398131\n",
      "39         304           1     0  0.388280\n",
      "40         306           1     0  0.427921\n",
      "41         310           0     0  0.424756\n",
      "42         312           0     1  0.504106\n",
      "43         317           1     1  0.523822\n",
      "44         332           1     0  0.471995\n",
      "45         339           0     0  0.384145\n",
      "46         340           1     0  0.384429\n",
      "47         344           1     0  0.462357\n",
      "48         347           0     0  0.409601\n",
      "49         351           0     1  0.507749\n",
      "50         359           1     1  0.544025\n",
      "51         377           0     0  0.487888\n",
      "52         379           0     0  0.424150\n",
      "53         390           0     0  0.425253\n",
      "54         401           0     0  0.396478\n",
      "55         414           0     0  0.362869\n",
      "56         416           1     0  0.365740\n",
      "57         419           0     0  0.364588\n",
      "58         425           1     0  0.355252\n",
      "59         432           0     0  0.414069\n",
      "60         442           1     1  0.517159\n",
      "61         446           0     0  0.493071\n",
      "62         454           0     1  0.515868\n",
      "63         456           1     0  0.499974\n",
      "64         466           1     1  0.582044\n",
      "65         472           1     0  0.497668\n",
      "66         493           1     1  0.567143\n",
      "67         494           1     1  0.556856\n",
      "68         499           1     0  0.484857\n",
      "69         505           1     1  0.549754\n",
      "70         511           1     1  0.528609\n",
      "71         519           0     1  0.503594\n",
      "72         524           1     1  0.526508\n",
      "73         526           1     0  0.464179\n",
      "74         538           0     1  0.579181\n",
      "75         540           0     1  0.532763\n",
      "76         547           0     1  0.532709\n",
      "77         549           1     1  0.521154\n",
      "78         552           1     1  0.587221\n",
      "79         558           1     1  0.545671\n",
      "80         575           0     0  0.442366\n",
      "81         577           1     0  0.498117\n",
      "82         582           1     0  0.410751\n",
      "83         583           1     1  0.553829\n",
      "84         586           1     1  0.522551\n",
      "85         594           1     0  0.469831\n",
      "86         596           0     0  0.496327\n",
      "87         597           1     0  0.467152\n",
      "88         598           1     0  0.483321\n",
      "89         616           0     1  0.508224\n",
      "90         641           0     1  0.509028\n",
      "91         642           0     1  0.534719\n",
      "92         651           0     1  0.527000\n",
      "93         652           1     0  0.480717\n",
      "94         656           1     0  0.469821\n",
      "95         657           0     1  0.520271\n",
      "96         667           0     1  0.527457\n",
      "97         690           1     1  0.531820\n",
      "98         704           1     1  0.592022\n",
      "99         706           0     1  0.518217\n",
      "100        714           1     1  0.569302\n",
      "101        716           1     1  0.541015\n",
      "102        744           0     0  0.497737\n",
      "103        747           0     1  0.512767\n",
      "104        750           1     1  0.535025\n",
      "105        753           0     1  0.505347\n",
      "106        759           0     0  0.483382\n",
      "107        773           1     1  0.523951\n",
      "108        777           1     1  0.555523\n",
      "109        784           1     0  0.489265\n",
      "110        789           1     1  0.501490\n",
      "111        794           1     0  0.471321\n",
      "112        837           0     0  0.434715\n",
      "113        838           1     0  0.425551\n",
      "114        840           1     0  0.394358\n",
      "115       1005           1     0  0.478806\n",
      "116       1009           0     1  0.512889\n",
      "Prediction AUC: 0.5760\n",
      "Prediction Accuracy: 0.5214\n",
      "Prediction Specificity: 0.4909\n",
      "Prediction Sensitivity: 0.5484\n",
      "Prediction Precision: 0.5484\n",
      "the score of the fold number 0 and the type T1wCE: 0.5759530791788856\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     0  0.575953  0.521368  0.490909  0.548387  0.548387\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 117/117 [00:16<00:00,  6.97it/s]\n",
      "Fold 1:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            0           1     1  0.705193\n",
      "1            5           1     1  0.696102\n",
      "2            8           1     1  0.690189\n",
      "3           17           0     1  0.625012\n",
      "4           19           0     1  0.693636\n",
      "5           20           1     1  0.660859\n",
      "6           24           0     1  0.650625\n",
      "7           36           0     1  0.676110\n",
      "8           62           1     1  0.689332\n",
      "9           66           1     1  0.670309\n",
      "10          85           1     1  0.609311\n",
      "11         102           0     1  0.548104\n",
      "12         111           0     1  0.579378\n",
      "13         116           0     1  0.523847\n",
      "14         117           1     1  0.686440\n",
      "15         130           0     1  0.585025\n",
      "16         132           0     1  0.649624\n",
      "17         136           1     1  0.630596\n",
      "18         144           1     1  0.757913\n",
      "19         146           1     1  0.648230\n",
      "20         154           0     1  0.595352\n",
      "21         165           0     1  0.539412\n",
      "22         166           1     1  0.639406\n",
      "23         167           0     1  0.557531\n",
      "24         188           1     1  0.582589\n",
      "25         192           0     1  0.502969\n",
      "26         206           0     1  0.552332\n",
      "27         222           1     1  0.547839\n",
      "28         234           1     1  0.640538\n",
      "29         236           0     1  0.591030\n",
      "30         237           0     1  0.546239\n",
      "31         245           1     1  0.618824\n",
      "32         247           0     1  0.607007\n",
      "33         251           0     1  0.570475\n",
      "34         258           0     1  0.551249\n",
      "35         259           0     1  0.569325\n",
      "36         261           0     1  0.566790\n",
      "37         269           0     1  0.613123\n",
      "38         275           0     0  0.484402\n",
      "39         281           1     1  0.695873\n",
      "40         286           0     1  0.546195\n",
      "41         289           0     1  0.595716\n",
      "42         293           1     1  0.557692\n",
      "43         294           1     0  0.491688\n",
      "44         311           1     1  0.564373\n",
      "45         331           1     1  0.564926\n",
      "46         338           1     1  0.567843\n",
      "47         349           0     0  0.492705\n",
      "48         366           1     1  0.706786\n",
      "49         370           1     1  0.569736\n",
      "50         371           1     1  0.589368\n",
      "51         373           0     1  0.555146\n",
      "52         388           0     1  0.553740\n",
      "53         389           0     1  0.676028\n",
      "54         402           0     1  0.550105\n",
      "55         403           1     1  0.791057\n",
      "56         417           0     1  0.592909\n",
      "57         418           0     1  0.622877\n",
      "58         426           1     1  0.596293\n",
      "59         430           0     1  0.519297\n",
      "60         451           1     1  0.685112\n",
      "61         469           0     1  0.678305\n",
      "62         470           1     1  0.652356\n",
      "63         479           1     1  0.608246\n",
      "64         491           1     1  0.629453\n",
      "65         516           1     1  0.620035\n",
      "66         520           1     1  0.708210\n",
      "67         532           1     1  0.697711\n",
      "68         548           1     1  0.692879\n",
      "69         551           1     1  0.635600\n",
      "70         557           1     1  0.717302\n",
      "71         565           0     1  0.658312\n",
      "72         572           0     1  0.708836\n",
      "73         576           1     1  0.594811\n",
      "74         590           1     1  0.627217\n",
      "75         599           1     1  0.621766\n",
      "76         601           0     1  0.637167\n",
      "77         604           1     1  0.652904\n",
      "78         608           1     1  0.653428\n",
      "79         621           1     1  0.630687\n",
      "80         645           0     1  0.631100\n",
      "81         646           1     1  0.610414\n",
      "82         649           0     1  0.630619\n",
      "83         650           1     1  0.651515\n",
      "84         654           0     1  0.710179\n",
      "85         655           1     1  0.590933\n",
      "86         658           1     1  0.628937\n",
      "87         661           1     1  0.668300\n",
      "88         674           1     1  0.717563\n",
      "89         676           1     1  0.657848\n",
      "90         683           0     1  0.677309\n",
      "91         684           0     1  0.628898\n",
      "92         687           0     1  0.692091\n",
      "93         698           1     1  0.656290\n",
      "94         705           1     1  0.672312\n",
      "95         724           0     1  0.686613\n",
      "96         729           0     1  0.616134\n",
      "97         732           1     1  0.698702\n",
      "98         734           0     1  0.612755\n",
      "99         736           1     1  0.618325\n",
      "100        742           0     1  0.645917\n",
      "101        756           0     1  0.651244\n",
      "102        757           1     1  0.679729\n",
      "103        758           1     1  0.617679\n",
      "104        764           0     1  0.688799\n",
      "105        791           1     1  0.513992\n",
      "106        800           0     0  0.498208\n",
      "107        801           1     1  0.527790\n",
      "108        803           0     1  0.535371\n",
      "109        804           0     1  0.571729\n",
      "110        811           1     1  0.569581\n",
      "111        823           1     0  0.498143\n",
      "112        824           0     1  0.553421\n",
      "113        834           0     1  0.668059\n",
      "114        839           0     1  0.572784\n",
      "115        999           1     1  0.551333\n",
      "116       1008           1     1  0.595452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction AUC: 0.6531\n",
      "Prediction Accuracy: 0.5385\n",
      "Prediction Specificity: 0.0545\n",
      "Prediction Sensitivity: 0.9677\n",
      "Prediction Precision: 0.5357\n",
      "the score of the fold number 1 and the type T1wCE: 0.6530791788856305\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     1  0.653079  0.538462  0.054545  0.967742  0.535714\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 117/117 [00:16<00:00,  7.25it/s]\n",
      "Fold 2:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            2           1     1  0.612273\n",
      "1            6           1     1  0.625027\n",
      "2           11           1     0  0.410529\n",
      "3           30           0     1  0.523880\n",
      "4           59           1     0  0.495441\n",
      "5           61           0     1  0.591775\n",
      "6           64           0     1  0.640633\n",
      "7           77           1     1  0.571820\n",
      "8           78           1     1  0.592387\n",
      "9          105           1     0  0.286855\n",
      "10         107           1     0  0.252142\n",
      "11         108           0     0  0.195402\n",
      "12         110           0     0  0.421910\n",
      "13         112           0     0  0.202504\n",
      "14         121           0     0  0.242352\n",
      "15         122           0     0  0.378502\n",
      "16         123           0     0  0.137043\n",
      "17         138           1     0  0.285427\n",
      "18         142           0     0  0.385459\n",
      "19         157           0     1  0.532795\n",
      "20         159           1     0  0.427841\n",
      "21         169           0     0  0.243308\n",
      "22         185           1     0  0.496752\n",
      "23         186           1     0  0.426277\n",
      "24         187           1     0  0.399098\n",
      "25         194           0     0  0.465503\n",
      "26         199           1     0  0.487547\n",
      "27         201           0     0  0.437838\n",
      "28         218           0     0  0.399903\n",
      "29         227           0     1  0.517204\n",
      "30         230           1     0  0.299416\n",
      "31         239           0     0  0.411000\n",
      "32         241           0     0  0.405766\n",
      "33         262           0     0  0.337927\n",
      "34         271           1     0  0.476993\n",
      "35         282           1     0  0.425622\n",
      "36         284           1     0  0.370500\n",
      "37         291           1     0  0.263473\n",
      "38         298           0     0  0.367846\n",
      "39         300           0     0  0.443178\n",
      "40         303           1     0  0.393524\n",
      "41         308           0     0  0.371508\n",
      "42         321           1     0  0.467822\n",
      "43         327           0     0  0.358310\n",
      "44         334           1     0  0.424382\n",
      "45         336           0     0  0.466299\n",
      "46         346           0     0  0.479908\n",
      "47         350           1     0  0.131463\n",
      "48         352           1     0  0.408979\n",
      "49         356           0     0  0.356779\n",
      "50         367           1     0  0.238305\n",
      "51         369           1     0  0.338361\n",
      "52         383           1     0  0.340047\n",
      "53         386           1     0  0.360876\n",
      "54         392           0     0  0.475850\n",
      "55         397           0     0  0.407780\n",
      "56         400           1     0  0.475999\n",
      "57         406           1     0  0.455680\n",
      "58         412           0     0  0.376902\n",
      "59         423           0     0  0.425312\n",
      "60         433           0     0  0.493626\n",
      "61         436           1     0  0.151505\n",
      "62         441           0     0  0.258770\n",
      "63         449           1     0  0.434205\n",
      "64         452           0     0  0.462175\n",
      "65         459           0     0  0.391080\n",
      "66         468           1     1  0.559632\n",
      "67         480           1     1  0.568026\n",
      "68         481           0     1  0.586761\n",
      "69         483           1     1  0.590105\n",
      "70         495           0     0  0.491063\n",
      "71         510           0     1  0.549844\n",
      "72         525           1     1  0.589148\n",
      "73         528           1     1  0.505221\n",
      "74         530           0     1  0.548143\n",
      "75         533           0     0  0.464581\n",
      "76         537           1     0  0.331806\n",
      "77         542           1     1  0.552776\n",
      "78         544           1     1  0.588921\n",
      "79         556           1     0  0.444252\n",
      "80         567           0     0  0.467109\n",
      "81         569           0     0  0.378087\n",
      "82         571           0     0  0.233570\n",
      "83         588           0     0  0.425203\n",
      "84         589           0     0  0.411046\n",
      "85         593           1     0  0.476882\n",
      "86         602           1     1  0.554334\n",
      "87         610           1     1  0.525633\n",
      "88         613           1     1  0.578538\n",
      "89         618           1     0  0.447785\n",
      "90         623           0     1  0.559836\n",
      "91         624           0     1  0.616930\n",
      "92         679           1     0  0.254238\n",
      "93         688           0     1  0.510821\n",
      "94         691           1     0  0.400703\n",
      "95         692           1     0  0.460314\n",
      "96         694           1     0  0.462419\n",
      "97         703           0     1  0.571518\n",
      "98         709           0     1  0.567503\n",
      "99         731           1     1  0.552659\n",
      "100        735           0     0  0.490697\n",
      "101        751           0     0  0.434748\n",
      "102        765           1     1  0.612421\n",
      "103        772           1     1  0.566490\n",
      "104        781           1     0  0.308066\n",
      "105        782           1     0  0.364184\n",
      "106        787           1     0  0.314047\n",
      "107        795           1     0  0.281636\n",
      "108        796           0     0  0.416541\n",
      "109        799           0     0  0.291699\n",
      "110        802           0     0  0.419962\n",
      "111        816           1     0  0.252386\n",
      "112        818           0     0  0.349447\n",
      "113        998           1     0  0.018931\n",
      "114       1002           1     0  0.272732\n",
      "115       1003           1     0  0.191653\n",
      "116       1010           0     0  0.474224\n",
      "Prediction AUC: 0.4971\n",
      "Prediction Accuracy: 0.5128\n",
      "Prediction Specificity: 0.7679\n",
      "Prediction Sensitivity: 0.2787\n",
      "Prediction Precision: 0.5667\n",
      "the score of the fold number 2 and the type T1wCE: 0.497072599531616\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     2  0.497073  0.512821  0.767857  0.278689  0.566667\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 117/117 [00:18<00:00,  6.46it/s]\n",
      "Fold 3:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0           26           1     0  0.393890\n",
      "1           31           1     1  0.501990\n",
      "2           43           1     1  0.514937\n",
      "3           45           0     0  0.382004\n",
      "4           54           1     0  0.488947\n",
      "5           56           1     0  0.440049\n",
      "6           63           1     0  0.492415\n",
      "7           68           1     0  0.289394\n",
      "8           70           1     0  0.351083\n",
      "9           71           1     0  0.424353\n",
      "10          74           1     0  0.482404\n",
      "11          88           0     0  0.413402\n",
      "12          89           1     1  0.501703\n",
      "13          99           0     0  0.443553\n",
      "14         106           1     0  0.070632\n",
      "15         109           1     0  0.157875\n",
      "16         113           0     0  0.187670\n",
      "17         139           1     0  0.153971\n",
      "18         147           0     0  0.202236\n",
      "19         149           0     0  0.182871\n",
      "20         150           0     0  0.274508\n",
      "21         151           0     0  0.087255\n",
      "22         155           1     0  0.243585\n",
      "23         170           0     0  0.186169\n",
      "24         171           1     0  0.240088\n",
      "25         176           0     0  0.430867\n",
      "26         191           0     0  0.260901\n",
      "27         196           1     0  0.214065\n",
      "28         203           1     0  0.300390\n",
      "29         209           0     0  0.294865\n",
      "30         216           0     0  0.162695\n",
      "31         221           0     0  0.258887\n",
      "32         228           0     0  0.203701\n",
      "33         231           0     0  0.325070\n",
      "34         235           1     0  0.367529\n",
      "35         238           0     0  0.217962\n",
      "36         240           1     0  0.353144\n",
      "37         242           0     0  0.188305\n",
      "38         253           1     0  0.281420\n",
      "39         263           1     0  0.177420\n",
      "40         267           0     0  0.296569\n",
      "41         288           0     0  0.085919\n",
      "42         290           0     0  0.263248\n",
      "43         297           0     0  0.216263\n",
      "44         301           0     0  0.221859\n",
      "45         309           0     0  0.264636\n",
      "46         313           1     0  0.289855\n",
      "47         316           0     0  0.240873\n",
      "48         320           0     0  0.279295\n",
      "49         324           0     0  0.326834\n",
      "50         328           1     0  0.171184\n",
      "51         343           0     0  0.296677\n",
      "52         348           0     0  0.251380\n",
      "53         376           0     0  0.029547\n",
      "54         387           0     0  0.359074\n",
      "55         395           0     0  0.279804\n",
      "56         404           1     0  0.340193\n",
      "57         409           1     0  0.291493\n",
      "58         431           1     0  0.297192\n",
      "59         440           1     0  0.336253\n",
      "60         444           0     0  0.224943\n",
      "61         455           0     0  0.218980\n",
      "62         457           1     0  0.096120\n",
      "63         478           1     0  0.327717\n",
      "64         496           0     0  0.296914\n",
      "65         498           0     0  0.423320\n",
      "66         501           1     0  0.425213\n",
      "67         504           1     0  0.405686\n",
      "68         512           0     0  0.488946\n",
      "69         513           1     0  0.473728\n",
      "70         529           1     0  0.350275\n",
      "71         539           1     0  0.450705\n",
      "72         543           1     0  0.389523\n",
      "73         545           0     0  0.498074\n",
      "74         555           0     0  0.362949\n",
      "75         559           1     0  0.384602\n",
      "76         574           0     0  0.295691\n",
      "77         578           0     0  0.140789\n",
      "78         581           0     0  0.236787\n",
      "79         587           0     0  0.211106\n",
      "80         606           1     0  0.287392\n",
      "81         611           1     0  0.480094\n",
      "82         620           0     0  0.394851\n",
      "83         625           1     0  0.350247\n",
      "84         626           1     0  0.455442\n",
      "85         631           1     0  0.367123\n",
      "86         640           1     0  0.471566\n",
      "87         675           1     0  0.459438\n",
      "88         677           1     0  0.245407\n",
      "89         680           1     0  0.359881\n",
      "90         685           0     0  0.410071\n",
      "91         686           0     0  0.497782\n",
      "92         693           1     0  0.429868\n",
      "93         707           1     0  0.335114\n",
      "94         708           1     0  0.492122\n",
      "95         715           1     0  0.468231\n",
      "96         718           1     0  0.292453\n",
      "97         723           0     0  0.435255\n",
      "98         728           0     0  0.477193\n",
      "99         730           0     1  0.512430\n",
      "100        737           1     0  0.379514\n",
      "101        746           1     0  0.449077\n",
      "102        760           1     0  0.326680\n",
      "103        767           0     0  0.350598\n",
      "104        768           1     0  0.366450\n",
      "105        775           1     0  0.336957\n",
      "106        780           0     0  0.167249\n",
      "107        788           0     0  0.170793\n",
      "108        793           1     0  0.132330\n",
      "109        805           0     0  0.172405\n",
      "110        806           0     0  0.311101\n",
      "111        809           0     0  0.196249\n",
      "112        819           1     0  0.203348\n",
      "113       1000           1     0  0.349293\n",
      "114       1001           1     0  0.247117\n",
      "115       1004           0     0  0.310807\n",
      "116       1007           1     0  0.150174\n",
      "Prediction AUC: 0.6513\n",
      "Prediction Accuracy: 0.4957\n",
      "Prediction Specificity: 0.9821\n",
      "Prediction Sensitivity: 0.0492\n",
      "Prediction Precision: 0.7500\n",
      "the score of the fold number 3 and the type T1wCE: 0.6513466042154566\n",
      "  model       AUC       acc      spec     sens  prec\n",
      "0     3  0.651347  0.495726  0.982143  0.04918  0.75\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 117/117 [00:17<00:00,  6.73it/s]\n",
      "Fold 4:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            9           0     1  0.652635\n",
      "1           14           1     1  0.640172\n",
      "2           18           0     1  0.644432\n",
      "3           35           1     1  0.655470\n",
      "4           44           0     1  0.653089\n",
      "5           52           1     1  0.629109\n",
      "6           53           0     1  0.834944\n",
      "7           60           1     1  0.585243\n",
      "8           81           0     1  0.637737\n",
      "9           95           0     1  0.774231\n",
      "10          98           1     1  0.806121\n",
      "11         100           1     0  0.413707\n",
      "12         104           0     0  0.481362\n",
      "13         120           1     1  0.540661\n",
      "14         128           1     0  0.386363\n",
      "15         134           1     0  0.371252\n",
      "16         143           1     1  0.542263\n",
      "17         158           0     1  0.563857\n",
      "18         160           1     1  0.529754\n",
      "19         177           1     1  0.533624\n",
      "20         184           0     1  0.516942\n",
      "21         193           0     0  0.397154\n",
      "22         195           0     1  0.564775\n",
      "23         197           1     1  0.537092\n",
      "24         204           1     1  0.534969\n",
      "25         210           1     1  0.557479\n",
      "26         212           1     1  0.521028\n",
      "27         217           0     0  0.435965\n",
      "28         219           0     1  0.558838\n",
      "29         220           1     1  0.530529\n",
      "30         233           1     1  0.525778\n",
      "31         246           1     1  0.542534\n",
      "32         250           1     1  0.596607\n",
      "33         254           1     1  0.537358\n",
      "34         273           1     1  0.532813\n",
      "35         274           0     1  0.564082\n",
      "36         280           0     1  0.540529\n",
      "37         296           1     1  0.560699\n",
      "38         305           1     1  0.606260\n",
      "39         314           0     1  0.506660\n",
      "40         318           0     1  0.521649\n",
      "41         322           1     1  0.555115\n",
      "42         325           0     0  0.494430\n",
      "43         329           1     1  0.539431\n",
      "44         341           0     1  0.532051\n",
      "45         353           0     1  0.524047\n",
      "46         360           1     1  0.573665\n",
      "47         364           1     1  0.562308\n",
      "48         378           0     1  0.550577\n",
      "49         380           0     1  0.555968\n",
      "50         382           0     1  0.538578\n",
      "51         391           0     0  0.496715\n",
      "52         399           0     1  0.552963\n",
      "53         405           0     0  0.465741\n",
      "54         407           0     1  0.508612\n",
      "55         408           1     1  0.562665\n",
      "56         410           0     1  0.566760\n",
      "57         413           1     1  0.575333\n",
      "58         421           0     1  0.549755\n",
      "59         429           1     0  0.333508\n",
      "60         443           1     1  0.615857\n",
      "61         445           0     1  0.569181\n",
      "62         464           0     1  0.601265\n",
      "63         477           0     1  0.626092\n",
      "64         485           1     1  0.653530\n",
      "65         488           1     1  0.624358\n",
      "66         500           1     1  0.648048\n",
      "67         502           1     1  0.615176\n",
      "68         506           1     1  0.637254\n",
      "69         507           0     1  0.666005\n",
      "70         514           0     1  0.658569\n",
      "71         517           1     1  0.624014\n",
      "72         518           0     1  0.650951\n",
      "73         523           1     1  0.652765\n",
      "74         550           1     1  0.662881\n",
      "75         554           1     1  0.821847\n",
      "76         561           1     1  0.623254\n",
      "77         563           0     1  0.608080\n",
      "78         564           1     1  0.654906\n",
      "79         568           0     1  0.514649\n",
      "80         570           1     1  0.522952\n",
      "81         579           1     1  0.633200\n",
      "82         584           1     1  0.510001\n",
      "83         591           0     1  0.635520\n",
      "84         605           0     1  0.618630\n",
      "85         607           1     1  0.610458\n",
      "86         612           1     1  0.651905\n",
      "87         615           1     1  0.677073\n",
      "88         619           0     1  0.613073\n",
      "89         622           1     1  0.657247\n",
      "90         628           1     1  0.651368\n",
      "91         630           0     1  0.682798\n",
      "92         636           0     1  0.613189\n",
      "93         638           1     1  0.669677\n",
      "94         639           1     1  0.649925\n",
      "95         659           1     1  0.659660\n",
      "96         663           0     1  0.612850\n",
      "97         668           0     1  0.734670\n",
      "98         682           0     1  0.646250\n",
      "99         697           1     1  0.639853\n",
      "100        725           1     1  0.641405\n",
      "101        727           0     1  0.609204\n",
      "102        733           0     1  0.615667\n",
      "103        739           1     1  0.588890\n",
      "104        740           1     1  0.666540\n",
      "105        774           0     1  0.609739\n",
      "106        778           0     1  0.673546\n",
      "107        792           0     0  0.387407\n",
      "108        797           0     0  0.429546\n",
      "109        807           1     0  0.342624\n",
      "110        808           1     0  0.336783\n",
      "111        810           0     0  0.438668\n",
      "112        814           0     0  0.415325\n",
      "113        820           0     1  0.521105\n",
      "114        828           1     1  0.579398\n",
      "115        830           0     0  0.414072\n",
      "116        836           0     0  0.370688\n",
      "Prediction AUC: 0.5694\n",
      "Prediction Accuracy: 0.5726\n",
      "Prediction Specificity: 0.2143\n",
      "Prediction Sensitivity: 0.9016\n",
      "Prediction Precision: 0.5556\n",
      "the score of the fold number 4 and the type T1wCE: 0.5693793911007026\n",
      "  model       AUC      acc      spec      sens      prec\n",
      "0     4  0.569379  0.57265  0.214286  0.901639  0.555556\n",
      "     BraTS21ID  MGMT_real_value  MGMT_pred_value  MGMT_prob_value\n",
      "0            0                1              1.0         0.705193\n",
      "1            2                1              1.0         0.612273\n",
      "2            3                0              1.0         0.578185\n",
      "3            5                1              1.0         0.696102\n",
      "4            6                1              1.0         0.625027\n",
      "5            8                1              1.0         0.690189\n",
      "6            9                0              1.0         0.652635\n",
      "7           11                1              0.0         0.410529\n",
      "8           12                1              1.0         0.580087\n",
      "9           14                1              1.0         0.640172\n",
      "10          17                0              1.0         0.625012\n",
      "11          18                0              1.0         0.644432\n",
      "12          19                0              1.0         0.693636\n",
      "13          20                1              1.0         0.660859\n",
      "14          21                0              1.0         0.551007\n",
      "15          22                0              1.0         0.535661\n",
      "16          24                0              1.0         0.650625\n",
      "17          25                1              1.0         0.531831\n",
      "18          26                1              0.0         0.393890\n",
      "19          28                1              1.0         0.568403\n",
      "20          30                0              1.0         0.523880\n",
      "21          31                1              1.0         0.501990\n",
      "22          32                0              1.0         0.536821\n",
      "23          33                1              1.0         0.515511\n",
      "24          35                1              1.0         0.655470\n",
      "25          36                0              1.0         0.676110\n",
      "26          43                1              1.0         0.514937\n",
      "27          44                0              1.0         0.653089\n",
      "28          45                0              0.0         0.382004\n",
      "29          46                1              1.0         0.545175\n",
      "30          48                1              1.0         0.531933\n",
      "31          49                0              1.0         0.526710\n",
      "32          52                1              1.0         0.629109\n",
      "33          53                0              1.0         0.834944\n",
      "34          54                1              0.0         0.488947\n",
      "35          56                1              0.0         0.440049\n",
      "36          58                1              1.0         0.565037\n",
      "37          59                1              0.0         0.495441\n",
      "38          60                1              1.0         0.585243\n",
      "39          61                0              1.0         0.591775\n",
      "40          62                1              1.0         0.689332\n",
      "41          63                1              0.0         0.492415\n",
      "42          64                0              1.0         0.640633\n",
      "43          66                1              1.0         0.670309\n",
      "44          68                1              0.0         0.289394\n",
      "45          70                1              0.0         0.351083\n",
      "46          71                1              0.0         0.424353\n",
      "47          72                0              0.0         0.490792\n",
      "48          74                1              0.0         0.482404\n",
      "49          77                1              1.0         0.571820\n",
      "50          78                1              1.0         0.592387\n",
      "51          81                0              1.0         0.637737\n",
      "52          84                0              1.0         0.566028\n",
      "53          85                1              1.0         0.609311\n",
      "54          87                1              1.0         0.578662\n",
      "55          88                0              0.0         0.413402\n",
      "56          89                1              1.0         0.501703\n",
      "57          90                0              1.0         0.566209\n",
      "58          94                1              1.0         0.513592\n",
      "59          95                0              1.0         0.774231\n",
      "60          96                1              1.0         0.544165\n",
      "61          97                0              1.0         0.569968\n",
      "62          98                1              1.0         0.806121\n",
      "63          99                0              0.0         0.443553\n",
      "64         100                1              0.0         0.413707\n",
      "65         102                0              1.0         0.548104\n",
      "66         104                0              0.0         0.481362\n",
      "67         105                1              0.0         0.286855\n",
      "68         106                1              0.0         0.070632\n",
      "69         107                1              0.0         0.252142\n",
      "70         108                0              0.0         0.195402\n",
      "71         109                1              0.0         0.157875\n",
      "72         110                0              0.0         0.421910\n",
      "73         111                0              1.0         0.579378\n",
      "74         112                0              0.0         0.202504\n",
      "75         113                0              0.0         0.187670\n",
      "76         116                0              1.0         0.523847\n",
      "77         117                1              1.0         0.686440\n",
      "78         120                1              1.0         0.540661\n",
      "79         121                0              0.0         0.242352\n",
      "80         122                0              0.0         0.378502\n",
      "81         123                0              0.0         0.137043\n",
      "82         124                0              1.0         0.520298\n",
      "83         128                1              0.0         0.386363\n",
      "84         130                0              1.0         0.585025\n",
      "85         132                0              1.0         0.649624\n",
      "86         133                0              0.0         0.472914\n",
      "87         134                1              0.0         0.371252\n",
      "88         136                1              1.0         0.630596\n",
      "89         137                0              1.0         0.547426\n",
      "90         138                1              0.0         0.285427\n",
      "91         139                1              0.0         0.153971\n",
      "92         140                1              1.0         0.524466\n",
      "93         142                0              0.0         0.385459\n",
      "94         143                1              1.0         0.542263\n",
      "95         144                1              1.0         0.757913\n",
      "96         146                1              1.0         0.648230\n",
      "97         147                0              0.0         0.202236\n",
      "98         148                0              0.0         0.489850\n",
      "99         149                0              0.0         0.182871\n",
      "100        150                0              0.0         0.274508\n",
      "101        151                0              0.0         0.087255\n",
      "102        154                0              1.0         0.595352\n",
      "103        155                1              0.0         0.243585\n",
      "104        156                1              1.0         0.507196\n",
      "105        157                0              1.0         0.532795\n",
      "106        158                0              1.0         0.563857\n",
      "107        159                1              0.0         0.427841\n",
      "108        160                1              1.0         0.529754\n",
      "109        162                0              0.0         0.419667\n",
      "110        165                0              1.0         0.539412\n",
      "111        166                1              1.0         0.639406\n",
      "112        167                0              1.0         0.557531\n",
      "113        169                0              0.0         0.243308\n",
      "114        170                0              0.0         0.186169\n",
      "115        171                1              0.0         0.240088\n",
      "116        172                0              0.0         0.376039\n",
      "117        176                0              0.0         0.430867\n",
      "118        177                1              1.0         0.533624\n",
      "119        178                1              0.0         0.396785\n",
      "120        183                0              0.0         0.388896\n",
      "121        184                0              1.0         0.516942\n",
      "122        185                1              0.0         0.496752\n",
      "123        186                1              0.0         0.426277\n",
      "124        187                1              0.0         0.399098\n",
      "125        188                1              1.0         0.582589\n",
      "126        191                0              0.0         0.260901\n",
      "127        192                0              1.0         0.502969\n",
      "128        193                0              0.0         0.397154\n",
      "129        194                0              0.0         0.465503\n",
      "130        195                0              1.0         0.564775\n",
      "131        196                1              0.0         0.214065\n",
      "132        197                1              1.0         0.537092\n",
      "133        199                1              0.0         0.487547\n",
      "134        201                0              0.0         0.437838\n",
      "135        203                1              0.0         0.300390\n",
      "136        204                1              1.0         0.534969\n",
      "137        206                0              1.0         0.552332\n",
      "138        209                0              0.0         0.294865\n",
      "139        210                1              1.0         0.557479\n",
      "140        211                0              0.0         0.421519\n",
      "141        212                1              1.0         0.521028\n",
      "142        214                0              0.0         0.455193\n",
      "143        216                0              0.0         0.162695\n",
      "144        217                0              0.0         0.435965\n",
      "145        218                0              0.0         0.399903\n",
      "146        219                0              1.0         0.558838\n",
      "147        220                1              1.0         0.530529\n",
      "148        221                0              0.0         0.258887\n",
      "149        222                1              1.0         0.547839\n",
      "150        227                0              1.0         0.517204\n",
      "151        228                0              0.0         0.203701\n",
      "152        230                1              0.0         0.299416\n",
      "153        231                0              0.0         0.325070\n",
      "154        233                1              1.0         0.525778\n",
      "155        234                1              1.0         0.640538\n",
      "156        235                1              0.0         0.367529\n",
      "157        236                0              1.0         0.591030\n",
      "158        237                0              1.0         0.546239\n",
      "159        238                0              0.0         0.217962\n",
      "160        239                0              0.0         0.411000\n",
      "161        240                1              0.0         0.353144\n",
      "162        241                0              0.0         0.405766\n",
      "163        242                0              0.0         0.188305\n",
      "164        243                0              0.0         0.469055\n",
      "165        245                1              1.0         0.618824\n",
      "166        246                1              1.0         0.542534\n",
      "167        247                0              1.0         0.607007\n",
      "168        249                0              0.0         0.378771\n",
      "169        250                1              1.0         0.596607\n",
      "170        251                0              1.0         0.570475\n",
      "171        253                1              0.0         0.281420\n",
      "172        254                1              1.0         0.537358\n",
      "173        258                0              1.0         0.551249\n",
      "174        259                0              1.0         0.569325\n",
      "175        260                1              0.0         0.367893\n",
      "176        261                0              1.0         0.566790\n",
      "177        262                0              0.0         0.337927\n",
      "178        263                1              0.0         0.177420\n",
      "179        266                0              1.0         0.508694\n",
      "180        267                0              0.0         0.296569\n",
      "181        269                0              1.0         0.613123\n",
      "182        270                1              0.0         0.491935\n",
      "183        271                1              0.0         0.476993\n",
      "184        273                1              1.0         0.532813\n",
      "185        274                0              1.0         0.564082\n",
      "186        275                0              0.0         0.484402\n",
      "187        280                0              1.0         0.540529\n",
      "188        281                1              1.0         0.695873\n",
      "189        282                1              0.0         0.425622\n",
      "190        283                0              0.0         0.351897\n",
      "191        284                1              0.0         0.370500\n",
      "192        285                1              0.0         0.370327\n",
      "193        286                0              1.0         0.546195\n",
      "194        288                0              0.0         0.085919\n",
      "195        289                0              1.0         0.595716\n",
      "196        290                0              0.0         0.263248\n",
      "197        291                1              0.0         0.263473\n",
      "198        293                1              1.0         0.557692\n",
      "199        294                1              0.0         0.491688\n",
      "200        296                1              1.0         0.560699\n",
      "201        297                0              0.0         0.216263\n",
      "202        298                0              0.0         0.367846\n",
      "203        299                1              0.0         0.398131\n",
      "204        300                0              0.0         0.443178\n",
      "205        301                0              0.0         0.221859\n",
      "206        303                1              0.0         0.393524\n",
      "207        304                1              0.0         0.388280\n",
      "208        305                1              1.0         0.606260\n",
      "209        306                1              0.0         0.427921\n",
      "210        308                0              0.0         0.371508\n",
      "211        309                0              0.0         0.264636\n",
      "212        310                0              0.0         0.424756\n",
      "213        311                1              1.0         0.564373\n",
      "214        312                0              1.0         0.504106\n",
      "215        313                1              0.0         0.289855\n",
      "216        314                0              1.0         0.506660\n",
      "217        316                0              0.0         0.240873\n",
      "218        317                1              1.0         0.523822\n",
      "219        318                0              1.0         0.521649\n",
      "220        320                0              0.0         0.279295\n",
      "221        321                1              0.0         0.467822\n",
      "222        322                1              1.0         0.555115\n",
      "223        324                0              0.0         0.326834\n",
      "224        325                0              0.0         0.494430\n",
      "225        327                0              0.0         0.358310\n",
      "226        328                1              0.0         0.171184\n",
      "227        329                1              1.0         0.539431\n",
      "228        331                1              1.0         0.564926\n",
      "229        332                1              0.0         0.471995\n",
      "230        334                1              0.0         0.424382\n",
      "231        336                0              0.0         0.466299\n",
      "232        338                1              1.0         0.567843\n",
      "233        339                0              0.0         0.384145\n",
      "234        340                1              0.0         0.384429\n",
      "235        341                0              1.0         0.532051\n",
      "236        343                0              0.0         0.296677\n",
      "237        344                1              0.0         0.462357\n",
      "238        346                0              0.0         0.479908\n",
      "239        347                0              0.0         0.409601\n",
      "240        348                0              0.0         0.251380\n",
      "241        349                0              0.0         0.492705\n",
      "242        350                1              0.0         0.131463\n",
      "243        351                0              1.0         0.507749\n",
      "244        352                1              0.0         0.408979\n",
      "245        353                0              1.0         0.524047\n",
      "246        356                0              0.0         0.356779\n",
      "247        359                1              1.0         0.544025\n",
      "248        360                1              1.0         0.573665\n",
      "249        364                1              1.0         0.562308\n",
      "250        366                1              1.0         0.706786\n",
      "251        367                1              0.0         0.238305\n",
      "252        369                1              0.0         0.338361\n",
      "253        370                1              1.0         0.569736\n",
      "254        371                1              1.0         0.589368\n",
      "255        373                0              1.0         0.555146\n",
      "256        376                0              0.0         0.029547\n",
      "257        377                0              0.0         0.487888\n",
      "258        378                0              1.0         0.550577\n",
      "259        379                0              0.0         0.424150\n",
      "260        380                0              1.0         0.555968\n",
      "261        382                0              1.0         0.538578\n",
      "262        383                1              0.0         0.340047\n",
      "263        386                1              0.0         0.360876\n",
      "264        387                0              0.0         0.359074\n",
      "265        388                0              1.0         0.553740\n",
      "266        389                0              1.0         0.676028\n",
      "267        390                0              0.0         0.425253\n",
      "268        391                0              0.0         0.496715\n",
      "269        392                0              0.0         0.475850\n",
      "270        395                0              0.0         0.279804\n",
      "271        397                0              0.0         0.407780\n",
      "272        399                0              1.0         0.552963\n",
      "273        400                1              0.0         0.475999\n",
      "274        401                0              0.0         0.396478\n",
      "275        402                0              1.0         0.550105\n",
      "276        403                1              1.0         0.791057\n",
      "277        404                1              0.0         0.340193\n",
      "278        405                0              0.0         0.465741\n",
      "279        406                1              0.0         0.455680\n",
      "280        407                0              1.0         0.508612\n",
      "281        408                1              1.0         0.562665\n",
      "282        409                1              0.0         0.291493\n",
      "283        410                0              1.0         0.566760\n",
      "284        412                0              0.0         0.376902\n",
      "285        413                1              1.0         0.575333\n",
      "286        414                0              0.0         0.362869\n",
      "287        416                1              0.0         0.365740\n",
      "288        417                0              1.0         0.592909\n",
      "289        418                0              1.0         0.622877\n",
      "290        419                0              0.0         0.364588\n",
      "291        421                0              1.0         0.549755\n",
      "292        423                0              0.0         0.425312\n",
      "293        425                1              0.0         0.355252\n",
      "294        426                1              1.0         0.596293\n",
      "295        429                1              0.0         0.333508\n",
      "296        430                0              1.0         0.519297\n",
      "297        431                1              0.0         0.297192\n",
      "298        432                0              0.0         0.414069\n",
      "299        433                0              0.0         0.493626\n",
      "300        436                1              0.0         0.151505\n",
      "301        440                1              0.0         0.336253\n",
      "302        441                0              0.0         0.258770\n",
      "303        442                1              1.0         0.517159\n",
      "304        443                1              1.0         0.615857\n",
      "305        444                0              0.0         0.224943\n",
      "306        445                0              1.0         0.569181\n",
      "307        446                0              0.0         0.493071\n",
      "308        449                1              0.0         0.434205\n",
      "309        451                1              1.0         0.685112\n",
      "310        452                0              0.0         0.462175\n",
      "311        454                0              1.0         0.515868\n",
      "312        455                0              0.0         0.218980\n",
      "313        456                1              0.0         0.499974\n",
      "314        457                1              0.0         0.096120\n",
      "315        459                0              0.0         0.391080\n",
      "316        464                0              1.0         0.601265\n",
      "317        466                1              1.0         0.582044\n",
      "318        468                1              1.0         0.559632\n",
      "319        469                0              1.0         0.678305\n",
      "320        470                1              1.0         0.652356\n",
      "321        472                1              0.0         0.497668\n",
      "322        477                0              1.0         0.626092\n",
      "323        478                1              0.0         0.327717\n",
      "324        479                1              1.0         0.608246\n",
      "325        480                1              1.0         0.568026\n",
      "326        481                0              1.0         0.586761\n",
      "327        483                1              1.0         0.590105\n",
      "328        485                1              1.0         0.653530\n",
      "329        488                1              1.0         0.624358\n",
      "330        491                1              1.0         0.629453\n",
      "331        493                1              1.0         0.567143\n",
      "332        494                1              1.0         0.556856\n",
      "333        495                0              0.0         0.491063\n",
      "334        496                0              0.0         0.296914\n",
      "335        498                0              0.0         0.423320\n",
      "336        499                1              0.0         0.484857\n",
      "337        500                1              1.0         0.648048\n",
      "338        501                1              0.0         0.425213\n",
      "339        502                1              1.0         0.615176\n",
      "340        504                1              0.0         0.405686\n",
      "341        505                1              1.0         0.549754\n",
      "342        506                1              1.0         0.637254\n",
      "343        507                0              1.0         0.666005\n",
      "344        510                0              1.0         0.549844\n",
      "345        511                1              1.0         0.528609\n",
      "346        512                0              0.0         0.488946\n",
      "347        513                1              0.0         0.473728\n",
      "348        514                0              1.0         0.658569\n",
      "349        516                1              1.0         0.620035\n",
      "350        517                1              1.0         0.624014\n",
      "351        518                0              1.0         0.650951\n",
      "352        519                0              1.0         0.503594\n",
      "353        520                1              1.0         0.708210\n",
      "354        523                1              1.0         0.652765\n",
      "355        524                1              1.0         0.526508\n",
      "356        525                1              1.0         0.589148\n",
      "357        526                1              0.0         0.464179\n",
      "358        528                1              1.0         0.505221\n",
      "359        529                1              0.0         0.350275\n",
      "360        530                0              1.0         0.548143\n",
      "361        532                1              1.0         0.697711\n",
      "362        533                0              0.0         0.464581\n",
      "363        537                1              0.0         0.331806\n",
      "364        538                0              1.0         0.579181\n",
      "365        539                1              0.0         0.450705\n",
      "366        540                0              1.0         0.532763\n",
      "367        542                1              1.0         0.552776\n",
      "368        543                1              0.0         0.389523\n",
      "369        544                1              1.0         0.588921\n",
      "370        545                0              0.0         0.498074\n",
      "371        547                0              1.0         0.532709\n",
      "372        548                1              1.0         0.692879\n",
      "373        549                1              1.0         0.521154\n",
      "374        550                1              1.0         0.662881\n",
      "375        551                1              1.0         0.635600\n",
      "376        552                1              1.0         0.587221\n",
      "377        554                1              1.0         0.821847\n",
      "378        555                0              0.0         0.362949\n",
      "379        556                1              0.0         0.444252\n",
      "380        557                1              1.0         0.717302\n",
      "381        558                1              1.0         0.545671\n",
      "382        559                1              0.0         0.384602\n",
      "383        561                1              1.0         0.623254\n",
      "384        563                0              1.0         0.608080\n",
      "385        564                1              1.0         0.654906\n",
      "386        565                0              1.0         0.658312\n",
      "387        567                0              0.0         0.467109\n",
      "388        568                0              1.0         0.514649\n",
      "389        569                0              0.0         0.378087\n",
      "390        570                1              1.0         0.522952\n",
      "391        571                0              0.0         0.233570\n",
      "392        572                0              1.0         0.708836\n",
      "393        574                0              0.0         0.295691\n",
      "394        575                0              0.0         0.442366\n",
      "395        576                1              1.0         0.594811\n",
      "396        577                1              0.0         0.498117\n",
      "397        578                0              0.0         0.140789\n",
      "398        579                1              1.0         0.633200\n",
      "399        581                0              0.0         0.236787\n",
      "400        582                1              0.0         0.410751\n",
      "401        583                1              1.0         0.553829\n",
      "402        584                1              1.0         0.510001\n",
      "403        586                1              1.0         0.522551\n",
      "404        587                0              0.0         0.211106\n",
      "405        588                0              0.0         0.425203\n",
      "406        589                0              0.0         0.411046\n",
      "407        590                1              1.0         0.627217\n",
      "408        591                0              1.0         0.635520\n",
      "409        593                1              0.0         0.476882\n",
      "410        594                1              0.0         0.469831\n",
      "411        596                0              0.0         0.496327\n",
      "412        597                1              0.0         0.467152\n",
      "413        598                1              0.0         0.483321\n",
      "414        599                1              1.0         0.621766\n",
      "415        601                0              1.0         0.637167\n",
      "416        602                1              1.0         0.554334\n",
      "417        604                1              1.0         0.652904\n",
      "418        605                0              1.0         0.618630\n",
      "419        606                1              0.0         0.287392\n",
      "420        607                1              1.0         0.610458\n",
      "421        608                1              1.0         0.653428\n",
      "422        610                1              1.0         0.525633\n",
      "423        611                1              0.0         0.480094\n",
      "424        612                1              1.0         0.651905\n",
      "425        613                1              1.0         0.578538\n",
      "426        615                1              1.0         0.677073\n",
      "427        616                0              1.0         0.508224\n",
      "428        618                1              0.0         0.447785\n",
      "429        619                0              1.0         0.613073\n",
      "430        620                0              0.0         0.394851\n",
      "431        621                1              1.0         0.630687\n",
      "432        622                1              1.0         0.657247\n",
      "433        623                0              1.0         0.559836\n",
      "434        624                0              1.0         0.616930\n",
      "435        625                1              0.0         0.350247\n",
      "436        626                1              0.0         0.455442\n",
      "437        628                1              1.0         0.651368\n",
      "438        630                0              1.0         0.682798\n",
      "439        631                1              0.0         0.367123\n",
      "440        636                0              1.0         0.613189\n",
      "441        638                1              1.0         0.669677\n",
      "442        639                1              1.0         0.649925\n",
      "443        640                1              0.0         0.471566\n",
      "444        641                0              1.0         0.509028\n",
      "445        642                0              1.0         0.534719\n",
      "446        645                0              1.0         0.631100\n",
      "447        646                1              1.0         0.610414\n",
      "448        649                0              1.0         0.630619\n",
      "449        650                1              1.0         0.651515\n",
      "450        651                0              1.0         0.527000\n",
      "451        652                1              0.0         0.480717\n",
      "452        654                0              1.0         0.710179\n",
      "453        655                1              1.0         0.590933\n",
      "454        656                1              0.0         0.469821\n",
      "455        657                0              1.0         0.520271\n",
      "456        658                1              1.0         0.628937\n",
      "457        659                1              1.0         0.659660\n",
      "458        661                1              1.0         0.668300\n",
      "459        663                0              1.0         0.612850\n",
      "460        667                0              1.0         0.527457\n",
      "461        668                0              1.0         0.734670\n",
      "462        674                1              1.0         0.717563\n",
      "463        675                1              0.0         0.459438\n",
      "464        676                1              1.0         0.657848\n",
      "465        677                1              0.0         0.245407\n",
      "466        679                1              0.0         0.254238\n",
      "467        680                1              0.0         0.359881\n",
      "468        682                0              1.0         0.646250\n",
      "469        683                0              1.0         0.677309\n",
      "470        684                0              1.0         0.628898\n",
      "471        685                0              0.0         0.410071\n",
      "472        686                0              0.0         0.497782\n",
      "473        687                0              1.0         0.692091\n",
      "474        688                0              1.0         0.510821\n",
      "475        690                1              1.0         0.531820\n",
      "476        691                1              0.0         0.400703\n",
      "477        692                1              0.0         0.460314\n",
      "478        693                1              0.0         0.429868\n",
      "479        694                1              0.0         0.462419\n",
      "480        697                1              1.0         0.639853\n",
      "481        698                1              1.0         0.656290\n",
      "482        703                0              1.0         0.571518\n",
      "483        704                1              1.0         0.592022\n",
      "484        705                1              1.0         0.672312\n",
      "485        706                0              1.0         0.518217\n",
      "486        707                1              0.0         0.335114\n",
      "487        708                1              0.0         0.492122\n",
      "488        709                0              1.0         0.567503\n",
      "489        714                1              1.0         0.569302\n",
      "490        715                1              0.0         0.468231\n",
      "491        716                1              1.0         0.541015\n",
      "492        718                1              0.0         0.292453\n",
      "493        723                0              0.0         0.435255\n",
      "494        724                0              1.0         0.686613\n",
      "495        725                1              1.0         0.641405\n",
      "496        727                0              1.0         0.609204\n",
      "497        728                0              0.0         0.477193\n",
      "498        729                0              1.0         0.616134\n",
      "499        730                0              1.0         0.512430\n",
      "500        731                1              1.0         0.552659\n",
      "501        732                1              1.0         0.698702\n",
      "502        733                0              1.0         0.615667\n",
      "503        734                0              1.0         0.612755\n",
      "504        735                0              0.0         0.490697\n",
      "505        736                1              1.0         0.618325\n",
      "506        737                1              0.0         0.379514\n",
      "507        739                1              1.0         0.588890\n",
      "508        740                1              1.0         0.666540\n",
      "509        742                0              1.0         0.645917\n",
      "510        744                0              0.0         0.497737\n",
      "511        746                1              0.0         0.449077\n",
      "512        747                0              1.0         0.512767\n",
      "513        750                1              1.0         0.535025\n",
      "514        751                0              0.0         0.434748\n",
      "515        753                0              1.0         0.505347\n",
      "516        756                0              1.0         0.651244\n",
      "517        757                1              1.0         0.679729\n",
      "518        758                1              1.0         0.617679\n",
      "519        759                0              0.0         0.483382\n",
      "520        760                1              0.0         0.326680\n",
      "521        764                0              1.0         0.688799\n",
      "522        765                1              1.0         0.612421\n",
      "523        767                0              0.0         0.350598\n",
      "524        768                1              0.0         0.366450\n",
      "525        772                1              1.0         0.566490\n",
      "526        773                1              1.0         0.523951\n",
      "527        774                0              1.0         0.609739\n",
      "528        775                1              0.0         0.336957\n",
      "529        777                1              1.0         0.555523\n",
      "530        778                0              1.0         0.673546\n",
      "531        780                0              0.0         0.167249\n",
      "532        781                1              0.0         0.308066\n",
      "533        782                1              0.0         0.364184\n",
      "534        784                1              0.0         0.489265\n",
      "535        787                1              0.0         0.314047\n",
      "536        788                0              0.0         0.170793\n",
      "537        789                1              1.0         0.501490\n",
      "538        791                1              1.0         0.513992\n",
      "539        792                0              0.0         0.387407\n",
      "540        793                1              0.0         0.132330\n",
      "541        794                1              0.0         0.471321\n",
      "542        795                1              0.0         0.281636\n",
      "543        796                0              0.0         0.416541\n",
      "544        797                0              0.0         0.429546\n",
      "545        799                0              0.0         0.291699\n",
      "546        800                0              0.0         0.498208\n",
      "547        801                1              1.0         0.527790\n",
      "548        802                0              0.0         0.419962\n",
      "549        803                0              1.0         0.535371\n",
      "550        804                0              1.0         0.571729\n",
      "551        805                0              0.0         0.172405\n",
      "552        806                0              0.0         0.311101\n",
      "553        807                1              0.0         0.342624\n",
      "554        808                1              0.0         0.336783\n",
      "555        809                0              0.0         0.196249\n",
      "556        810                0              0.0         0.438668\n",
      "557        811                1              1.0         0.569581\n",
      "558        814                0              0.0         0.415325\n",
      "559        816                1              0.0         0.252386\n",
      "560        818                0              0.0         0.349447\n",
      "561        819                1              0.0         0.203348\n",
      "562        820                0              1.0         0.521105\n",
      "563        823                1              0.0         0.498143\n",
      "564        824                0              1.0         0.553421\n",
      "565        828                1              1.0         0.579398\n",
      "566        830                0              0.0         0.414072\n",
      "567        834                0              1.0         0.668059\n",
      "568        836                0              0.0         0.370688\n",
      "569        837                0              0.0         0.434715\n",
      "570        838                1              0.0         0.425551\n",
      "571        839                0              1.0         0.572784\n",
      "572        840                1              0.0         0.394358\n",
      "573        998                1              0.0         0.018931\n",
      "574        999                1              1.0         0.551333\n",
      "575       1000                1              0.0         0.349293\n",
      "576       1001                1              0.0         0.247117\n",
      "577       1002                1              0.0         0.272732\n",
      "578       1003                1              0.0         0.191653\n",
      "579       1004                0              0.0         0.310807\n",
      "580       1005                1              0.0         0.478806\n",
      "581       1007                1              0.0         0.150174\n",
      "582       1008                1              1.0         0.595452\n",
      "583       1009                0              1.0         0.512889\n",
      "584       1010                0              0.0         0.474224\n",
      "Prediction AUC: 0.5498\n",
      "Prediction Accuracy: 0.5282\n",
      "Prediction Specificity: 0.5036\n",
      "Prediction Sensitivity: 0.5505\n",
      "Prediction Precision: 0.5505\n",
      "Prediction AUC: 0.5498\n",
      "Prediction Accuracy: 0.5282\n",
      "Prediction Specificity: 0.5036\n",
      "Prediction Sensitivity: 0.5505\n",
      "Prediction Precision: 0.5505\n",
      "the final socre of the type T1wCE\n",
      "0.5498207297354298\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0   all  0.549821  0.528205  0.503597  0.550489  0.550489\n",
      "\n",
      "\n",
      "\n",
      "the final score is\n",
      "0.5498207297354298\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/validation.py --models_folder tunisiaai_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 185/185 [00:50<00:00,  3.64it/s]\n",
      "Fold 0:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0       100022           0     0  0.260301\n",
      "1       100034           0     0  0.367641\n",
      "2       100088           0     0  0.247411\n",
      "3       100091           0     0  0.270198\n",
      "4       100092           0     0  0.251348\n",
      "5       100093           0     0  0.244145\n",
      "6       100093           0     0  0.244145\n",
      "7       100094           0     0  0.283796\n",
      "8       100095           1     0  0.356227\n",
      "9       100098           1     0  0.339694\n",
      "10      100115           1     0  0.392235\n",
      "11      100117           0     0  0.299067\n",
      "12      100120           0     0  0.218644\n",
      "13      100121           0     0  0.098002\n",
      "14      100122           0     0  0.177474\n",
      "15      100122           0     0  0.177474\n",
      "16      100124           1     0  0.270870\n",
      "17      100128           1     0  0.342825\n",
      "18      100128           1     0  0.342825\n",
      "19      100129           0     0  0.254916\n",
      "20      100130           1     0  0.272267\n",
      "21      100131           1     0  0.177728\n",
      "22      100132           0     0  0.237834\n",
      "23      100133           0     0  0.211815\n",
      "24      100134           0     0  0.056868\n",
      "25      100134           0     0  0.056868\n",
      "26      100135           1     0  0.331897\n",
      "27      100136           0     0  0.235711\n",
      "28      100138           1     0  0.332711\n",
      "29      100139           0     0  0.367191\n",
      "30      100140           0     0  0.212621\n",
      "31      100140           0     0  0.212621\n",
      "32      100141           0     0  0.174085\n",
      "33      100143           0     0  0.221275\n",
      "34      100144           0     0  0.375538\n",
      "35      100145           0     0  0.319525\n",
      "36      100146           0     0  0.188756\n",
      "37      100147           0     0  0.288114\n",
      "38      100148           0     0  0.202061\n",
      "39      100148           0     0  0.202061\n",
      "40      100149           0     0  0.414897\n",
      "41      100150           0     0  0.210446\n",
      "42      100150           0     0  0.210446\n",
      "43      100151           0     0  0.374454\n",
      "44      100183           1     0  0.251386\n",
      "45      100197           1     0  0.111196\n",
      "46      100197           1     0  0.111196\n",
      "47      100240           1     0  0.238408\n",
      "48      100244           0     0  0.147485\n",
      "49      100246           0     0  0.186268\n",
      "50      100260           1     0  0.235331\n",
      "51      100262           0     0  0.195216\n",
      "52      100263           1     0  0.217217\n",
      "53      100264           1     0  0.386121\n",
      "54      100267           0     0  0.276014\n",
      "55      100267           0     0  0.276014\n",
      "56      100269           1     0  0.296012\n",
      "57      100282           0     0  0.271633\n",
      "58      100287           1     0  0.266994\n",
      "59      100290           0     0  0.328837\n",
      "60      100294           0     0  0.271609\n",
      "61      100301           0     0  0.323255\n",
      "62      100302           1     0  0.205761\n",
      "63      100304           0     0  0.206577\n",
      "64      100310           1     0  0.188748\n",
      "65      100312           0     0  0.201379\n",
      "66      100312           0     0  0.201379\n",
      "67      100314           0     0  0.360970\n",
      "68      100316           0     0  0.162841\n",
      "69      100336           0     0  0.186977\n",
      "70      100344           1     0  0.242537\n",
      "71      100344           1     0  0.242537\n",
      "72      100350           1     0  0.216876\n",
      "73      100351           1     0  0.189280\n",
      "74      100352           0     0  0.220847\n",
      "75      100352           0     0  0.220847\n",
      "76      100354           0     0  0.205535\n",
      "77      100355           0     0  0.236194\n",
      "78      100356           1     0  0.315055\n",
      "79      100358           1     0  0.178200\n",
      "80      100359           1     0  0.302726\n",
      "81      100360           1     0  0.243473\n",
      "82      100362           0     0  0.347956\n",
      "83      100363           1     0  0.161354\n",
      "84      100367           0     0  0.304252\n",
      "85      100368           0     0  0.186162\n",
      "86      100369           0     0  0.245628\n",
      "87      100370           0     0  0.189392\n",
      "88      100371           1     0  0.184902\n",
      "89      100373           0     0  0.300408\n",
      "90      100375           0     0  0.192933\n",
      "91      100376           0     0  0.227202\n",
      "92      100378           1     0  0.213733\n",
      "93      100379           0     0  0.245464\n",
      "94      100380           1     0  0.210561\n",
      "95      100381           1     0  0.241314\n",
      "96      100384           0     0  0.311264\n",
      "97      100385           0     0  0.274611\n",
      "98      100387           0     0  0.276481\n",
      "99      100388           1     0  0.146528\n",
      "100     100390           0     0  0.220907\n",
      "101     100391           0     0  0.235579\n",
      "102     100392           1     0  0.347522\n",
      "103     100393           0     0  0.366682\n",
      "104     100395           1     0  0.210497\n",
      "105     100398           0     0  0.192873\n",
      "106     100399           1     0  0.299464\n",
      "107     100401           1     0  0.392215\n",
      "108     100402           0     0  0.375335\n",
      "109     100403           1     0  0.249654\n",
      "110     100404           1     0  0.186117\n",
      "111     100405           0     0  0.251920\n",
      "112     100406           1     0  0.249338\n",
      "113     100407           1     0  0.368656\n",
      "114     100408           0     0  0.310764\n",
      "115     100409           1     0  0.309368\n",
      "116     100410           0     0  0.210161\n",
      "117     100411           0     0  0.271509\n",
      "118     100412           1     0  0.188368\n",
      "119     100413           0     0  0.285202\n",
      "120     100414           1     0  0.368021\n",
      "121     100415           0     0  0.360800\n",
      "122     100416           0     0  0.286007\n",
      "123     100418           1     0  0.307893\n",
      "124     100419           0     0  0.277577\n",
      "125     100420           1     0  0.246425\n",
      "126     100421           0     0  0.337003\n",
      "127     100423           1     0  0.413308\n",
      "128     100424           1     0  0.201296\n",
      "129     100425           1     0  0.218590\n",
      "130     100426           0     0  0.197838\n",
      "131     100428           0     0  0.243511\n",
      "132     100430           1     0  0.296900\n",
      "133     100431           0     0  0.389656\n",
      "134     100432           0     0  0.262679\n",
      "135     100434           0     0  0.348803\n",
      "136     100435           0     0  0.231943\n",
      "137     100436           0     0  0.276453\n",
      "138     100438           1     0  0.349457\n",
      "139     100439           1     0  0.423619\n",
      "140     100442           1     0  0.261209\n",
      "141     100443           1     0  0.151127\n",
      "142     100444           1     0  0.180238\n",
      "143     100445           1     0  0.280419\n",
      "144     100446           1     0  0.257893\n",
      "145     100447           0     0  0.207645\n",
      "146     100448           0     0  0.310469\n",
      "147     100449           0     0  0.321508\n",
      "148     100452           0     0  0.407911\n",
      "149     100453           1     0  0.287202\n",
      "150     100454           0     0  0.398458\n",
      "151     100455           1     0  0.277340\n",
      "152     100456           0     0  0.199129\n",
      "153     100457           0     0  0.267751\n",
      "154     100458           0     0  0.170617\n",
      "155     100459           0     0  0.305134\n",
      "156     100460           0     0  0.296334\n",
      "157     100461           1     0  0.255115\n",
      "158     100462           0     0  0.355280\n",
      "159     100463           0     0  0.389445\n",
      "160     100469           1     0  0.359677\n",
      "161     100470           0     0  0.239420\n",
      "162     100471           0     0  0.316182\n",
      "163     100473           0     0  0.199341\n",
      "164     100474           1     0  0.167791\n",
      "165     100475           0     0  0.242459\n",
      "166     100476           0     0  0.276929\n",
      "167     100479           0     0  0.332128\n",
      "168     100480           0     0  0.269885\n",
      "169     100481           1     0  0.458337\n",
      "170     100482           0     0  0.280803\n",
      "171     100483           0     0  0.355384\n",
      "172     100484           0     0  0.323421\n",
      "173     100486           0     0  0.247156\n",
      "174     100487           0     0  0.228885\n",
      "175     100488           1     0  0.333271\n",
      "176     100489           0     0  0.379810\n",
      "177     100490           0     0  0.249853\n",
      "178     100492           0     0  0.399923\n",
      "179     100494           1     0  0.218496\n",
      "180     100495           0     0  0.463137\n",
      "181     100496           1     0  0.268636\n",
      "182     100498           1     0  0.333659\n",
      "183     100499           0     0  0.066099\n",
      "184     100500           0     0  0.337138\n",
      "Prediction AUC: 0.5087\n",
      "Prediction Accuracy: 0.6270\n",
      "Prediction Specificity: 1.0000\n",
      "Prediction Sensitivity: 0.0000\n",
      "Prediction Precision: 0.0000\n",
      "the score of the fold number 0 and the type T1wCE: 0.5087456271864068\n",
      "  model       AUC       acc  spec  sens  prec\n",
      "0     0  0.508746  0.627027   1.0   0.0     0\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 185/185 [00:50<00:00,  3.65it/s]\n",
      "Fold 1:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0       100022           0     1  0.694093\n",
      "1       100034           0     1  0.726850\n",
      "2       100088           0     1  0.621802\n",
      "3       100091           0     1  0.663218\n",
      "4       100092           0     1  0.523536\n",
      "5       100093           0     1  0.613923\n",
      "6       100093           0     1  0.613923\n",
      "7       100094           0     0  0.345664\n",
      "8       100095           1     1  0.506155\n",
      "9       100098           1     1  0.511734\n",
      "10      100115           1     1  0.665782\n",
      "11      100117           0     1  0.617304\n",
      "12      100120           0     1  0.708875\n",
      "13      100121           0     0  0.423099\n",
      "14      100122           0     1  0.532154\n",
      "15      100122           0     1  0.532154\n",
      "16      100124           1     1  0.710748\n",
      "17      100128           1     1  0.685068\n",
      "18      100128           1     1  0.685068\n",
      "19      100129           0     1  0.551006\n",
      "20      100130           1     1  0.540980\n",
      "21      100131           1     0  0.386886\n",
      "22      100132           0     0  0.492474\n",
      "23      100133           0     1  0.508357\n",
      "24      100134           0     1  0.927183\n",
      "25      100134           0     1  0.927183\n",
      "26      100135           1     0  0.371206\n",
      "27      100136           0     1  0.580573\n",
      "28      100138           1     1  0.666657\n",
      "29      100139           0     1  0.574297\n",
      "30      100140           0     1  0.503426\n",
      "31      100140           0     1  0.503426\n",
      "32      100141           0     0  0.374569\n",
      "33      100143           0     1  0.822054\n",
      "34      100144           0     0  0.421413\n",
      "35      100145           0     1  0.699609\n",
      "36      100146           0     1  0.510141\n",
      "37      100147           0     1  0.631898\n",
      "38      100148           0     0  0.474690\n",
      "39      100148           0     0  0.474690\n",
      "40      100149           0     1  0.707316\n",
      "41      100150           0     0  0.470534\n",
      "42      100150           0     0  0.470534\n",
      "43      100151           0     0  0.420306\n",
      "44      100183           1     1  0.650790\n",
      "45      100197           1     1  0.655232\n",
      "46      100197           1     1  0.655232\n",
      "47      100240           1     0  0.414088\n",
      "48      100244           0     1  0.834175\n",
      "49      100246           0     1  0.535728\n",
      "50      100260           1     1  0.556821\n",
      "51      100262           0     0  0.477946\n",
      "52      100263           1     0  0.470054\n",
      "53      100264           1     0  0.434019\n",
      "54      100267           0     1  0.659684\n",
      "55      100267           0     1  0.659684\n",
      "56      100269           1     1  0.736747\n",
      "57      100282           0     1  0.572561\n",
      "58      100287           1     1  0.551099\n",
      "59      100290           0     1  0.677190\n",
      "60      100294           0     1  0.639129\n",
      "61      100301           0     1  0.657089\n",
      "62      100302           1     0  0.497495\n",
      "63      100304           0     0  0.437790\n",
      "64      100310           1     0  0.346085\n",
      "65      100312           0     1  0.516739\n",
      "66      100312           0     1  0.516739\n",
      "67      100314           0     1  0.699519\n",
      "68      100316           0     0  0.472873\n",
      "69      100336           0     0  0.463886\n",
      "70      100344           1     1  0.578854\n",
      "71      100344           1     1  0.578854\n",
      "72      100350           1     0  0.485921\n",
      "73      100351           1     0  0.472231\n",
      "74      100352           0     1  0.610309\n",
      "75      100352           0     1  0.610309\n",
      "76      100354           0     1  0.689195\n",
      "77      100355           0     0  0.445987\n",
      "78      100356           1     1  0.673000\n",
      "79      100358           1     0  0.456372\n",
      "80      100359           1     1  0.619217\n",
      "81      100360           1     1  0.640321\n",
      "82      100362           0     1  0.681802\n",
      "83      100363           1     0  0.457255\n",
      "84      100367           0     1  0.675597\n",
      "85      100368           0     1  0.514871\n",
      "86      100369           0     0  0.481064\n",
      "87      100370           0     1  0.582829\n",
      "88      100371           1     1  0.568968\n",
      "89      100373           0     0  0.413601\n",
      "90      100375           0     0  0.456591\n",
      "91      100376           0     0  0.434881\n",
      "92      100378           1     1  0.517385\n",
      "93      100379           0     1  0.638548\n",
      "94      100380           1     1  0.655704\n",
      "95      100381           1     1  0.505188\n",
      "96      100384           0     1  0.700127\n",
      "97      100385           0     1  0.569803\n",
      "98      100387           0     1  0.651978\n",
      "99      100388           1     0  0.429403\n",
      "100     100390           0     1  0.558199\n",
      "101     100391           0     1  0.607162\n",
      "102     100392           1     1  0.567971\n",
      "103     100393           0     1  0.654804\n",
      "104     100395           1     1  0.549608\n",
      "105     100398           0     1  0.533225\n",
      "106     100399           1     1  0.642543\n",
      "107     100401           1     1  0.661094\n",
      "108     100402           0     1  0.627810\n",
      "109     100403           1     1  0.621772\n",
      "110     100404           1     0  0.474885\n",
      "111     100405           0     1  0.566224\n",
      "112     100406           1     1  0.624817\n",
      "113     100407           1     1  0.651495\n",
      "114     100408           0     1  0.712366\n",
      "115     100409           1     1  0.573328\n",
      "116     100410           0     0  0.480703\n",
      "117     100411           0     1  0.630827\n",
      "118     100412           1     1  0.583875\n",
      "119     100413           0     1  0.610093\n",
      "120     100414           1     1  0.516849\n",
      "121     100415           0     1  0.517351\n",
      "122     100416           0     1  0.514651\n",
      "123     100418           1     1  0.655648\n",
      "124     100419           0     1  0.660079\n",
      "125     100420           1     0  0.455563\n",
      "126     100421           0     1  0.621777\n",
      "127     100423           1     1  0.553954\n",
      "128     100424           1     1  0.508656\n",
      "129     100425           1     1  0.602528\n",
      "130     100426           0     1  0.507877\n",
      "131     100428           0     1  0.596908\n",
      "132     100430           1     1  0.669860\n",
      "133     100431           0     1  0.629521\n",
      "134     100432           0     1  0.553279\n",
      "135     100434           0     1  0.632572\n",
      "136     100435           0     0  0.469482\n",
      "137     100436           0     1  0.599836\n",
      "138     100438           1     1  0.639109\n",
      "139     100439           1     1  0.712734\n",
      "140     100442           1     1  0.691587\n",
      "141     100443           1     0  0.394411\n",
      "142     100444           1     0  0.361019\n",
      "143     100445           1     1  0.516769\n",
      "144     100446           1     1  0.552716\n",
      "145     100447           0     0  0.427801\n",
      "146     100448           0     1  0.630545\n",
      "147     100449           0     0  0.395911\n",
      "148     100452           0     1  0.529869\n",
      "149     100453           1     1  0.635527\n",
      "150     100454           0     1  0.689787\n",
      "151     100455           1     1  0.581277\n",
      "152     100456           0     1  0.612271\n",
      "153     100457           0     1  0.508956\n",
      "154     100458           0     0  0.466507\n",
      "155     100459           0     1  0.629862\n",
      "156     100460           0     1  0.622870\n",
      "157     100461           1     1  0.567801\n",
      "158     100462           0     1  0.666458\n",
      "159     100463           0     1  0.665960\n",
      "160     100469           1     1  0.671848\n",
      "161     100470           0     1  0.562600\n",
      "162     100471           0     1  0.648091\n",
      "163     100473           0     0  0.425958\n",
      "164     100474           1     1  0.872582\n",
      "165     100475           0     1  0.541413\n",
      "166     100476           0     1  0.637551\n",
      "167     100479           0     0  0.467934\n",
      "168     100480           0     1  0.556033\n",
      "169     100481           1     1  0.560774\n",
      "170     100482           0     0  0.355536\n",
      "171     100483           0     1  0.500160\n",
      "172     100484           0     0  0.429038\n",
      "173     100486           0     0  0.288105\n",
      "174     100487           0     0  0.388156\n",
      "175     100488           1     0  0.390076\n",
      "176     100489           0     1  0.658746\n",
      "177     100490           0     0  0.495744\n",
      "178     100492           0     1  0.575244\n",
      "179     100494           1     0  0.380542\n",
      "180     100495           0     1  0.677461\n",
      "181     100496           1     0  0.451432\n",
      "182     100498           1     0  0.479430\n",
      "183     100499           0     1  0.656580\n",
      "184     100500           0     1  0.593923\n",
      "Prediction AUC: 0.4840\n",
      "Prediction Accuracy: 0.4324\n",
      "Prediction Specificity: 0.2672\n",
      "Prediction Sensitivity: 0.7101\n",
      "Prediction Precision: 0.3657\n",
      "the score of the fold number 1 and the type T1wCE: 0.484007996001999\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     1  0.484008  0.432432  0.267241  0.710145  0.365672\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 185/185 [00:50<00:00,  3.64it/s]\n",
      "Fold 2:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0       100022           0     0  0.166792\n",
      "1       100034           0     0  0.421159\n",
      "2       100088           0     0  0.244890\n",
      "3       100091           0     0  0.345206\n",
      "4       100092           0     0  0.153257\n",
      "5       100093           0     0  0.241735\n",
      "6       100093           0     0  0.241735\n",
      "7       100094           0     0  0.128140\n",
      "8       100095           1     0  0.029192\n",
      "9       100098           1     0  0.335260\n",
      "10      100115           1     0  0.413671\n",
      "11      100117           0     0  0.317763\n",
      "12      100120           0     0  0.106692\n",
      "13      100121           0     0  0.051612\n",
      "14      100122           0     0  0.218657\n",
      "15      100122           0     0  0.218657\n",
      "16      100124           1     0  0.250323\n",
      "17      100128           1     0  0.362754\n",
      "18      100128           1     0  0.362754\n",
      "19      100129           0     0  0.242581\n",
      "20      100130           1     0  0.277331\n",
      "21      100131           1     0  0.059804\n",
      "22      100132           0     0  0.147349\n",
      "23      100133           0     0  0.090672\n",
      "24      100134           0     0  0.000047\n",
      "25      100134           0     0  0.000047\n",
      "26      100135           1     0  0.100372\n",
      "27      100136           0     0  0.145639\n",
      "28      100138           1     0  0.352112\n",
      "29      100139           0     0  0.303803\n",
      "30      100140           0     0  0.130025\n",
      "31      100140           0     0  0.130025\n",
      "32      100141           0     0  0.097080\n",
      "33      100143           0     0  0.298042\n",
      "34      100144           0     0  0.181869\n",
      "35      100145           0     0  0.291611\n",
      "36      100146           0     0  0.357483\n",
      "37      100147           0     0  0.327424\n",
      "38      100148           0     0  0.238340\n",
      "39      100148           0     0  0.238340\n",
      "40      100149           0     0  0.441592\n",
      "41      100150           0     0  0.078543\n",
      "42      100150           0     0  0.078543\n",
      "43      100151           0     0  0.185015\n",
      "44      100183           1     0  0.272518\n",
      "45      100197           1     0  0.014697\n",
      "46      100197           1     0  0.014697\n",
      "47      100240           1     0  0.183655\n",
      "48      100244           0     0  0.060391\n",
      "49      100246           0     0  0.144523\n",
      "50      100260           1     0  0.153870\n",
      "51      100262           0     0  0.105186\n",
      "52      100263           1     0  0.170114\n",
      "53      100264           1     0  0.113189\n",
      "54      100267           0     0  0.224039\n",
      "55      100267           0     0  0.224039\n",
      "56      100269           1     0  0.312284\n",
      "57      100282           0     0  0.310962\n",
      "58      100287           1     0  0.226167\n",
      "59      100290           0     0  0.389524\n",
      "60      100294           0     0  0.378344\n",
      "61      100301           0     0  0.395932\n",
      "62      100302           1     0  0.288813\n",
      "63      100304           0     0  0.109868\n",
      "64      100310           1     0  0.145368\n",
      "65      100312           0     0  0.145610\n",
      "66      100312           0     0  0.145610\n",
      "67      100314           0     0  0.402983\n",
      "68      100316           0     0  0.058045\n",
      "69      100336           0     0  0.095137\n",
      "70      100344           1     0  0.197140\n",
      "71      100344           1     0  0.197140\n",
      "72      100350           1     0  0.163109\n",
      "73      100351           1     0  0.050658\n",
      "74      100352           0     0  0.231915\n",
      "75      100352           0     0  0.231915\n",
      "76      100354           0     0  0.176832\n",
      "77      100355           0     0  0.162480\n",
      "78      100356           1     0  0.377584\n",
      "79      100358           1     0  0.080401\n",
      "80      100359           1     0  0.363846\n",
      "81      100360           1     0  0.319516\n",
      "82      100362           0     0  0.363981\n",
      "83      100363           1     0  0.066122\n",
      "84      100367           0     0  0.358464\n",
      "85      100368           0     0  0.129385\n",
      "86      100369           0     0  0.075190\n",
      "87      100370           0     0  0.218029\n",
      "88      100371           1     0  0.237306\n",
      "89      100373           0     0  0.095107\n",
      "90      100375           0     0  0.081050\n",
      "91      100376           0     0  0.171810\n",
      "92      100378           1     0  0.200549\n",
      "93      100379           0     0  0.281292\n",
      "94      100380           1     0  0.289302\n",
      "95      100381           1     0  0.130387\n",
      "96      100384           0     0  0.362718\n",
      "97      100385           0     0  0.231710\n",
      "98      100387           0     0  0.367066\n",
      "99      100388           1     0  0.035954\n",
      "100     100390           0     0  0.200618\n",
      "101     100391           0     0  0.287700\n",
      "102     100392           1     0  0.160833\n",
      "103     100393           0     0  0.217302\n",
      "104     100395           1     0  0.132684\n",
      "105     100398           0     0  0.134622\n",
      "106     100399           1     0  0.357067\n",
      "107     100401           1     0  0.386363\n",
      "108     100402           0     0  0.418071\n",
      "109     100403           1     0  0.247136\n",
      "110     100404           1     0  0.200868\n",
      "111     100405           0     0  0.127088\n",
      "112     100406           1     0  0.279284\n",
      "113     100407           1     0  0.469933\n",
      "114     100408           0     0  0.291628\n",
      "115     100409           1     0  0.156201\n",
      "116     100410           0     0  0.189838\n",
      "117     100411           0     0  0.321338\n",
      "118     100412           1     0  0.271398\n",
      "119     100413           0     0  0.344232\n",
      "120     100414           1     0  0.247360\n",
      "121     100415           0     0  0.285594\n",
      "122     100416           0     0  0.117870\n",
      "123     100418           1     0  0.271911\n",
      "124     100419           0     0  0.262762\n",
      "125     100420           1     0  0.090731\n",
      "126     100421           0     0  0.314560\n",
      "127     100423           1     0  0.197339\n",
      "128     100424           1     0  0.187257\n",
      "129     100425           1     0  0.181458\n",
      "130     100426           0     0  0.087140\n",
      "131     100428           0     0  0.246332\n",
      "132     100430           1     0  0.379475\n",
      "133     100431           0     0  0.363904\n",
      "134     100432           0     0  0.138799\n",
      "135     100434           0     0  0.377537\n",
      "136     100435           0     0  0.117733\n",
      "137     100436           0     0  0.155610\n",
      "138     100438           1     0  0.379047\n",
      "139     100439           1     0  0.407094\n",
      "140     100442           1     0  0.277421\n",
      "141     100443           1     0  0.071370\n",
      "142     100444           1     0  0.080358\n",
      "143     100445           1     0  0.100153\n",
      "144     100446           1     0  0.203729\n",
      "145     100447           0     0  0.154882\n",
      "146     100448           0     0  0.264341\n",
      "147     100449           0     0  0.139740\n",
      "148     100452           0     0  0.261383\n",
      "149     100453           1     0  0.370491\n",
      "150     100454           0     0  0.412440\n",
      "151     100455           1     0  0.251824\n",
      "152     100456           0     0  0.145094\n",
      "153     100457           0     0  0.198241\n",
      "154     100458           0     0  0.077632\n",
      "155     100459           0     0  0.323955\n",
      "156     100460           0     0  0.268965\n",
      "157     100461           1     0  0.281423\n",
      "158     100462           0     0  0.406693\n",
      "159     100463           0     0  0.458553\n",
      "160     100469           1     0  0.385501\n",
      "161     100470           0     0  0.248627\n",
      "162     100471           0     0  0.413497\n",
      "163     100473           0     0  0.106635\n",
      "164     100474           1     0  0.120094\n",
      "165     100475           0     0  0.226950\n",
      "166     100476           0     0  0.271174\n",
      "167     100479           0     0  0.180115\n",
      "168     100480           0     0  0.337804\n",
      "169     100481           1     0  0.385621\n",
      "170     100482           0     0  0.140700\n",
      "171     100483           0     0  0.198651\n",
      "172     100484           0     0  0.180882\n",
      "173     100486           0     0  0.095126\n",
      "174     100487           0     0  0.122071\n",
      "175     100488           1     0  0.152191\n",
      "176     100489           0     0  0.369712\n",
      "177     100490           0     0  0.218710\n",
      "178     100492           0     0  0.435434\n",
      "179     100494           1     0  0.109113\n",
      "180     100495           0     0  0.481505\n",
      "181     100496           1     0  0.085973\n",
      "182     100498           1     0  0.168891\n",
      "183     100499           0     0  0.004548\n",
      "184     100500           0     0  0.373990\n",
      "Prediction AUC: 0.4933\n",
      "Prediction Accuracy: 0.6270\n",
      "Prediction Specificity: 1.0000\n",
      "Prediction Sensitivity: 0.0000\n",
      "Prediction Precision: 0.0000\n",
      "the score of the fold number 2 and the type T1wCE: 0.49325337331334335\n",
      "  model       AUC       acc  spec  sens  prec\n",
      "0     2  0.493253  0.627027   1.0   0.0     0\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 185/185 [00:50<00:00,  3.68it/s]\n",
      "Fold 3:\n",
      "     BraTS21ID  MGMT_value  pred          prob\n",
      "0       100022           0     0  5.510684e-02\n",
      "1       100034           0     0  2.209112e-01\n",
      "2       100088           0     0  1.043878e-01\n",
      "3       100091           0     0  1.605466e-01\n",
      "4       100092           0     0  5.275624e-02\n",
      "5       100093           0     0  1.038401e-01\n",
      "6       100093           0     0  1.038401e-01\n",
      "7       100094           0     0  6.613749e-02\n",
      "8       100095           1     0  6.133833e-03\n",
      "9       100098           1     0  2.351608e-01\n",
      "10      100115           1     0  2.362673e-01\n",
      "11      100117           0     0  1.687540e-01\n",
      "12      100120           0     0  2.263205e-02\n",
      "13      100121           0     0  1.069350e-02\n",
      "14      100122           0     0  9.666080e-02\n",
      "15      100122           0     0  9.666080e-02\n",
      "16      100124           1     0  9.038408e-02\n",
      "17      100128           1     0  1.750648e-01\n",
      "18      100128           1     0  1.750648e-01\n",
      "19      100129           0     0  1.045074e-01\n",
      "20      100130           1     0  1.408423e-01\n",
      "21      100131           1     0  1.607370e-02\n",
      "22      100132           0     0  5.170271e-02\n",
      "23      100133           0     0  2.201739e-02\n",
      "24      100134           0     0  2.645197e-08\n",
      "25      100134           0     0  2.645197e-08\n",
      "26      100135           1     0  4.106784e-02\n",
      "27      100136           0     0  4.346880e-02\n",
      "28      100138           1     0  1.893125e-01\n",
      "29      100139           0     0  1.782328e-01\n",
      "30      100140           0     0  4.255767e-02\n",
      "31      100140           0     0  4.255767e-02\n",
      "32      100141           0     0  3.056609e-02\n",
      "33      100143           0     0  7.950649e-02\n",
      "34      100144           0     0  9.206061e-02\n",
      "35      100145           0     0  1.122456e-01\n",
      "36      100146           0     0  2.101326e-01\n",
      "37      100147           0     0  1.557995e-01\n",
      "38      100148           0     0  1.269809e-01\n",
      "39      100148           0     0  1.269809e-01\n",
      "40      100149           0     0  2.365629e-01\n",
      "41      100150           0     0  1.951039e-02\n",
      "42      100150           0     0  1.951039e-02\n",
      "43      100151           0     0  1.014288e-01\n",
      "44      100183           1     0  1.065087e-01\n",
      "45      100197           1     0  8.866971e-04\n",
      "46      100197           1     0  8.866971e-04\n",
      "47      100240           1     0  7.394151e-02\n",
      "48      100244           0     0  7.179444e-03\n",
      "49      100246           0     0  4.397949e-02\n",
      "50      100260           1     0  5.180589e-02\n",
      "51      100262           0     0  2.920867e-02\n",
      "52      100263           1     0  6.512704e-02\n",
      "53      100264           1     0  4.350151e-02\n",
      "54      100267           0     0  7.916840e-02\n",
      "55      100267           0     0  7.916840e-02\n",
      "56      100269           1     0  1.139828e-01\n",
      "57      100282           0     0  1.842156e-01\n",
      "58      100287           1     0  9.827185e-02\n",
      "59      100290           0     0  2.072566e-01\n",
      "60      100294           0     0  1.987614e-01\n",
      "61      100301           0     0  1.849342e-01\n",
      "62      100302           1     0  1.522056e-01\n",
      "63      100304           0     0  3.459065e-02\n",
      "64      100310           1     0  6.361341e-02\n",
      "65      100312           0     0  4.562607e-02\n",
      "66      100312           0     0  4.562607e-02\n",
      "67      100314           0     0  2.051810e-01\n",
      "68      100316           0     0  1.068002e-02\n",
      "69      100336           0     0  2.396443e-02\n",
      "70      100344           1     0  7.432299e-02\n",
      "71      100344           1     0  7.432299e-02\n",
      "72      100350           1     0  6.132256e-02\n",
      "73      100351           1     0  9.509725e-03\n",
      "74      100352           0     0  8.962236e-02\n",
      "75      100352           0     0  8.962236e-02\n",
      "76      100354           0     0  4.863100e-02\n",
      "77      100355           0     0  5.938117e-02\n",
      "78      100356           1     0  1.781885e-01\n",
      "79      100358           1     0  2.064267e-02\n",
      "80      100359           1     0  2.442057e-01\n",
      "81      100360           1     0  1.498977e-01\n",
      "82      100362           0     0  1.729372e-01\n",
      "83      100363           1     0  1.527533e-02\n",
      "84      100367           0     0  1.752526e-01\n",
      "85      100368           0     0  3.712361e-02\n",
      "86      100369           0     0  1.834059e-02\n",
      "87      100370           0     0  8.277873e-02\n",
      "88      100371           1     0  9.910519e-02\n",
      "89      100373           0     0  3.474764e-02\n",
      "90      100375           0     0  1.966237e-02\n",
      "91      100376           0     0  6.792904e-02\n",
      "92      100378           1     0  8.007355e-02\n",
      "93      100379           0     0  1.166482e-01\n",
      "94      100380           1     0  1.122442e-01\n",
      "95      100381           1     0  4.240234e-02\n",
      "96      100384           0     0  1.615678e-01\n",
      "97      100385           0     0  8.924660e-02\n",
      "98      100387           0     0  1.740244e-01\n",
      "99      100388           1     0  6.339889e-03\n",
      "100     100390           0     0  7.531566e-02\n",
      "101     100391           0     0  1.480065e-01\n",
      "102     100392           1     0  5.212832e-02\n",
      "103     100393           0     0  7.433690e-02\n",
      "104     100395           1     0  3.743194e-02\n",
      "105     100398           0     0  4.209671e-02\n",
      "106     100399           1     0  1.768837e-01\n",
      "107     100401           1     0  1.900688e-01\n",
      "108     100402           0     0  2.609103e-01\n",
      "109     100403           1     0  9.408418e-02\n",
      "110     100404           1     0  8.879635e-02\n",
      "111     100405           0     0  3.937542e-02\n",
      "112     100406           1     0  1.186599e-01\n",
      "113     100407           1     0  2.952516e-01\n",
      "114     100408           0     0  1.182964e-01\n",
      "115     100409           1     0  5.046688e-02\n",
      "116     100410           0     0  7.594253e-02\n",
      "117     100411           0     0  1.425328e-01\n",
      "118     100412           1     0  1.286072e-01\n",
      "119     100413           0     0  1.770959e-01\n",
      "120     100414           1     0  9.914958e-02\n",
      "121     100415           0     0  1.601804e-01\n",
      "122     100416           0     0  3.868709e-02\n",
      "123     100418           1     0  1.151343e-01\n",
      "124     100419           0     0  1.114379e-01\n",
      "125     100420           1     0  2.510352e-02\n",
      "126     100421           0     0  1.584402e-01\n",
      "127     100423           1     0  7.058210e-02\n",
      "128     100424           1     0  7.339421e-02\n",
      "129     100425           1     0  6.115843e-02\n",
      "130     100426           0     0  2.056544e-02\n",
      "131     100428           0     0  9.726822e-02\n",
      "132     100430           1     0  1.880824e-01\n",
      "133     100431           0     0  2.137586e-01\n",
      "134     100432           0     0  4.436731e-02\n",
      "135     100434           0     0  2.291969e-01\n",
      "136     100435           0     0  3.896986e-02\n",
      "137     100436           0     0  5.705573e-02\n",
      "138     100438           1     0  2.091680e-01\n",
      "139     100439           1     0  1.909635e-01\n",
      "140     100442           1     0  1.032638e-01\n",
      "141     100443           1     0  1.862659e-02\n",
      "142     100444           1     0  2.131698e-02\n",
      "143     100445           1     0  2.580130e-02\n",
      "144     100446           1     0  8.969737e-02\n",
      "145     100447           0     0  6.045227e-02\n",
      "146     100448           0     0  1.346614e-01\n",
      "147     100449           0     0  6.129415e-02\n",
      "148     100452           0     0  1.369480e-01\n",
      "149     100453           1     0  1.961003e-01\n",
      "150     100454           0     0  2.293933e-01\n",
      "151     100455           1     0  1.108494e-01\n",
      "152     100456           0     0  4.427669e-02\n",
      "153     100457           0     0  8.514298e-02\n",
      "154     100458           0     0  1.859647e-02\n",
      "155     100459           0     0  1.594422e-01\n",
      "156     100460           0     0  1.287807e-01\n",
      "157     100461           1     0  1.309943e-01\n",
      "158     100462           0     0  2.358439e-01\n",
      "159     100463           0     0  2.812776e-01\n",
      "160     100469           1     0  2.039490e-01\n",
      "161     100470           0     0  1.088668e-01\n",
      "162     100471           0     0  2.378304e-01\n",
      "163     100473           0     0  3.406285e-02\n",
      "164     100474           1     0  1.658139e-02\n",
      "165     100475           0     0  9.179301e-02\n",
      "166     100476           0     0  1.131375e-01\n",
      "167     100479           0     0  7.495262e-02\n",
      "168     100480           0     0  1.947034e-01\n",
      "169     100481           1     0  3.092640e-01\n",
      "170     100482           0     0  7.351545e-02\n",
      "171     100483           0     0  8.034855e-02\n",
      "172     100484           0     0  8.986228e-02\n",
      "173     100486           0     0  4.210526e-02\n",
      "174     100487           0     0  5.677212e-02\n",
      "175     100488           1     0  7.549080e-02\n",
      "176     100489           0     0  1.817397e-01\n",
      "177     100490           0     0  1.039718e-01\n",
      "178     100492           0     0  3.621196e-01\n",
      "179     100494           1     0  3.927028e-02\n",
      "180     100495           0     0  2.945050e-01\n",
      "181     100496           1     0  2.671431e-02\n",
      "182     100498           1     0  5.999920e-02\n",
      "183     100499           0     0  9.255615e-05\n",
      "184     100500           0     0  2.257769e-01\n",
      "Prediction AUC: 0.4834\n",
      "Prediction Accuracy: 0.6270\n",
      "Prediction Specificity: 1.0000\n",
      "Prediction Sensitivity: 0.0000\n",
      "Prediction Precision: 0.0000\n",
      "the score of the fold number 3 and the type T1wCE: 0.4833833083458271\n",
      "  model       AUC       acc  spec  sens  prec\n",
      "0     3  0.483383  0.627027   1.0   0.0     0\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 185/185 [00:51<00:00,  3.61it/s]\n",
      "Fold 4:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0       100022           0     0  0.441719\n",
      "1       100034           0     1  0.530244\n",
      "2       100088           0     0  0.428063\n",
      "3       100091           0     0  0.470057\n",
      "4       100092           0     0  0.389615\n",
      "5       100093           0     0  0.419943\n",
      "6       100093           0     0  0.419943\n",
      "7       100094           0     0  0.194888\n",
      "8       100095           1     0  0.293126\n",
      "9       100098           1     0  0.482392\n",
      "10      100115           1     1  0.557141\n",
      "11      100117           0     0  0.471496\n",
      "12      100120           0     0  0.487068\n",
      "13      100121           0     0  0.235707\n",
      "14      100122           0     0  0.450138\n",
      "15      100122           0     0  0.450138\n",
      "16      100124           1     0  0.431975\n",
      "17      100128           1     1  0.593424\n",
      "18      100128           1     1  0.593424\n",
      "19      100129           0     1  0.550937\n",
      "20      100130           1     0  0.430753\n",
      "21      100131           1     0  0.251948\n",
      "22      100132           0     0  0.347796\n",
      "23      100133           0     0  0.316536\n",
      "24      100134           0     0  0.045064\n",
      "25      100134           0     0  0.045064\n",
      "26      100135           1     0  0.327958\n",
      "27      100136           0     0  0.363084\n",
      "28      100138           1     0  0.497667\n",
      "29      100139           0     1  0.502790\n",
      "30      100140           0     0  0.314141\n",
      "31      100140           0     0  0.314141\n",
      "32      100141           0     0  0.331693\n",
      "33      100143           0     0  0.483355\n",
      "34      100144           0     0  0.449710\n",
      "35      100145           0     0  0.471130\n",
      "36      100146           0     0  0.472652\n",
      "37      100147           0     0  0.478404\n",
      "38      100148           0     0  0.398575\n",
      "39      100148           0     0  0.398575\n",
      "40      100149           0     1  0.563153\n",
      "41      100150           0     0  0.318486\n",
      "42      100150           0     0  0.318486\n",
      "43      100151           0     0  0.405156\n",
      "44      100183           1     1  0.543288\n",
      "45      100197           1     0  0.248303\n",
      "46      100197           1     0  0.248303\n",
      "47      100240           1     0  0.381758\n",
      "48      100244           0     0  0.341415\n",
      "49      100246           0     0  0.333222\n",
      "50      100260           1     0  0.410370\n",
      "51      100262           0     0  0.317412\n",
      "52      100263           1     0  0.373764\n",
      "53      100264           1     0  0.440008\n",
      "54      100267           0     0  0.460018\n",
      "55      100267           0     0  0.460018\n",
      "56      100269           1     0  0.474895\n",
      "57      100282           0     1  0.537490\n",
      "58      100287           1     0  0.455845\n",
      "59      100290           0     1  0.513216\n",
      "60      100294           0     1  0.518843\n",
      "61      100301           0     0  0.468904\n",
      "62      100302           1     0  0.447835\n",
      "63      100304           0     0  0.319404\n",
      "64      100310           1     0  0.408189\n",
      "65      100312           0     0  0.338460\n",
      "66      100312           0     0  0.338460\n",
      "67      100314           0     1  0.521985\n",
      "68      100316           0     0  0.265636\n",
      "69      100336           0     0  0.301350\n",
      "70      100344           1     0  0.408797\n",
      "71      100344           1     0  0.408797\n",
      "72      100350           1     0  0.365470\n",
      "73      100351           1     0  0.272145\n",
      "74      100352           0     0  0.416604\n",
      "75      100352           0     0  0.416604\n",
      "76      100354           0     0  0.394283\n",
      "77      100355           0     0  0.361687\n",
      "78      100356           1     1  0.502543\n",
      "79      100358           1     0  0.258151\n",
      "80      100359           1     1  0.561201\n",
      "81      100360           1     0  0.445923\n",
      "82      100362           0     0  0.498675\n",
      "83      100363           1     0  0.222909\n",
      "84      100367           0     0  0.485381\n",
      "85      100368           0     0  0.332916\n",
      "86      100369           0     0  0.262920\n",
      "87      100370           0     0  0.385264\n",
      "88      100371           1     0  0.363485\n",
      "89      100373           0     0  0.369990\n",
      "90      100375           0     0  0.307349\n",
      "91      100376           0     0  0.350394\n",
      "92      100378           1     0  0.380025\n",
      "93      100379           0     0  0.446732\n",
      "94      100380           1     0  0.432001\n",
      "95      100381           1     0  0.357945\n",
      "96      100384           0     0  0.489056\n",
      "97      100385           0     0  0.428305\n",
      "98      100387           0     1  0.504346\n",
      "99      100388           1     0  0.323401\n",
      "100     100390           0     0  0.383771\n",
      "101     100391           0     0  0.461860\n",
      "102     100392           1     0  0.399768\n",
      "103     100393           0     0  0.461848\n",
      "104     100395           1     0  0.326208\n",
      "105     100398           0     0  0.364574\n",
      "106     100399           1     0  0.495713\n",
      "107     100401           1     1  0.540865\n",
      "108     100402           0     1  0.559534\n",
      "109     100403           1     0  0.428158\n",
      "110     100404           1     0  0.384253\n",
      "111     100405           0     0  0.346705\n",
      "112     100406           1     0  0.450614\n",
      "113     100407           1     1  0.561756\n",
      "114     100408           0     0  0.480222\n",
      "115     100409           1     0  0.396050\n",
      "116     100410           0     0  0.363878\n",
      "117     100411           0     0  0.440807\n",
      "118     100412           1     0  0.376805\n",
      "119     100413           0     0  0.485141\n",
      "120     100414           1     0  0.454648\n",
      "121     100415           0     0  0.499699\n",
      "122     100416           0     0  0.377847\n",
      "123     100418           1     0  0.445540\n",
      "124     100419           0     0  0.432453\n",
      "125     100420           1     0  0.316067\n",
      "126     100421           0     0  0.482352\n",
      "127     100423           1     1  0.606589\n",
      "128     100424           1     0  0.376256\n",
      "129     100425           1     0  0.394138\n",
      "130     100426           0     0  0.280081\n",
      "131     100428           0     0  0.411228\n",
      "132     100430           1     0  0.467746\n",
      "133     100431           0     1  0.544434\n",
      "134     100432           0     0  0.420107\n",
      "135     100434           0     1  0.591197\n",
      "136     100435           0     0  0.323893\n",
      "137     100436           0     0  0.383446\n",
      "138     100438           1     1  0.520592\n",
      "139     100439           1     1  0.547227\n",
      "140     100442           1     0  0.430554\n",
      "141     100443           1     0  0.237383\n",
      "142     100444           1     0  0.262133\n",
      "143     100445           1     0  0.327810\n",
      "144     100446           1     0  0.427310\n",
      "145     100447           0     0  0.392285\n",
      "146     100448           0     0  0.457769\n",
      "147     100449           0     0  0.432388\n",
      "148     100452           0     0  0.320651\n",
      "149     100453           1     0  0.497033\n",
      "150     100454           0     1  0.548923\n",
      "151     100455           1     0  0.432075\n",
      "152     100456           0     0  0.392156\n",
      "153     100457           0     0  0.415485\n",
      "154     100458           0     0  0.269632\n",
      "155     100459           0     0  0.482938\n",
      "156     100460           0     0  0.478941\n",
      "157     100461           1     0  0.460797\n",
      "158     100462           0     1  0.524767\n",
      "159     100463           0     1  0.573350\n",
      "160     100469           1     1  0.521220\n",
      "161     100470           0     0  0.416937\n",
      "162     100471           0     1  0.550124\n",
      "163     100473           0     0  0.318494\n",
      "164     100474           1     0  0.356001\n",
      "165     100475           0     0  0.407778\n",
      "166     100476           0     0  0.434053\n",
      "167     100479           0     0  0.435094\n",
      "168     100480           0     0  0.469261\n",
      "169     100481           1     0  0.436452\n",
      "170     100482           0     0  0.189523\n",
      "171     100483           0     0  0.470000\n",
      "172     100484           0     0  0.231039\n",
      "173     100486           0     0  0.153059\n",
      "174     100487           0     0  0.217956\n",
      "175     100488           1     0  0.250345\n",
      "176     100489           0     1  0.527111\n",
      "177     100490           0     0  0.439095\n",
      "178     100492           0     1  0.606743\n",
      "179     100494           1     0  0.180883\n",
      "180     100495           0     1  0.593955\n",
      "181     100496           1     0  0.291548\n",
      "182     100498           1     1  0.534650\n",
      "183     100499           0     0  0.111573\n",
      "184     100500           0     0  0.498317\n",
      "Prediction AUC: 0.4916\n",
      "Prediction Accuracy: 0.5946\n",
      "Prediction Specificity: 0.8362\n",
      "Prediction Sensitivity: 0.1884\n",
      "Prediction Precision: 0.4062\n",
      "the score of the fold number 4 and the type T1wCE: 0.4916291854072964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model       AUC       acc      spec      sens     prec\r\n",
      "0     4  0.491629  0.594595  0.836207  0.188406  0.40625\r\n",
      "Prediction AUC: 0.4958\r\n",
      "Prediction Accuracy: 0.3730\r\n",
      "Prediction Specificity: 0.2672\r\n",
      "Prediction Sensitivity: 0.5507\r\n",
      "Prediction Precision: 0.3654\r\n",
      "the final socre of the type T1wCE\r\n",
      "0.495752123938031\r\n",
      "  model       AUC       acc     spec     sens      prec\r\n",
      "0   all  0.495752  0.581622  0.82069  0.17971  0.154384\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "the final score is\r\n",
      "0.495752123938031\r\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/validation.py --models_folder tunisiaai_A --csv_file upenn_train_fold_t1wce.csv --full_set True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 185/185 [01:00<00:00,  3.04it/s]\n",
      "Fold 0:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0       100022           0     0  0.260301\n",
      "1       100034           0     0  0.367641\n",
      "2       100088           0     0  0.247411\n",
      "3       100091           0     0  0.270198\n",
      "4       100092           0     0  0.251348\n",
      "5       100093           0     0  0.244145\n",
      "6       100093           0     0  0.244145\n",
      "7       100094           0     0  0.283796\n",
      "8       100095           1     0  0.356227\n",
      "9       100098           1     0  0.339694\n",
      "10      100115           1     0  0.392235\n",
      "11      100117           0     0  0.299067\n",
      "12      100120           0     0  0.218644\n",
      "13      100121           0     0  0.098002\n",
      "14      100122           0     0  0.177474\n",
      "15      100122           0     0  0.177474\n",
      "16      100124           1     0  0.270870\n",
      "17      100128           1     0  0.342825\n",
      "18      100128           1     0  0.342825\n",
      "19      100129           0     0  0.254916\n",
      "20      100130           1     0  0.272267\n",
      "21      100131           1     0  0.177728\n",
      "22      100132           0     0  0.237834\n",
      "23      100133           0     0  0.211815\n",
      "24      100134           0     0  0.056868\n",
      "25      100134           0     0  0.056868\n",
      "26      100135           1     0  0.331897\n",
      "27      100136           0     0  0.235711\n",
      "28      100138           1     0  0.332711\n",
      "29      100139           0     0  0.367191\n",
      "30      100140           0     0  0.212621\n",
      "31      100140           0     0  0.212621\n",
      "32      100141           0     0  0.174085\n",
      "33      100143           0     0  0.221275\n",
      "34      100144           0     0  0.375538\n",
      "35      100145           0     0  0.319525\n",
      "36      100146           0     0  0.188756\n",
      "37      100147           0     0  0.288114\n",
      "38      100148           0     0  0.202061\n",
      "39      100148           0     0  0.202061\n",
      "40      100149           0     0  0.414897\n",
      "41      100150           0     0  0.210446\n",
      "42      100150           0     0  0.210446\n",
      "43      100151           0     0  0.374454\n",
      "44      100183           1     0  0.251386\n",
      "45      100197           1     0  0.111196\n",
      "46      100197           1     0  0.111196\n",
      "47      100240           1     0  0.238408\n",
      "48      100244           0     0  0.147485\n",
      "49      100246           0     0  0.186268\n",
      "50      100260           1     0  0.235331\n",
      "51      100262           0     0  0.195216\n",
      "52      100263           1     0  0.217217\n",
      "53      100264           1     0  0.386121\n",
      "54      100267           0     0  0.276014\n",
      "55      100267           0     0  0.276014\n",
      "56      100269           1     0  0.296012\n",
      "57      100282           0     0  0.271633\n",
      "58      100287           1     0  0.266994\n",
      "59      100290           0     0  0.328837\n",
      "60      100294           0     0  0.271609\n",
      "61      100301           0     0  0.323255\n",
      "62      100302           1     0  0.205761\n",
      "63      100304           0     0  0.206577\n",
      "64      100310           1     0  0.188748\n",
      "65      100312           0     0  0.201379\n",
      "66      100312           0     0  0.201379\n",
      "67      100314           0     0  0.360970\n",
      "68      100316           0     0  0.162841\n",
      "69      100336           0     0  0.186977\n",
      "70      100344           1     0  0.242537\n",
      "71      100344           1     0  0.242537\n",
      "72      100350           1     0  0.216876\n",
      "73      100351           1     0  0.189280\n",
      "74      100352           0     0  0.220847\n",
      "75      100352           0     0  0.220847\n",
      "76      100354           0     0  0.205535\n",
      "77      100355           0     0  0.236194\n",
      "78      100356           1     0  0.315055\n",
      "79      100358           1     0  0.178200\n",
      "80      100359           1     0  0.302726\n",
      "81      100360           1     0  0.243473\n",
      "82      100362           0     0  0.347956\n",
      "83      100363           1     0  0.161354\n",
      "84      100367           0     0  0.304252\n",
      "85      100368           0     0  0.186162\n",
      "86      100369           0     0  0.245628\n",
      "87      100370           0     0  0.189392\n",
      "88      100371           1     0  0.184902\n",
      "89      100373           0     0  0.300408\n",
      "90      100375           0     0  0.192933\n",
      "91      100376           0     0  0.227202\n",
      "92      100378           1     0  0.213733\n",
      "93      100379           0     0  0.245464\n",
      "94      100380           1     0  0.210561\n",
      "95      100381           1     0  0.241314\n",
      "96      100384           0     0  0.311264\n",
      "97      100385           0     0  0.274611\n",
      "98      100387           0     0  0.276481\n",
      "99      100388           1     0  0.146528\n",
      "100     100390           0     0  0.220907\n",
      "101     100391           0     0  0.235579\n",
      "102     100392           1     0  0.347522\n",
      "103     100393           0     0  0.366682\n",
      "104     100395           1     0  0.210497\n",
      "105     100398           0     0  0.192873\n",
      "106     100399           1     0  0.299464\n",
      "107     100401           1     0  0.392215\n",
      "108     100402           0     0  0.375335\n",
      "109     100403           1     0  0.249654\n",
      "110     100404           1     0  0.186117\n",
      "111     100405           0     0  0.251920\n",
      "112     100406           1     0  0.249338\n",
      "113     100407           1     0  0.368656\n",
      "114     100408           0     0  0.310764\n",
      "115     100409           1     0  0.309368\n",
      "116     100410           0     0  0.210161\n",
      "117     100411           0     0  0.271509\n",
      "118     100412           1     0  0.188368\n",
      "119     100413           0     0  0.285202\n",
      "120     100414           1     0  0.368021\n",
      "121     100415           0     0  0.360800\n",
      "122     100416           0     0  0.286007\n",
      "123     100418           1     0  0.307893\n",
      "124     100419           0     0  0.277577\n",
      "125     100420           1     0  0.246425\n",
      "126     100421           0     0  0.337003\n",
      "127     100423           1     0  0.413308\n",
      "128     100424           1     0  0.201296\n",
      "129     100425           1     0  0.218590\n",
      "130     100426           0     0  0.197838\n",
      "131     100428           0     0  0.243511\n",
      "132     100430           1     0  0.296900\n",
      "133     100431           0     0  0.389656\n",
      "134     100432           0     0  0.262679\n",
      "135     100434           0     0  0.348803\n",
      "136     100435           0     0  0.231943\n",
      "137     100436           0     0  0.276453\n",
      "138     100438           1     0  0.349457\n",
      "139     100439           1     0  0.423619\n",
      "140     100442           1     0  0.261209\n",
      "141     100443           1     0  0.151127\n",
      "142     100444           1     0  0.180238\n",
      "143     100445           1     0  0.280419\n",
      "144     100446           1     0  0.257893\n",
      "145     100447           0     0  0.207645\n",
      "146     100448           0     0  0.310469\n",
      "147     100449           0     0  0.321508\n",
      "148     100452           0     0  0.407911\n",
      "149     100453           1     0  0.287202\n",
      "150     100454           0     0  0.398458\n",
      "151     100455           1     0  0.277340\n",
      "152     100456           0     0  0.199129\n",
      "153     100457           0     0  0.267751\n",
      "154     100458           0     0  0.170617\n",
      "155     100459           0     0  0.305134\n",
      "156     100460           0     0  0.296334\n",
      "157     100461           1     0  0.255115\n",
      "158     100462           0     0  0.355280\n",
      "159     100463           0     0  0.389445\n",
      "160     100469           1     0  0.359677\n",
      "161     100470           0     0  0.239420\n",
      "162     100471           0     0  0.316182\n",
      "163     100473           0     0  0.199341\n",
      "164     100474           1     0  0.167791\n",
      "165     100475           0     0  0.242459\n",
      "166     100476           0     0  0.276929\n",
      "167     100479           0     0  0.332128\n",
      "168     100480           0     0  0.269885\n",
      "169     100481           1     0  0.458337\n",
      "170     100482           0     0  0.280803\n",
      "171     100483           0     0  0.355384\n",
      "172     100484           0     0  0.323421\n",
      "173     100486           0     0  0.247156\n",
      "174     100487           0     0  0.228885\n",
      "175     100488           1     0  0.333271\n",
      "176     100489           0     0  0.379810\n",
      "177     100490           0     0  0.249853\n",
      "178     100492           0     0  0.399923\n",
      "179     100494           1     0  0.218496\n",
      "180     100495           0     0  0.463137\n",
      "181     100496           1     0  0.268636\n",
      "182     100498           1     0  0.333659\n",
      "183     100499           0     0  0.066099\n",
      "184     100500           0     0  0.337138\n",
      "Prediction AUC: 0.5087\n",
      "Prediction Accuracy: 0.6270\n",
      "Prediction Specificity: 1.0000\n",
      "Prediction Sensitivity: 0.0000\n",
      "Prediction Precision: 0.0000\n",
      "the score of the fold number 0 and the type T1wCE: 0.5087456271864068\n",
      "  model       AUC       acc  spec  sens  prec\n",
      "0     0  0.508746  0.627027   1.0   0.0     0\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 185/185 [00:51<00:00,  3.56it/s]\n",
      "Fold 1:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0       100022           0     1  0.694093\n",
      "1       100034           0     1  0.726850\n",
      "2       100088           0     1  0.621802\n",
      "3       100091           0     1  0.663218\n",
      "4       100092           0     1  0.523536\n",
      "5       100093           0     1  0.613923\n",
      "6       100093           0     1  0.613923\n",
      "7       100094           0     0  0.345664\n",
      "8       100095           1     1  0.506155\n",
      "9       100098           1     1  0.511734\n",
      "10      100115           1     1  0.665782\n",
      "11      100117           0     1  0.617304\n",
      "12      100120           0     1  0.708875\n",
      "13      100121           0     0  0.423099\n",
      "14      100122           0     1  0.532154\n",
      "15      100122           0     1  0.532154\n",
      "16      100124           1     1  0.710748\n",
      "17      100128           1     1  0.685068\n",
      "18      100128           1     1  0.685068\n",
      "19      100129           0     1  0.551006\n",
      "20      100130           1     1  0.540980\n",
      "21      100131           1     0  0.386886\n",
      "22      100132           0     0  0.492474\n",
      "23      100133           0     1  0.508357\n",
      "24      100134           0     1  0.927183\n",
      "25      100134           0     1  0.927183\n",
      "26      100135           1     0  0.371206\n",
      "27      100136           0     1  0.580573\n",
      "28      100138           1     1  0.666657\n",
      "29      100139           0     1  0.574297\n",
      "30      100140           0     1  0.503426\n",
      "31      100140           0     1  0.503426\n",
      "32      100141           0     0  0.374569\n",
      "33      100143           0     1  0.822054\n",
      "34      100144           0     0  0.421413\n",
      "35      100145           0     1  0.699609\n",
      "36      100146           0     1  0.510141\n",
      "37      100147           0     1  0.631898\n",
      "38      100148           0     0  0.474690\n",
      "39      100148           0     0  0.474690\n",
      "40      100149           0     1  0.707316\n",
      "41      100150           0     0  0.470534\n",
      "42      100150           0     0  0.470534\n",
      "43      100151           0     0  0.420306\n",
      "44      100183           1     1  0.650790\n",
      "45      100197           1     1  0.655232\n",
      "46      100197           1     1  0.655232\n",
      "47      100240           1     0  0.414088\n",
      "48      100244           0     1  0.834175\n",
      "49      100246           0     1  0.535728\n",
      "50      100260           1     1  0.556821\n",
      "51      100262           0     0  0.477946\n",
      "52      100263           1     0  0.470054\n",
      "53      100264           1     0  0.434019\n",
      "54      100267           0     1  0.659684\n",
      "55      100267           0     1  0.659684\n",
      "56      100269           1     1  0.736747\n",
      "57      100282           0     1  0.572561\n",
      "58      100287           1     1  0.551099\n",
      "59      100290           0     1  0.677190\n",
      "60      100294           0     1  0.639129\n",
      "61      100301           0     1  0.657089\n",
      "62      100302           1     0  0.497495\n",
      "63      100304           0     0  0.437790\n",
      "64      100310           1     0  0.346085\n",
      "65      100312           0     1  0.516739\n",
      "66      100312           0     1  0.516739\n",
      "67      100314           0     1  0.699519\n",
      "68      100316           0     0  0.472873\n",
      "69      100336           0     0  0.463886\n",
      "70      100344           1     1  0.578854\n",
      "71      100344           1     1  0.578854\n",
      "72      100350           1     0  0.485921\n",
      "73      100351           1     0  0.472231\n",
      "74      100352           0     1  0.610309\n",
      "75      100352           0     1  0.610309\n",
      "76      100354           0     1  0.689195\n",
      "77      100355           0     0  0.445987\n",
      "78      100356           1     1  0.673000\n",
      "79      100358           1     0  0.456372\n",
      "80      100359           1     1  0.619217\n",
      "81      100360           1     1  0.640321\n",
      "82      100362           0     1  0.681802\n",
      "83      100363           1     0  0.457255\n",
      "84      100367           0     1  0.675597\n",
      "85      100368           0     1  0.514871\n",
      "86      100369           0     0  0.481064\n",
      "87      100370           0     1  0.582829\n",
      "88      100371           1     1  0.568968\n",
      "89      100373           0     0  0.413601\n",
      "90      100375           0     0  0.456591\n",
      "91      100376           0     0  0.434881\n",
      "92      100378           1     1  0.517385\n",
      "93      100379           0     1  0.638548\n",
      "94      100380           1     1  0.655704\n",
      "95      100381           1     1  0.505188\n",
      "96      100384           0     1  0.700127\n",
      "97      100385           0     1  0.569803\n",
      "98      100387           0     1  0.651978\n",
      "99      100388           1     0  0.429403\n",
      "100     100390           0     1  0.558199\n",
      "101     100391           0     1  0.607162\n",
      "102     100392           1     1  0.567971\n",
      "103     100393           0     1  0.654804\n",
      "104     100395           1     1  0.549608\n",
      "105     100398           0     1  0.533225\n",
      "106     100399           1     1  0.642543\n",
      "107     100401           1     1  0.661094\n",
      "108     100402           0     1  0.627810\n",
      "109     100403           1     1  0.621772\n",
      "110     100404           1     0  0.474885\n",
      "111     100405           0     1  0.566224\n",
      "112     100406           1     1  0.624817\n",
      "113     100407           1     1  0.651495\n",
      "114     100408           0     1  0.712366\n",
      "115     100409           1     1  0.573328\n",
      "116     100410           0     0  0.480703\n",
      "117     100411           0     1  0.630827\n",
      "118     100412           1     1  0.583875\n",
      "119     100413           0     1  0.610093\n",
      "120     100414           1     1  0.516849\n",
      "121     100415           0     1  0.517351\n",
      "122     100416           0     1  0.514651\n",
      "123     100418           1     1  0.655648\n",
      "124     100419           0     1  0.660079\n",
      "125     100420           1     0  0.455563\n",
      "126     100421           0     1  0.621777\n",
      "127     100423           1     1  0.553954\n",
      "128     100424           1     1  0.508656\n",
      "129     100425           1     1  0.602528\n",
      "130     100426           0     1  0.507877\n",
      "131     100428           0     1  0.596908\n",
      "132     100430           1     1  0.669860\n",
      "133     100431           0     1  0.629521\n",
      "134     100432           0     1  0.553279\n",
      "135     100434           0     1  0.632572\n",
      "136     100435           0     0  0.469482\n",
      "137     100436           0     1  0.599836\n",
      "138     100438           1     1  0.639109\n",
      "139     100439           1     1  0.712734\n",
      "140     100442           1     1  0.691587\n",
      "141     100443           1     0  0.394411\n",
      "142     100444           1     0  0.361019\n",
      "143     100445           1     1  0.516769\n",
      "144     100446           1     1  0.552716\n",
      "145     100447           0     0  0.427801\n",
      "146     100448           0     1  0.630545\n",
      "147     100449           0     0  0.395911\n",
      "148     100452           0     1  0.529869\n",
      "149     100453           1     1  0.635527\n",
      "150     100454           0     1  0.689787\n",
      "151     100455           1     1  0.581277\n",
      "152     100456           0     1  0.612271\n",
      "153     100457           0     1  0.508956\n",
      "154     100458           0     0  0.466507\n",
      "155     100459           0     1  0.629862\n",
      "156     100460           0     1  0.622870\n",
      "157     100461           1     1  0.567801\n",
      "158     100462           0     1  0.666458\n",
      "159     100463           0     1  0.665960\n",
      "160     100469           1     1  0.671848\n",
      "161     100470           0     1  0.562600\n",
      "162     100471           0     1  0.648091\n",
      "163     100473           0     0  0.425958\n",
      "164     100474           1     1  0.872582\n",
      "165     100475           0     1  0.541413\n",
      "166     100476           0     1  0.637551\n",
      "167     100479           0     0  0.467934\n",
      "168     100480           0     1  0.556033\n",
      "169     100481           1     1  0.560774\n",
      "170     100482           0     0  0.355536\n",
      "171     100483           0     1  0.500160\n",
      "172     100484           0     0  0.429038\n",
      "173     100486           0     0  0.288105\n",
      "174     100487           0     0  0.388156\n",
      "175     100488           1     0  0.390076\n",
      "176     100489           0     1  0.658746\n",
      "177     100490           0     0  0.495744\n",
      "178     100492           0     1  0.575244\n",
      "179     100494           1     0  0.380542\n",
      "180     100495           0     1  0.677461\n",
      "181     100496           1     0  0.451432\n",
      "182     100498           1     0  0.479430\n",
      "183     100499           0     1  0.656580\n",
      "184     100500           0     1  0.593923\n",
      "Prediction AUC: 0.4840\n",
      "Prediction Accuracy: 0.4324\n",
      "Prediction Specificity: 0.2672\n",
      "Prediction Sensitivity: 0.7101\n",
      "Prediction Precision: 0.3657\n",
      "the score of the fold number 1 and the type T1wCE: 0.484007996001999\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     1  0.484008  0.432432  0.267241  0.710145  0.365672\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 185/185 [00:51<00:00,  3.56it/s]\n",
      "Fold 2:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0       100022           0     0  0.166792\n",
      "1       100034           0     0  0.421159\n",
      "2       100088           0     0  0.244890\n",
      "3       100091           0     0  0.345206\n",
      "4       100092           0     0  0.153257\n",
      "5       100093           0     0  0.241735\n",
      "6       100093           0     0  0.241735\n",
      "7       100094           0     0  0.128140\n",
      "8       100095           1     0  0.029192\n",
      "9       100098           1     0  0.335260\n",
      "10      100115           1     0  0.413671\n",
      "11      100117           0     0  0.317763\n",
      "12      100120           0     0  0.106692\n",
      "13      100121           0     0  0.051612\n",
      "14      100122           0     0  0.218657\n",
      "15      100122           0     0  0.218657\n",
      "16      100124           1     0  0.250323\n",
      "17      100128           1     0  0.362754\n",
      "18      100128           1     0  0.362754\n",
      "19      100129           0     0  0.242581\n",
      "20      100130           1     0  0.277331\n",
      "21      100131           1     0  0.059804\n",
      "22      100132           0     0  0.147349\n",
      "23      100133           0     0  0.090672\n",
      "24      100134           0     0  0.000047\n",
      "25      100134           0     0  0.000047\n",
      "26      100135           1     0  0.100372\n",
      "27      100136           0     0  0.145639\n",
      "28      100138           1     0  0.352112\n",
      "29      100139           0     0  0.303803\n",
      "30      100140           0     0  0.130025\n",
      "31      100140           0     0  0.130025\n",
      "32      100141           0     0  0.097080\n",
      "33      100143           0     0  0.298042\n",
      "34      100144           0     0  0.181869\n",
      "35      100145           0     0  0.291611\n",
      "36      100146           0     0  0.357483\n",
      "37      100147           0     0  0.327424\n",
      "38      100148           0     0  0.238340\n",
      "39      100148           0     0  0.238340\n",
      "40      100149           0     0  0.441592\n",
      "41      100150           0     0  0.078543\n",
      "42      100150           0     0  0.078543\n",
      "43      100151           0     0  0.185015\n",
      "44      100183           1     0  0.272518\n",
      "45      100197           1     0  0.014697\n",
      "46      100197           1     0  0.014697\n",
      "47      100240           1     0  0.183655\n",
      "48      100244           0     0  0.060391\n",
      "49      100246           0     0  0.144523\n",
      "50      100260           1     0  0.153870\n",
      "51      100262           0     0  0.105186\n",
      "52      100263           1     0  0.170114\n",
      "53      100264           1     0  0.113189\n",
      "54      100267           0     0  0.224039\n",
      "55      100267           0     0  0.224039\n",
      "56      100269           1     0  0.312284\n",
      "57      100282           0     0  0.310962\n",
      "58      100287           1     0  0.226167\n",
      "59      100290           0     0  0.389524\n",
      "60      100294           0     0  0.378344\n",
      "61      100301           0     0  0.395932\n",
      "62      100302           1     0  0.288813\n",
      "63      100304           0     0  0.109868\n",
      "64      100310           1     0  0.145368\n",
      "65      100312           0     0  0.145610\n",
      "66      100312           0     0  0.145610\n",
      "67      100314           0     0  0.402983\n",
      "68      100316           0     0  0.058045\n",
      "69      100336           0     0  0.095137\n",
      "70      100344           1     0  0.197140\n",
      "71      100344           1     0  0.197140\n",
      "72      100350           1     0  0.163109\n",
      "73      100351           1     0  0.050658\n",
      "74      100352           0     0  0.231915\n",
      "75      100352           0     0  0.231915\n",
      "76      100354           0     0  0.176832\n",
      "77      100355           0     0  0.162480\n",
      "78      100356           1     0  0.377584\n",
      "79      100358           1     0  0.080401\n",
      "80      100359           1     0  0.363846\n",
      "81      100360           1     0  0.319516\n",
      "82      100362           0     0  0.363981\n",
      "83      100363           1     0  0.066122\n",
      "84      100367           0     0  0.358464\n",
      "85      100368           0     0  0.129385\n",
      "86      100369           0     0  0.075190\n",
      "87      100370           0     0  0.218029\n",
      "88      100371           1     0  0.237306\n",
      "89      100373           0     0  0.095107\n",
      "90      100375           0     0  0.081050\n",
      "91      100376           0     0  0.171810\n",
      "92      100378           1     0  0.200549\n",
      "93      100379           0     0  0.281292\n",
      "94      100380           1     0  0.289302\n",
      "95      100381           1     0  0.130387\n",
      "96      100384           0     0  0.362718\n",
      "97      100385           0     0  0.231710\n",
      "98      100387           0     0  0.367066\n",
      "99      100388           1     0  0.035954\n",
      "100     100390           0     0  0.200618\n",
      "101     100391           0     0  0.287700\n",
      "102     100392           1     0  0.160833\n",
      "103     100393           0     0  0.217302\n",
      "104     100395           1     0  0.132684\n",
      "105     100398           0     0  0.134622\n",
      "106     100399           1     0  0.357067\n",
      "107     100401           1     0  0.386363\n",
      "108     100402           0     0  0.418071\n",
      "109     100403           1     0  0.247136\n",
      "110     100404           1     0  0.200868\n",
      "111     100405           0     0  0.127088\n",
      "112     100406           1     0  0.279284\n",
      "113     100407           1     0  0.469933\n",
      "114     100408           0     0  0.291628\n",
      "115     100409           1     0  0.156201\n",
      "116     100410           0     0  0.189838\n",
      "117     100411           0     0  0.321338\n",
      "118     100412           1     0  0.271398\n",
      "119     100413           0     0  0.344232\n",
      "120     100414           1     0  0.247360\n",
      "121     100415           0     0  0.285594\n",
      "122     100416           0     0  0.117870\n",
      "123     100418           1     0  0.271911\n",
      "124     100419           0     0  0.262762\n",
      "125     100420           1     0  0.090731\n",
      "126     100421           0     0  0.314560\n",
      "127     100423           1     0  0.197339\n",
      "128     100424           1     0  0.187257\n",
      "129     100425           1     0  0.181458\n",
      "130     100426           0     0  0.087140\n",
      "131     100428           0     0  0.246332\n",
      "132     100430           1     0  0.379475\n",
      "133     100431           0     0  0.363904\n",
      "134     100432           0     0  0.138799\n",
      "135     100434           0     0  0.377537\n",
      "136     100435           0     0  0.117733\n",
      "137     100436           0     0  0.155610\n",
      "138     100438           1     0  0.379047\n",
      "139     100439           1     0  0.407094\n",
      "140     100442           1     0  0.277421\n",
      "141     100443           1     0  0.071370\n",
      "142     100444           1     0  0.080358\n",
      "143     100445           1     0  0.100153\n",
      "144     100446           1     0  0.203729\n",
      "145     100447           0     0  0.154882\n",
      "146     100448           0     0  0.264341\n",
      "147     100449           0     0  0.139740\n",
      "148     100452           0     0  0.261383\n",
      "149     100453           1     0  0.370491\n",
      "150     100454           0     0  0.412440\n",
      "151     100455           1     0  0.251824\n",
      "152     100456           0     0  0.145094\n",
      "153     100457           0     0  0.198241\n",
      "154     100458           0     0  0.077632\n",
      "155     100459           0     0  0.323955\n",
      "156     100460           0     0  0.268965\n",
      "157     100461           1     0  0.281423\n",
      "158     100462           0     0  0.406693\n",
      "159     100463           0     0  0.458553\n",
      "160     100469           1     0  0.385501\n",
      "161     100470           0     0  0.248627\n",
      "162     100471           0     0  0.413497\n",
      "163     100473           0     0  0.106635\n",
      "164     100474           1     0  0.120094\n",
      "165     100475           0     0  0.226950\n",
      "166     100476           0     0  0.271174\n",
      "167     100479           0     0  0.180115\n",
      "168     100480           0     0  0.337804\n",
      "169     100481           1     0  0.385621\n",
      "170     100482           0     0  0.140700\n",
      "171     100483           0     0  0.198651\n",
      "172     100484           0     0  0.180882\n",
      "173     100486           0     0  0.095126\n",
      "174     100487           0     0  0.122071\n",
      "175     100488           1     0  0.152191\n",
      "176     100489           0     0  0.369712\n",
      "177     100490           0     0  0.218710\n",
      "178     100492           0     0  0.435434\n",
      "179     100494           1     0  0.109113\n",
      "180     100495           0     0  0.481505\n",
      "181     100496           1     0  0.085973\n",
      "182     100498           1     0  0.168891\n",
      "183     100499           0     0  0.004548\n",
      "184     100500           0     0  0.373990\n",
      "Prediction AUC: 0.4933\n",
      "Prediction Accuracy: 0.6270\n",
      "Prediction Specificity: 1.0000\n",
      "Prediction Sensitivity: 0.0000\n",
      "Prediction Precision: 0.0000\n",
      "the score of the fold number 2 and the type T1wCE: 0.49325337331334335\n",
      "  model       AUC       acc  spec  sens  prec\n",
      "0     2  0.493253  0.627027   1.0   0.0     0\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 185/185 [00:51<00:00,  3.61it/s]\n",
      "Fold 3:\n",
      "     BraTS21ID  MGMT_value  pred          prob\n",
      "0       100022           0     0  5.510684e-02\n",
      "1       100034           0     0  2.209112e-01\n",
      "2       100088           0     0  1.043878e-01\n",
      "3       100091           0     0  1.605466e-01\n",
      "4       100092           0     0  5.275624e-02\n",
      "5       100093           0     0  1.038401e-01\n",
      "6       100093           0     0  1.038401e-01\n",
      "7       100094           0     0  6.613749e-02\n",
      "8       100095           1     0  6.133833e-03\n",
      "9       100098           1     0  2.351608e-01\n",
      "10      100115           1     0  2.362673e-01\n",
      "11      100117           0     0  1.687540e-01\n",
      "12      100120           0     0  2.263205e-02\n",
      "13      100121           0     0  1.069350e-02\n",
      "14      100122           0     0  9.666080e-02\n",
      "15      100122           0     0  9.666080e-02\n",
      "16      100124           1     0  9.038408e-02\n",
      "17      100128           1     0  1.750648e-01\n",
      "18      100128           1     0  1.750648e-01\n",
      "19      100129           0     0  1.045074e-01\n",
      "20      100130           1     0  1.408423e-01\n",
      "21      100131           1     0  1.607370e-02\n",
      "22      100132           0     0  5.170271e-02\n",
      "23      100133           0     0  2.201739e-02\n",
      "24      100134           0     0  2.645197e-08\n",
      "25      100134           0     0  2.645197e-08\n",
      "26      100135           1     0  4.106784e-02\n",
      "27      100136           0     0  4.346880e-02\n",
      "28      100138           1     0  1.893125e-01\n",
      "29      100139           0     0  1.782328e-01\n",
      "30      100140           0     0  4.255767e-02\n",
      "31      100140           0     0  4.255767e-02\n",
      "32      100141           0     0  3.056609e-02\n",
      "33      100143           0     0  7.950649e-02\n",
      "34      100144           0     0  9.206061e-02\n",
      "35      100145           0     0  1.122456e-01\n",
      "36      100146           0     0  2.101326e-01\n",
      "37      100147           0     0  1.557995e-01\n",
      "38      100148           0     0  1.269809e-01\n",
      "39      100148           0     0  1.269809e-01\n",
      "40      100149           0     0  2.365629e-01\n",
      "41      100150           0     0  1.951039e-02\n",
      "42      100150           0     0  1.951039e-02\n",
      "43      100151           0     0  1.014288e-01\n",
      "44      100183           1     0  1.065087e-01\n",
      "45      100197           1     0  8.866971e-04\n",
      "46      100197           1     0  8.866971e-04\n",
      "47      100240           1     0  7.394151e-02\n",
      "48      100244           0     0  7.179444e-03\n",
      "49      100246           0     0  4.397949e-02\n",
      "50      100260           1     0  5.180589e-02\n",
      "51      100262           0     0  2.920867e-02\n",
      "52      100263           1     0  6.512704e-02\n",
      "53      100264           1     0  4.350151e-02\n",
      "54      100267           0     0  7.916840e-02\n",
      "55      100267           0     0  7.916840e-02\n",
      "56      100269           1     0  1.139828e-01\n",
      "57      100282           0     0  1.842156e-01\n",
      "58      100287           1     0  9.827185e-02\n",
      "59      100290           0     0  2.072566e-01\n",
      "60      100294           0     0  1.987614e-01\n",
      "61      100301           0     0  1.849342e-01\n",
      "62      100302           1     0  1.522056e-01\n",
      "63      100304           0     0  3.459065e-02\n",
      "64      100310           1     0  6.361341e-02\n",
      "65      100312           0     0  4.562607e-02\n",
      "66      100312           0     0  4.562607e-02\n",
      "67      100314           0     0  2.051810e-01\n",
      "68      100316           0     0  1.068002e-02\n",
      "69      100336           0     0  2.396443e-02\n",
      "70      100344           1     0  7.432299e-02\n",
      "71      100344           1     0  7.432299e-02\n",
      "72      100350           1     0  6.132256e-02\n",
      "73      100351           1     0  9.509725e-03\n",
      "74      100352           0     0  8.962236e-02\n",
      "75      100352           0     0  8.962236e-02\n",
      "76      100354           0     0  4.863100e-02\n",
      "77      100355           0     0  5.938117e-02\n",
      "78      100356           1     0  1.781885e-01\n",
      "79      100358           1     0  2.064267e-02\n",
      "80      100359           1     0  2.442057e-01\n",
      "81      100360           1     0  1.498977e-01\n",
      "82      100362           0     0  1.729372e-01\n",
      "83      100363           1     0  1.527533e-02\n",
      "84      100367           0     0  1.752526e-01\n",
      "85      100368           0     0  3.712361e-02\n",
      "86      100369           0     0  1.834059e-02\n",
      "87      100370           0     0  8.277873e-02\n",
      "88      100371           1     0  9.910519e-02\n",
      "89      100373           0     0  3.474764e-02\n",
      "90      100375           0     0  1.966237e-02\n",
      "91      100376           0     0  6.792904e-02\n",
      "92      100378           1     0  8.007355e-02\n",
      "93      100379           0     0  1.166482e-01\n",
      "94      100380           1     0  1.122442e-01\n",
      "95      100381           1     0  4.240234e-02\n",
      "96      100384           0     0  1.615678e-01\n",
      "97      100385           0     0  8.924660e-02\n",
      "98      100387           0     0  1.740244e-01\n",
      "99      100388           1     0  6.339889e-03\n",
      "100     100390           0     0  7.531566e-02\n",
      "101     100391           0     0  1.480065e-01\n",
      "102     100392           1     0  5.212832e-02\n",
      "103     100393           0     0  7.433690e-02\n",
      "104     100395           1     0  3.743194e-02\n",
      "105     100398           0     0  4.209671e-02\n",
      "106     100399           1     0  1.768837e-01\n",
      "107     100401           1     0  1.900688e-01\n",
      "108     100402           0     0  2.609103e-01\n",
      "109     100403           1     0  9.408418e-02\n",
      "110     100404           1     0  8.879635e-02\n",
      "111     100405           0     0  3.937542e-02\n",
      "112     100406           1     0  1.186599e-01\n",
      "113     100407           1     0  2.952516e-01\n",
      "114     100408           0     0  1.182964e-01\n",
      "115     100409           1     0  5.046688e-02\n",
      "116     100410           0     0  7.594253e-02\n",
      "117     100411           0     0  1.425328e-01\n",
      "118     100412           1     0  1.286072e-01\n",
      "119     100413           0     0  1.770959e-01\n",
      "120     100414           1     0  9.914958e-02\n",
      "121     100415           0     0  1.601804e-01\n",
      "122     100416           0     0  3.868709e-02\n",
      "123     100418           1     0  1.151343e-01\n",
      "124     100419           0     0  1.114379e-01\n",
      "125     100420           1     0  2.510352e-02\n",
      "126     100421           0     0  1.584402e-01\n",
      "127     100423           1     0  7.058210e-02\n",
      "128     100424           1     0  7.339421e-02\n",
      "129     100425           1     0  6.115843e-02\n",
      "130     100426           0     0  2.056544e-02\n",
      "131     100428           0     0  9.726822e-02\n",
      "132     100430           1     0  1.880824e-01\n",
      "133     100431           0     0  2.137586e-01\n",
      "134     100432           0     0  4.436731e-02\n",
      "135     100434           0     0  2.291969e-01\n",
      "136     100435           0     0  3.896986e-02\n",
      "137     100436           0     0  5.705573e-02\n",
      "138     100438           1     0  2.091680e-01\n",
      "139     100439           1     0  1.909635e-01\n",
      "140     100442           1     0  1.032638e-01\n",
      "141     100443           1     0  1.862659e-02\n",
      "142     100444           1     0  2.131698e-02\n",
      "143     100445           1     0  2.580130e-02\n",
      "144     100446           1     0  8.969737e-02\n",
      "145     100447           0     0  6.045227e-02\n",
      "146     100448           0     0  1.346614e-01\n",
      "147     100449           0     0  6.129415e-02\n",
      "148     100452           0     0  1.369480e-01\n",
      "149     100453           1     0  1.961003e-01\n",
      "150     100454           0     0  2.293933e-01\n",
      "151     100455           1     0  1.108494e-01\n",
      "152     100456           0     0  4.427669e-02\n",
      "153     100457           0     0  8.514298e-02\n",
      "154     100458           0     0  1.859647e-02\n",
      "155     100459           0     0  1.594422e-01\n",
      "156     100460           0     0  1.287807e-01\n",
      "157     100461           1     0  1.309943e-01\n",
      "158     100462           0     0  2.358439e-01\n",
      "159     100463           0     0  2.812776e-01\n",
      "160     100469           1     0  2.039490e-01\n",
      "161     100470           0     0  1.088668e-01\n",
      "162     100471           0     0  2.378304e-01\n",
      "163     100473           0     0  3.406285e-02\n",
      "164     100474           1     0  1.658139e-02\n",
      "165     100475           0     0  9.179301e-02\n",
      "166     100476           0     0  1.131375e-01\n",
      "167     100479           0     0  7.495262e-02\n",
      "168     100480           0     0  1.947034e-01\n",
      "169     100481           1     0  3.092640e-01\n",
      "170     100482           0     0  7.351545e-02\n",
      "171     100483           0     0  8.034855e-02\n",
      "172     100484           0     0  8.986228e-02\n",
      "173     100486           0     0  4.210526e-02\n",
      "174     100487           0     0  5.677212e-02\n",
      "175     100488           1     0  7.549080e-02\n",
      "176     100489           0     0  1.817397e-01\n",
      "177     100490           0     0  1.039718e-01\n",
      "178     100492           0     0  3.621196e-01\n",
      "179     100494           1     0  3.927028e-02\n",
      "180     100495           0     0  2.945050e-01\n",
      "181     100496           1     0  2.671431e-02\n",
      "182     100498           1     0  5.999920e-02\n",
      "183     100499           0     0  9.255615e-05\n",
      "184     100500           0     0  2.257769e-01\n",
      "Prediction AUC: 0.4834\n",
      "Prediction Accuracy: 0.6270\n",
      "Prediction Specificity: 1.0000\n",
      "Prediction Sensitivity: 0.0000\n",
      "Prediction Precision: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the score of the fold number 3 and the type T1wCE: 0.4833833083458271\n",
      "  model       AUC       acc  spec  sens  prec\n",
      "0     3  0.483383  0.627027   1.0   0.0     0\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 185/185 [00:51<00:00,  3.59it/s]\n",
      "Fold 4:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0       100022           0     0  0.441719\n",
      "1       100034           0     1  0.530244\n",
      "2       100088           0     0  0.428063\n",
      "3       100091           0     0  0.470057\n",
      "4       100092           0     0  0.389615\n",
      "5       100093           0     0  0.419943\n",
      "6       100093           0     0  0.419943\n",
      "7       100094           0     0  0.194888\n",
      "8       100095           1     0  0.293126\n",
      "9       100098           1     0  0.482392\n",
      "10      100115           1     1  0.557141\n",
      "11      100117           0     0  0.471496\n",
      "12      100120           0     0  0.487068\n",
      "13      100121           0     0  0.235707\n",
      "14      100122           0     0  0.450138\n",
      "15      100122           0     0  0.450138\n",
      "16      100124           1     0  0.431975\n",
      "17      100128           1     1  0.593424\n",
      "18      100128           1     1  0.593424\n",
      "19      100129           0     1  0.550937\n",
      "20      100130           1     0  0.430753\n",
      "21      100131           1     0  0.251948\n",
      "22      100132           0     0  0.347796\n",
      "23      100133           0     0  0.316536\n",
      "24      100134           0     0  0.045064\n",
      "25      100134           0     0  0.045064\n",
      "26      100135           1     0  0.327958\n",
      "27      100136           0     0  0.363084\n",
      "28      100138           1     0  0.497667\n",
      "29      100139           0     1  0.502790\n",
      "30      100140           0     0  0.314141\n",
      "31      100140           0     0  0.314141\n",
      "32      100141           0     0  0.331693\n",
      "33      100143           0     0  0.483355\n",
      "34      100144           0     0  0.449710\n",
      "35      100145           0     0  0.471130\n",
      "36      100146           0     0  0.472652\n",
      "37      100147           0     0  0.478404\n",
      "38      100148           0     0  0.398575\n",
      "39      100148           0     0  0.398575\n",
      "40      100149           0     1  0.563153\n",
      "41      100150           0     0  0.318486\n",
      "42      100150           0     0  0.318486\n",
      "43      100151           0     0  0.405156\n",
      "44      100183           1     1  0.543288\n",
      "45      100197           1     0  0.248303\n",
      "46      100197           1     0  0.248303\n",
      "47      100240           1     0  0.381758\n",
      "48      100244           0     0  0.341415\n",
      "49      100246           0     0  0.333222\n",
      "50      100260           1     0  0.410370\n",
      "51      100262           0     0  0.317412\n",
      "52      100263           1     0  0.373764\n",
      "53      100264           1     0  0.440008\n",
      "54      100267           0     0  0.460018\n",
      "55      100267           0     0  0.460018\n",
      "56      100269           1     0  0.474895\n",
      "57      100282           0     1  0.537490\n",
      "58      100287           1     0  0.455845\n",
      "59      100290           0     1  0.513216\n",
      "60      100294           0     1  0.518843\n",
      "61      100301           0     0  0.468904\n",
      "62      100302           1     0  0.447835\n",
      "63      100304           0     0  0.319404\n",
      "64      100310           1     0  0.408189\n",
      "65      100312           0     0  0.338460\n",
      "66      100312           0     0  0.338460\n",
      "67      100314           0     1  0.521985\n",
      "68      100316           0     0  0.265636\n",
      "69      100336           0     0  0.301350\n",
      "70      100344           1     0  0.408797\n",
      "71      100344           1     0  0.408797\n",
      "72      100350           1     0  0.365470\n",
      "73      100351           1     0  0.272145\n",
      "74      100352           0     0  0.416604\n",
      "75      100352           0     0  0.416604\n",
      "76      100354           0     0  0.394283\n",
      "77      100355           0     0  0.361687\n",
      "78      100356           1     1  0.502543\n",
      "79      100358           1     0  0.258151\n",
      "80      100359           1     1  0.561201\n",
      "81      100360           1     0  0.445923\n",
      "82      100362           0     0  0.498675\n",
      "83      100363           1     0  0.222909\n",
      "84      100367           0     0  0.485381\n",
      "85      100368           0     0  0.332916\n",
      "86      100369           0     0  0.262920\n",
      "87      100370           0     0  0.385264\n",
      "88      100371           1     0  0.363485\n",
      "89      100373           0     0  0.369990\n",
      "90      100375           0     0  0.307349\n",
      "91      100376           0     0  0.350394\n",
      "92      100378           1     0  0.380025\n",
      "93      100379           0     0  0.446732\n",
      "94      100380           1     0  0.432001\n",
      "95      100381           1     0  0.357945\n",
      "96      100384           0     0  0.489056\n",
      "97      100385           0     0  0.428305\n",
      "98      100387           0     1  0.504346\n",
      "99      100388           1     0  0.323401\n",
      "100     100390           0     0  0.383771\n",
      "101     100391           0     0  0.461860\n",
      "102     100392           1     0  0.399768\n",
      "103     100393           0     0  0.461848\n",
      "104     100395           1     0  0.326208\n",
      "105     100398           0     0  0.364574\n",
      "106     100399           1     0  0.495713\n",
      "107     100401           1     1  0.540865\n",
      "108     100402           0     1  0.559534\n",
      "109     100403           1     0  0.428158\n",
      "110     100404           1     0  0.384253\n",
      "111     100405           0     0  0.346705\n",
      "112     100406           1     0  0.450614\n",
      "113     100407           1     1  0.561756\n",
      "114     100408           0     0  0.480222\n",
      "115     100409           1     0  0.396050\n",
      "116     100410           0     0  0.363878\n",
      "117     100411           0     0  0.440807\n",
      "118     100412           1     0  0.376805\n",
      "119     100413           0     0  0.485141\n",
      "120     100414           1     0  0.454648\n",
      "121     100415           0     0  0.499699\n",
      "122     100416           0     0  0.377847\n",
      "123     100418           1     0  0.445540\n",
      "124     100419           0     0  0.432453\n",
      "125     100420           1     0  0.316067\n",
      "126     100421           0     0  0.482352\n",
      "127     100423           1     1  0.606589\n",
      "128     100424           1     0  0.376256\n",
      "129     100425           1     0  0.394138\n",
      "130     100426           0     0  0.280081\n",
      "131     100428           0     0  0.411228\n",
      "132     100430           1     0  0.467746\n",
      "133     100431           0     1  0.544434\n",
      "134     100432           0     0  0.420107\n",
      "135     100434           0     1  0.591197\n",
      "136     100435           0     0  0.323893\n",
      "137     100436           0     0  0.383446\n",
      "138     100438           1     1  0.520592\n",
      "139     100439           1     1  0.547227\n",
      "140     100442           1     0  0.430554\n",
      "141     100443           1     0  0.237383\n",
      "142     100444           1     0  0.262133\n",
      "143     100445           1     0  0.327810\n",
      "144     100446           1     0  0.427310\n",
      "145     100447           0     0  0.392285\n",
      "146     100448           0     0  0.457769\n",
      "147     100449           0     0  0.432388\n",
      "148     100452           0     0  0.320651\n",
      "149     100453           1     0  0.497033\n",
      "150     100454           0     1  0.548923\n",
      "151     100455           1     0  0.432075\n",
      "152     100456           0     0  0.392156\n",
      "153     100457           0     0  0.415485\n",
      "154     100458           0     0  0.269632\n",
      "155     100459           0     0  0.482938\n",
      "156     100460           0     0  0.478941\n",
      "157     100461           1     0  0.460797\n",
      "158     100462           0     1  0.524767\n",
      "159     100463           0     1  0.573350\n",
      "160     100469           1     1  0.521220\n",
      "161     100470           0     0  0.416937\n",
      "162     100471           0     1  0.550124\n",
      "163     100473           0     0  0.318494\n",
      "164     100474           1     0  0.356001\n",
      "165     100475           0     0  0.407778\n",
      "166     100476           0     0  0.434053\n",
      "167     100479           0     0  0.435094\n",
      "168     100480           0     0  0.469261\n",
      "169     100481           1     0  0.436452\n",
      "170     100482           0     0  0.189523\n",
      "171     100483           0     0  0.470000\n",
      "172     100484           0     0  0.231039\n",
      "173     100486           0     0  0.153059\n",
      "174     100487           0     0  0.217956\n",
      "175     100488           1     0  0.250345\n",
      "176     100489           0     1  0.527111\n",
      "177     100490           0     0  0.439095\n",
      "178     100492           0     1  0.606743\n",
      "179     100494           1     0  0.180883\n",
      "180     100495           0     1  0.593955\n",
      "181     100496           1     0  0.291548\n",
      "182     100498           1     1  0.534650\n",
      "183     100499           0     0  0.111573\n",
      "184     100500           0     0  0.498317\n",
      "Prediction AUC: 0.4916\n",
      "Prediction Accuracy: 0.5946\n",
      "Prediction Specificity: 0.8362\n",
      "Prediction Sensitivity: 0.1884\n",
      "Prediction Precision: 0.4062\n",
      "the score of the fold number 4 and the type T1wCE: 0.4916291854072964\n",
      "  model       AUC       acc      spec      sens     prec\n",
      "0     4  0.491629  0.594595  0.836207  0.188406  0.40625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction AUC: 0.4958\r\n",
      "Prediction Accuracy: 0.3730\r\n",
      "Prediction Specificity: 0.2672\r\n",
      "Prediction Sensitivity: 0.5507\r\n",
      "Prediction Precision: 0.3654\r\n",
      "the final socre of the type T1wCE\r\n",
      "0.495752123938031\r\n",
      "  model       AUC       acc     spec     sens      prec\r\n",
      "0   all  0.495752  0.581622  0.82069  0.17971  0.154384\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "the final score is\r\n",
      "0.495752123938031\r\n"
     ]
    }
   ],
   "source": [
    "#125, 377, 464, 493, post 500\n",
    "!python3 ../rsnaresnet10/working/validation.py --models_folder tunisiaai_A --csv_file upenn_train_fold_t1wce.csv --full_set True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 40/40 [00:10<00:00,  3.71it/s]\n",
      "Fold 0:\n",
      "    BraTS21ID  MGMT_value  pred      prob\n",
      "0      100091           0     1  0.952697\n",
      "1      100130           1     1  0.941964\n",
      "2      100132           0     1  0.996879\n",
      "3      100139           0     1  0.864961\n",
      "4      100143           0     1  0.997155\n",
      "5      100148           0     1  0.959713\n",
      "6      100148           0     1  0.959713\n",
      "7      100197           1     1  0.999993\n",
      "8      100260           1     1  0.995795\n",
      "9      100263           1     1  0.993729\n",
      "10     100301           0     1  0.884774\n",
      "11     100304           0     1  0.997142\n",
      "12     100312           0     1  0.995393\n",
      "13     100316           0     1  0.999213\n",
      "14     100354           0     1  0.997546\n",
      "15     100359           1     1  0.921186\n",
      "16     100360           1     1  0.966240\n",
      "17     100363           1     1  0.998141\n",
      "18     100367           0     1  0.922244\n",
      "19     100369           0     1  0.997623\n",
      "20     100375           0     1  0.997395\n",
      "21     100391           0     1  0.785796\n",
      "22     100395           1     1  0.997214\n",
      "23     100398           0     1  0.992494\n",
      "24     100401           1     1  0.978685\n",
      "25     100411           0     1  0.963483\n",
      "26     100413           0     1  0.930749\n",
      "27     100414           1     1  0.988097\n",
      "28     100419           0     1  0.892803\n",
      "29     100424           1     1  0.987781\n",
      "30     100446           1     1  0.986983\n",
      "31     100455           1     1  0.981252\n",
      "32     100457           0     1  0.986942\n",
      "33     100458           0     1  0.998715\n",
      "34     100463           0     1  0.872134\n",
      "35     100470           0     1  0.978168\n",
      "36     100486           0     1  0.946174\n",
      "37     100494           1     1  0.929350\n",
      "38     100496           1     1  0.996301\n",
      "39     100499           0     1  1.000000\n",
      "Prediction AUC: 0.5600\n",
      "Prediction Accuracy: 0.3750\n",
      "Prediction Specificity: 0.0000\n",
      "Prediction Sensitivity: 1.0000\n",
      "Prediction Precision: 0.3750\n",
      "the score of the fold number 0 and the type T1wCE: 0.56\n",
      "  model   AUC    acc  spec  sens   prec\n",
      "0     0  0.56  0.375   0.0   1.0  0.375\n",
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 36/36 [00:09<00:00,  3.65it/s]\n",
      "Fold 1:\n",
      "    BraTS21ID  MGMT_value  pred      prob\n",
      "0      100092           0     1  0.549978\n",
      "1      100093           0     1  0.561284\n",
      "2      100095           1     1  0.573849\n",
      "3      100120           0     1  0.548682\n",
      "4      100128           1     1  0.585757\n",
      "5      100129           0     1  0.574434\n",
      "6      100131           1     1  0.520084\n",
      "7      100138           1     1  0.584596\n",
      "8      100145           0     1  0.581994\n",
      "9      100147           0     1  0.579229\n",
      "10     100151           0     1  0.594984\n",
      "11     100197           1     1  0.523134\n",
      "12     100262           0     1  0.545164\n",
      "13     100269           1     1  0.591361\n",
      "14     100282           0     1  0.580955\n",
      "15     100294           0     1  0.586951\n",
      "16     100352           0     1  0.568139\n",
      "17     100355           0     1  0.548814\n",
      "18     100373           0     1  0.569515\n",
      "19     100381           1     1  0.542853\n",
      "20     100387           0     1  0.589034\n",
      "21     100388           1     1  0.501280\n",
      "22     100390           0     1  0.556531\n",
      "23     100393           0     1  0.584009\n",
      "24     100399           1     1  0.584136\n",
      "25     100408           0     1  0.584395\n",
      "26     100409           1     1  0.556031\n",
      "27     100410           0     1  0.551688\n",
      "28     100420           1     1  0.533896\n",
      "29     100443           1     1  0.518802\n",
      "30     100444           1     1  0.519328\n",
      "31     100448           0     1  0.584953\n",
      "32     100449           0     1  0.579357\n",
      "33     100476           0     1  0.575401\n",
      "34     100482           0     1  0.569715\n",
      "35     100490           0     1  0.555614\n",
      "Prediction AUC: 0.3211\n",
      "Prediction Accuracy: 0.3611\n",
      "Prediction Specificity: 0.0000\n",
      "Prediction Sensitivity: 1.0000\n",
      "Prediction Precision: 0.3611\n",
      "the score of the fold number 1 and the type T1wCE: 0.3210702341137124\n",
      "  model      AUC       acc  spec  sens      prec\n",
      "0     1  0.32107  0.361111   0.0   1.0  0.361111\n",
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 32/32 [00:09<00:00,  3.55it/s]\n",
      "Fold 2:\n",
      "    BraTS21ID  MGMT_value  pred      prob\n",
      "0      100022           0     0  0.054362\n",
      "1      100098           1     1  0.661188\n",
      "2      100128           1     0  0.303361\n",
      "3      100134           0     1  0.989431\n",
      "4      100140           0     0  0.182175\n",
      "5      100141           0     0  0.376117\n",
      "6      100150           0     0  0.472395\n",
      "7      100240           1     0  0.396894\n",
      "8      100244           0     0  0.178074\n",
      "9      100246           0     0  0.142611\n",
      "10     100264           1     1  0.526365\n",
      "11     100302           1     0  0.293940\n",
      "12     100336           0     0  0.372058\n",
      "13     100344           1     0  0.224225\n",
      "14     100356           1     0  0.214957\n",
      "15     100370           0     0  0.184413\n",
      "16     100403           1     0  0.266549\n",
      "17     100418           1     0  0.479315\n",
      "18     100426           0     0  0.209799\n",
      "19     100428           0     0  0.344821\n",
      "20     100434           0     0  0.353015\n",
      "21     100435           0     0  0.255516\n",
      "22     100447           0     0  0.358662\n",
      "23     100453           1     0  0.366048\n",
      "24     100456           0     0  0.139714\n",
      "25     100471           0     0  0.251107\n",
      "26     100473           0     0  0.369034\n",
      "27     100483           0     0  0.356249\n",
      "28     100484           0     0  0.342947\n",
      "29     100489           0     0  0.295527\n",
      "30     100495           0     0  0.289507\n",
      "31     100498           1     0  0.435733\n",
      "Prediction AUC: 0.6840\n",
      "Prediction Accuracy: 0.6875\n",
      "Prediction Specificity: 0.9524\n",
      "Prediction Sensitivity: 0.1818\n",
      "Prediction Precision: 0.6667\n",
      "the score of the fold number 2 and the type T1wCE: 0.683982683982684\n",
      "  model       AUC     acc      spec      sens      prec\n",
      "0     2  0.683983  0.6875  0.952381  0.181818  0.666667\n",
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 38/38 [00:10<00:00,  3.72it/s]\n",
      "Fold 3:\n",
      "    BraTS21ID  MGMT_value  pred      prob\n",
      "0      100094           0     0  0.363164\n",
      "1      100117           0     0  0.485051\n",
      "2      100121           0     0  0.287441\n",
      "3      100133           0     0  0.374529\n",
      "4      100134           0     0  0.459992\n",
      "5      100135           1     1  0.622796\n",
      "6      100146           0     0  0.228431\n",
      "7      100150           0     0  0.440161\n",
      "8      100183           1     1  0.562537\n",
      "9      100267           0     0  0.438536\n",
      "10     100267           0     0  0.438536\n",
      "11     100312           0     0  0.346167\n",
      "12     100350           1     0  0.353570\n",
      "13     100351           1     0  0.397883\n",
      "14     100352           0     0  0.408904\n",
      "15     100362           0     1  0.504537\n",
      "16     100376           0     0  0.380176\n",
      "17     100378           1     0  0.373139\n",
      "18     100379           0     0  0.445089\n",
      "19     100380           1     0  0.450827\n",
      "20     100384           0     1  0.508143\n",
      "21     100385           0     0  0.151235\n",
      "22     100392           1     1  0.574408\n",
      "23     100402           0     1  0.571011\n",
      "24     100405           0     0  0.417156\n",
      "25     100416           0     0  0.401998\n",
      "26     100430           1     1  0.566787\n",
      "27     100431           0     0  0.254118\n",
      "28     100436           0     0  0.390294\n",
      "29     100439           1     1  0.623294\n",
      "30     100442           1     0  0.453473\n",
      "31     100445           1     1  0.568986\n",
      "32     100460           0     0  0.308071\n",
      "33     100461           1     0  0.393501\n",
      "34     100462           0     1  0.561880\n",
      "35     100469           1     1  0.552588\n",
      "36     100475           0     0  0.442066\n",
      "37     100481           1     1  0.532300\n",
      "Prediction AUC: 0.7560\n",
      "Prediction Accuracy: 0.7368\n",
      "Prediction Specificity: 0.8333\n",
      "Prediction Sensitivity: 0.5714\n",
      "Prediction Precision: 0.6667\n",
      "the score of the fold number 3 and the type T1wCE: 0.7559523809523809\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     3  0.755952  0.736842  0.833333  0.571429  0.666667\n",
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 39/39 [00:10<00:00,  3.75it/s]\n",
      "Fold 4:\n",
      "    BraTS21ID  MGMT_value  pred      prob\n",
      "0      100034           0     0  0.443534\n",
      "1      100088           0     0  0.443175\n",
      "2      100093           0     0  0.416755\n",
      "3      100115           1     0  0.431773\n",
      "4      100122           0     0  0.305437\n",
      "5      100122           0     0  0.305437\n",
      "6      100124           1     0  0.437637\n",
      "7      100136           0     0  0.311247\n",
      "8      100140           0     0  0.267926\n",
      "9      100144           0     0  0.340242\n",
      "10     100149           0     0  0.402781\n",
      "11     100287           1     0  0.389037\n",
      "12     100290           0     0  0.373970\n",
      "13     100310           1     0  0.459248\n",
      "14     100314           0     0  0.412257\n",
      "15     100344           1     0  0.416822\n",
      "16     100358           1     0  0.392732\n",
      "17     100368           0     0  0.257472\n",
      "18     100371           1     0  0.394053\n",
      "19     100404           1     0  0.397945\n",
      "20     100406           1     0  0.353295\n",
      "21     100407           1     0  0.427117\n",
      "22     100412           1     0  0.405045\n",
      "23     100415           0     0  0.331539\n",
      "24     100421           0     0  0.399162\n",
      "25     100423           1     0  0.426651\n",
      "26     100425           1     1  0.565592\n",
      "27     100432           0     0  0.347296\n",
      "28     100438           1     0  0.408095\n",
      "29     100452           0     0  0.360663\n",
      "30     100454           0     0  0.410148\n",
      "31     100459           0     0  0.355006\n",
      "32     100474           1     0  0.407250\n",
      "33     100479           0     0  0.378586\n",
      "34     100480           0     0  0.398934\n",
      "35     100487           0     0  0.276316\n",
      "36     100488           1     0  0.481477\n",
      "37     100492           0     0  0.483803\n",
      "38     100500           0     0  0.308481\n",
      "Prediction AUC: 0.7717\n",
      "Prediction Accuracy: 0.6154\n",
      "Prediction Specificity: 1.0000\n",
      "Prediction Sensitivity: 0.0625\n",
      "Prediction Precision: 1.0000\n",
      "the score of the fold number 4 and the type T1wCE: 0.7717391304347826\n",
      "  model       AUC       acc  spec    sens  prec\n",
      "0     4  0.771739  0.615385   1.0  0.0625   1.0\n",
      "Prediction AUC: 0.5756\n",
      "Prediction Accuracy: 0.5514\n",
      "Prediction Specificity: 0.5431\n",
      "Prediction Sensitivity: 0.5652\n",
      "Prediction Precision: 0.4239\n",
      "Prediction AUC: 0.5756\n",
      "Prediction Accuracy: 0.5514\n",
      "Prediction Specificity: 0.5431\n",
      "Prediction Sensitivity: 0.5652\n",
      "Prediction Precision: 0.4239\n",
      "the final socre of the type T1wCE\n",
      "0.5755872063968016\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0   all  0.575587  0.551351  0.543103  0.565217  0.423913\n",
      "\n",
      "\n",
      "\n",
      "the final score is\n",
      "0.5755872063968016\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/validation.py --models_folder tunisiaai_B --csv_file upenn_train_fold_t1wce.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 40/40 [00:10<00:00,  3.66it/s]\n",
      "Fold 0:\n",
      "    BraTS21ID  MGMT_value  pred      prob\n",
      "0      100091           0     1  0.952697\n",
      "1      100130           1     1  0.941964\n",
      "2      100132           0     1  0.996879\n",
      "3      100139           0     1  0.864961\n",
      "4      100143           0     1  0.997155\n",
      "5      100148           0     1  0.959713\n",
      "6      100148           0     1  0.959713\n",
      "7      100197           1     1  0.999993\n",
      "8      100260           1     1  0.995795\n",
      "9      100263           1     1  0.993729\n",
      "10     100301           0     1  0.884774\n",
      "11     100304           0     1  0.997142\n",
      "12     100312           0     1  0.995393\n",
      "13     100316           0     1  0.999213\n",
      "14     100354           0     1  0.997546\n",
      "15     100359           1     1  0.921186\n",
      "16     100360           1     1  0.966240\n",
      "17     100363           1     1  0.998141\n",
      "18     100367           0     1  0.922244\n",
      "19     100369           0     1  0.997623\n",
      "20     100375           0     1  0.997395\n",
      "21     100391           0     1  0.785796\n",
      "22     100395           1     1  0.997214\n",
      "23     100398           0     1  0.992494\n",
      "24     100401           1     1  0.978685\n",
      "25     100411           0     1  0.963483\n",
      "26     100413           0     1  0.930749\n",
      "27     100414           1     1  0.988097\n",
      "28     100419           0     1  0.892803\n",
      "29     100424           1     1  0.987781\n",
      "30     100446           1     1  0.986983\n",
      "31     100455           1     1  0.981252\n",
      "32     100457           0     1  0.986942\n",
      "33     100458           0     1  0.998715\n",
      "34     100463           0     1  0.872134\n",
      "35     100470           0     1  0.978168\n",
      "36     100486           0     1  0.946174\n",
      "37     100494           1     1  0.929350\n",
      "38     100496           1     1  0.996301\n",
      "39     100499           0     1  1.000000\n",
      "Prediction AUC: 0.5600\n",
      "Prediction Accuracy: 0.3750\n",
      "Prediction Specificity: 0.0000\n",
      "Prediction Sensitivity: 1.0000\n",
      "Prediction Precision: 0.3750\n",
      "the score of the fold number 0 and the type T1wCE: 0.56\n",
      "  model   AUC    acc  spec  sens   prec\n",
      "0     0  0.56  0.375   0.0   1.0  0.375\n",
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 36/36 [00:10<00:00,  3.59it/s]\n",
      "Fold 1:\n",
      "    BraTS21ID  MGMT_value  pred      prob\n",
      "0      100092           0     1  0.549978\n",
      "1      100093           0     1  0.561284\n",
      "2      100095           1     1  0.573849\n",
      "3      100120           0     1  0.548682\n",
      "4      100128           1     1  0.585757\n",
      "5      100129           0     1  0.574434\n",
      "6      100131           1     1  0.520084\n",
      "7      100138           1     1  0.584596\n",
      "8      100145           0     1  0.581994\n",
      "9      100147           0     1  0.579229\n",
      "10     100151           0     1  0.594984\n",
      "11     100197           1     1  0.523134\n",
      "12     100262           0     1  0.545164\n",
      "13     100269           1     1  0.591361\n",
      "14     100282           0     1  0.580955\n",
      "15     100294           0     1  0.586951\n",
      "16     100352           0     1  0.568139\n",
      "17     100355           0     1  0.548814\n",
      "18     100373           0     1  0.569515\n",
      "19     100381           1     1  0.542853\n",
      "20     100387           0     1  0.589034\n",
      "21     100388           1     1  0.501280\n",
      "22     100390           0     1  0.556531\n",
      "23     100393           0     1  0.584009\n",
      "24     100399           1     1  0.584136\n",
      "25     100408           0     1  0.584395\n",
      "26     100409           1     1  0.556031\n",
      "27     100410           0     1  0.551688\n",
      "28     100420           1     1  0.533896\n",
      "29     100443           1     1  0.518802\n",
      "30     100444           1     1  0.519328\n",
      "31     100448           0     1  0.584953\n",
      "32     100449           0     1  0.579357\n",
      "33     100476           0     1  0.575401\n",
      "34     100482           0     1  0.569715\n",
      "35     100490           0     1  0.555614\n",
      "Prediction AUC: 0.3211\n",
      "Prediction Accuracy: 0.3611\n",
      "Prediction Specificity: 0.0000\n",
      "Prediction Sensitivity: 1.0000\n",
      "Prediction Precision: 0.3611\n",
      "the score of the fold number 1 and the type T1wCE: 0.3210702341137124\n",
      "  model      AUC       acc  spec  sens      prec\n",
      "0     1  0.32107  0.361111   0.0   1.0  0.361111\n",
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 32/32 [00:08<00:00,  3.59it/s]\n",
      "Fold 2:\n",
      "    BraTS21ID  MGMT_value  pred      prob\n",
      "0      100022           0     0  0.054362\n",
      "1      100098           1     1  0.661188\n",
      "2      100128           1     0  0.303361\n",
      "3      100134           0     1  0.989431\n",
      "4      100140           0     0  0.182175\n",
      "5      100141           0     0  0.376117\n",
      "6      100150           0     0  0.472395\n",
      "7      100240           1     0  0.396894\n",
      "8      100244           0     0  0.178074\n",
      "9      100246           0     0  0.142611\n",
      "10     100264           1     1  0.526365\n",
      "11     100302           1     0  0.293940\n",
      "12     100336           0     0  0.372058\n",
      "13     100344           1     0  0.224225\n",
      "14     100356           1     0  0.214957\n",
      "15     100370           0     0  0.184413\n",
      "16     100403           1     0  0.266549\n",
      "17     100418           1     0  0.479315\n",
      "18     100426           0     0  0.209799\n",
      "19     100428           0     0  0.344821\n",
      "20     100434           0     0  0.353015\n",
      "21     100435           0     0  0.255516\n",
      "22     100447           0     0  0.358662\n",
      "23     100453           1     0  0.366048\n",
      "24     100456           0     0  0.139714\n",
      "25     100471           0     0  0.251107\n",
      "26     100473           0     0  0.369034\n",
      "27     100483           0     0  0.356249\n",
      "28     100484           0     0  0.342947\n",
      "29     100489           0     0  0.295527\n",
      "30     100495           0     0  0.289507\n",
      "31     100498           1     0  0.435733\n",
      "Prediction AUC: 0.6840\n",
      "Prediction Accuracy: 0.6875\n",
      "Prediction Specificity: 0.9524\n",
      "Prediction Sensitivity: 0.1818\n",
      "Prediction Precision: 0.6667\n",
      "the score of the fold number 2 and the type T1wCE: 0.683982683982684\n",
      "  model       AUC     acc      spec      sens      prec\n",
      "0     2  0.683983  0.6875  0.952381  0.181818  0.666667\n",
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 38/38 [00:10<00:00,  3.72it/s]\n",
      "Fold 3:\n",
      "    BraTS21ID  MGMT_value  pred      prob\n",
      "0      100094           0     0  0.363164\n",
      "1      100117           0     0  0.485051\n",
      "2      100121           0     0  0.287441\n",
      "3      100133           0     0  0.374529\n",
      "4      100134           0     0  0.459992\n",
      "5      100135           1     1  0.622796\n",
      "6      100146           0     0  0.228431\n",
      "7      100150           0     0  0.440161\n",
      "8      100183           1     1  0.562537\n",
      "9      100267           0     0  0.438536\n",
      "10     100267           0     0  0.438536\n",
      "11     100312           0     0  0.346167\n",
      "12     100350           1     0  0.353570\n",
      "13     100351           1     0  0.397883\n",
      "14     100352           0     0  0.408904\n",
      "15     100362           0     1  0.504537\n",
      "16     100376           0     0  0.380176\n",
      "17     100378           1     0  0.373139\n",
      "18     100379           0     0  0.445089\n",
      "19     100380           1     0  0.450827\n",
      "20     100384           0     1  0.508143\n",
      "21     100385           0     0  0.151235\n",
      "22     100392           1     1  0.574408\n",
      "23     100402           0     1  0.571011\n",
      "24     100405           0     0  0.417156\n",
      "25     100416           0     0  0.401998\n",
      "26     100430           1     1  0.566787\n",
      "27     100431           0     0  0.254118\n",
      "28     100436           0     0  0.390294\n",
      "29     100439           1     1  0.623294\n",
      "30     100442           1     0  0.453473\n",
      "31     100445           1     1  0.568986\n",
      "32     100460           0     0  0.308071\n",
      "33     100461           1     0  0.393501\n",
      "34     100462           0     1  0.561880\n",
      "35     100469           1     1  0.552588\n",
      "36     100475           0     0  0.442066\n",
      "37     100481           1     1  0.532300\n",
      "Prediction AUC: 0.7560\n",
      "Prediction Accuracy: 0.7368\n",
      "Prediction Specificity: 0.8333\n",
      "Prediction Sensitivity: 0.5714\n",
      "Prediction Precision: 0.6667\n",
      "the score of the fold number 3 and the type T1wCE: 0.7559523809523809\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     3  0.755952  0.736842  0.833333  0.571429  0.666667\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 39/39 [00:10<00:00,  3.79it/s]\n",
      "Fold 4:\n",
      "    BraTS21ID  MGMT_value  pred      prob\n",
      "0      100034           0     0  0.443534\n",
      "1      100088           0     0  0.443175\n",
      "2      100093           0     0  0.416755\n",
      "3      100115           1     0  0.431773\n",
      "4      100122           0     0  0.305437\n",
      "5      100122           0     0  0.305437\n",
      "6      100124           1     0  0.437637\n",
      "7      100136           0     0  0.311247\n",
      "8      100140           0     0  0.267926\n",
      "9      100144           0     0  0.340242\n",
      "10     100149           0     0  0.402781\n",
      "11     100287           1     0  0.389037\n",
      "12     100290           0     0  0.373970\n",
      "13     100310           1     0  0.459248\n",
      "14     100314           0     0  0.412257\n",
      "15     100344           1     0  0.416822\n",
      "16     100358           1     0  0.392732\n",
      "17     100368           0     0  0.257472\n",
      "18     100371           1     0  0.394053\n",
      "19     100404           1     0  0.397945\n",
      "20     100406           1     0  0.353295\n",
      "21     100407           1     0  0.427117\n",
      "22     100412           1     0  0.405045\n",
      "23     100415           0     0  0.331539\n",
      "24     100421           0     0  0.399162\n",
      "25     100423           1     0  0.426651\n",
      "26     100425           1     1  0.565592\n",
      "27     100432           0     0  0.347296\n",
      "28     100438           1     0  0.408095\n",
      "29     100452           0     0  0.360663\n",
      "30     100454           0     0  0.410148\n",
      "31     100459           0     0  0.355006\n",
      "32     100474           1     0  0.407250\n",
      "33     100479           0     0  0.378586\n",
      "34     100480           0     0  0.398934\n",
      "35     100487           0     0  0.276316\n",
      "36     100488           1     0  0.481477\n",
      "37     100492           0     0  0.483803\n",
      "38     100500           0     0  0.308481\n",
      "Prediction AUC: 0.7717\n",
      "Prediction Accuracy: 0.6154\n",
      "Prediction Specificity: 1.0000\n",
      "Prediction Sensitivity: 0.0625\n",
      "Prediction Precision: 1.0000\n",
      "the score of the fold number 4 and the type T1wCE: 0.7717391304347826\n",
      "  model       AUC       acc  spec    sens  prec\n",
      "0     4  0.771739  0.615385   1.0  0.0625   1.0\n",
      "Prediction AUC: 0.5756\n",
      "Prediction Accuracy: 0.5514\n",
      "Prediction Specificity: 0.5431\n",
      "Prediction Sensitivity: 0.5652\n",
      "Prediction Precision: 0.4239\n",
      "Prediction AUC: 0.5756\n",
      "Prediction Accuracy: 0.5514\n",
      "Prediction Specificity: 0.5431\n",
      "Prediction Sensitivity: 0.5652\n",
      "Prediction Precision: 0.4239\n",
      "the final socre of the type T1wCE\n",
      "0.5755872063968016\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0   all  0.575587  0.551351  0.543103  0.565217  0.423913\n",
      "\n",
      "\n",
      "\n",
      "the final score is\n",
      "0.5755872063968016\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/validation.py --models_folder tunisiaai_B --csv_file upenn_train_fold_t1wce.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 585/585 [01:05<00:00,  8.98it/s]\n",
      "Fold 0:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            0           1     1  0.593086\n",
      "1            2           1     1  0.627343\n",
      "2            3           0     1  0.603141\n",
      "3            5           1     1  0.588373\n",
      "4            6           1     1  0.577472\n",
      "5            8           1     1  0.581511\n",
      "6            9           0     1  0.658364\n",
      "7           11           1     1  0.830363\n",
      "8           12           1     1  0.559150\n",
      "9           14           1     1  0.590464\n",
      "10          17           0     1  0.612659\n",
      "11          18           0     1  0.626219\n",
      "12          19           0     1  0.563840\n",
      "13          20           1     1  0.573405\n",
      "14          21           0     1  0.598406\n",
      "15          22           0     1  0.562067\n",
      "16          24           0     1  0.555749\n",
      "17          25           1     1  0.566412\n",
      "18          26           1     1  0.665934\n",
      "19          28           1     1  0.575078\n",
      "20          30           0     1  0.707434\n",
      "21          31           1     1  0.578549\n",
      "22          32           0     1  0.530748\n",
      "23          33           1     1  0.583853\n",
      "24          35           1     1  0.549088\n",
      "25          36           0     1  0.606475\n",
      "26          43           1     1  0.549327\n",
      "27          44           0     1  0.548145\n",
      "28          45           0     1  0.697236\n",
      "29          46           1     1  0.610430\n",
      "30          48           1     1  0.545864\n",
      "31          49           0     1  0.844325\n",
      "32          52           1     1  0.598444\n",
      "33          53           0     1  0.638470\n",
      "34          54           1     1  0.590171\n",
      "35          56           1     1  0.650239\n",
      "36          58           1     1  0.565164\n",
      "37          59           1     1  0.625144\n",
      "38          60           1     1  0.657449\n",
      "39          61           0     1  0.523776\n",
      "40          62           1     1  0.584370\n",
      "41          63           1     1  0.595462\n",
      "42          64           0     1  0.586775\n",
      "43          66           1     1  0.595773\n",
      "44          68           1     1  0.674732\n",
      "45          70           1     1  0.688277\n",
      "46          71           1     1  0.607872\n",
      "47          72           0     1  0.693168\n",
      "48          74           1     1  0.534104\n",
      "49          77           1     1  0.616572\n",
      "50          78           1     1  0.621502\n",
      "51          81           0     1  0.659580\n",
      "52          84           0     1  0.598006\n",
      "53          85           1     1  0.589149\n",
      "54          87           1     1  0.599624\n",
      "55          88           0     1  0.609003\n",
      "56          89           1     1  0.587048\n",
      "57          90           0     1  0.529885\n",
      "58          94           1     1  0.590088\n",
      "59          95           0     1  0.583224\n",
      "60          96           1     1  0.615210\n",
      "61          97           0     1  0.568854\n",
      "62          98           1     1  0.590858\n",
      "63          99           0     1  0.584016\n",
      "64         100           1     0  0.357209\n",
      "65         102           0     1  0.657659\n",
      "66         104           0     1  0.781938\n",
      "67         105           1     1  0.725614\n",
      "68         106           1     1  0.919354\n",
      "69         107           1     1  0.574353\n",
      "70         108           0     1  0.950270\n",
      "71         109           1     1  0.694899\n",
      "72         110           0     1  0.867681\n",
      "73         111           0     1  0.950032\n",
      "74         112           0     0  0.465253\n",
      "75         113           0     1  0.530925\n",
      "76         116           0     1  0.596296\n",
      "77         117           1     1  0.930984\n",
      "78         120           1     1  0.876037\n",
      "79         121           0     1  0.945350\n",
      "80         122           0     1  0.916691\n",
      "81         123           0     1  0.654342\n",
      "82         124           0     1  0.944435\n",
      "83         128           1     1  0.787119\n",
      "84         130           0     1  0.846986\n",
      "85         132           0     1  0.820529\n",
      "86         133           0     1  0.913318\n",
      "87         134           1     1  0.927637\n",
      "88         136           1     1  0.757835\n",
      "89         137           0     1  0.860900\n",
      "90         138           1     1  0.899493\n",
      "91         139           1     0  0.315211\n",
      "92         140           1     1  0.810826\n",
      "93         142           0     1  0.722631\n",
      "94         143           1     1  0.863048\n",
      "95         144           1     1  0.815165\n",
      "96         146           1     1  0.770329\n",
      "97         147           0     1  0.730708\n",
      "98         148           0     1  0.870952\n",
      "99         149           0     0  0.335006\n",
      "100        150           0     1  0.710724\n",
      "101        151           0     1  0.729930\n",
      "102        154           0     1  0.501268\n",
      "103        155           1     1  0.679265\n",
      "104        156           1     1  0.777344\n",
      "105        157           0     0  0.493304\n",
      "106        158           0     1  0.768895\n",
      "107        159           1     1  0.605509\n",
      "108        160           1     1  0.745333\n",
      "109        162           0     1  0.709327\n",
      "110        165           0     1  0.671220\n",
      "111        166           1     1  0.651830\n",
      "112        167           0     1  0.675196\n",
      "113        169           0     1  0.843466\n",
      "114        170           0     1  0.774215\n",
      "115        171           1     1  0.680401\n",
      "116        172           0     1  0.601538\n",
      "117        176           0     1  0.584777\n",
      "118        177           1     1  0.578981\n",
      "119        178           1     1  0.690469\n",
      "120        183           0     1  0.797474\n",
      "121        184           0     1  0.614980\n",
      "122        185           1     1  0.545437\n",
      "123        186           1     1  0.610084\n",
      "124        187           1     1  0.811047\n",
      "125        188           1     1  0.622676\n",
      "126        191           0     1  0.712219\n",
      "127        192           0     1  0.819151\n",
      "128        193           0     1  0.624014\n",
      "129        194           0     1  0.601750\n",
      "130        195           0     1  0.616990\n",
      "131        196           1     1  0.787449\n",
      "132        197           1     1  0.663471\n",
      "133        199           1     1  0.519231\n",
      "134        201           0     1  0.610689\n",
      "135        203           1     1  0.603196\n",
      "136        204           1     1  0.955477\n",
      "137        206           0     1  0.930814\n",
      "138        209           0     1  0.633779\n",
      "139        210           1     1  0.633051\n",
      "140        211           0     1  0.661877\n",
      "141        212           1     1  0.757898\n",
      "142        214           0     1  0.613686\n",
      "143        216           0     1  0.876365\n",
      "144        217           0     1  0.972084\n",
      "145        218           0     1  0.712495\n",
      "146        219           0     1  0.619945\n",
      "147        220           1     1  0.744135\n",
      "148        221           0     1  0.672122\n",
      "149        222           1     1  0.630345\n",
      "150        227           0     1  0.518818\n",
      "151        228           0     1  0.718287\n",
      "152        230           1     1  0.827515\n",
      "153        231           0     1  0.578842\n",
      "154        233           1     1  0.761378\n",
      "155        234           1     1  0.720774\n",
      "156        235           1     1  0.660543\n",
      "157        236           0     1  0.623265\n",
      "158        237           0     1  0.710452\n",
      "159        238           0     1  0.783811\n",
      "160        239           0     1  0.704822\n",
      "161        240           1     1  0.561075\n",
      "162        241           0     1  0.712463\n",
      "163        242           0     1  0.746735\n",
      "164        243           0     1  0.767638\n",
      "165        245           1     1  0.636793\n",
      "166        246           1     1  0.567614\n",
      "167        247           0     1  0.753206\n",
      "168        249           0     1  0.708448\n",
      "169        250           1     1  0.596224\n",
      "170        251           0     1  0.675132\n",
      "171        253           1     1  0.685134\n",
      "172        254           1     1  0.723861\n",
      "173        258           0     1  0.776087\n",
      "174        259           0     1  0.698607\n",
      "175        260           1     1  0.805574\n",
      "176        261           0     1  0.616499\n",
      "177        262           0     1  0.622377\n",
      "178        263           1     1  0.760490\n",
      "179        266           0     1  0.771561\n",
      "180        267           0     1  0.616609\n",
      "181        269           0     1  0.785398\n",
      "182        270           1     1  0.654409\n",
      "183        271           1     1  0.613872\n",
      "184        273           1     1  0.785996\n",
      "185        274           0     1  0.602516\n",
      "186        275           0     1  0.707297\n",
      "187        280           0     1  0.609223\n",
      "188        281           1     1  0.928669\n",
      "189        282           1     1  0.568670\n",
      "190        283           0     1  0.649177\n",
      "191        284           1     1  0.731863\n",
      "192        285           1     1  0.566351\n",
      "193        286           0     0  0.499631\n",
      "194        288           0     1  0.926550\n",
      "195        289           0     1  0.686605\n",
      "196        290           0     1  0.720894\n",
      "197        291           1     1  0.792940\n",
      "198        293           1     1  0.643633\n",
      "199        294           1     1  0.636425\n",
      "200        296           1     1  0.651997\n",
      "201        297           0     1  0.753600\n",
      "202        298           0     1  0.789185\n",
      "203        299           1     1  0.967221\n",
      "204        300           0     1  0.671888\n",
      "205        301           0     1  0.769915\n",
      "206        303           1     1  0.762258\n",
      "207        304           1     1  0.685538\n",
      "208        305           1     1  0.822770\n",
      "209        306           1     1  0.792989\n",
      "210        308           0     1  0.746597\n",
      "211        309           0     1  0.621456\n",
      "212        310           0     1  0.774900\n",
      "213        311           1     1  0.619131\n",
      "214        312           0     1  0.789362\n",
      "215        313           1     1  0.583011\n",
      "216        314           0     1  0.757212\n",
      "217        316           0     1  0.695794\n",
      "218        317           1     1  0.593955\n",
      "219        318           0     1  0.612871\n",
      "220        320           0     1  0.718844\n",
      "221        321           1     1  0.638148\n",
      "222        322           1     1  0.655438\n",
      "223        324           0     1  0.638341\n",
      "224        325           0     1  0.990438\n",
      "225        327           0     1  0.715522\n",
      "226        328           1     1  0.824093\n",
      "227        329           1     1  0.785135\n",
      "228        331           1     1  0.816061\n",
      "229        332           1     1  0.662528\n",
      "230        334           1     1  0.685606\n",
      "231        336           0     1  0.599602\n",
      "232        338           1     1  0.762029\n",
      "233        339           0     1  0.631404\n",
      "234        340           1     1  0.663962\n",
      "235        341           0     1  0.758062\n",
      "236        343           0     1  0.664566\n",
      "237        344           1     1  0.600528\n",
      "238        346           0     1  0.626992\n",
      "239        347           0     1  0.726704\n",
      "240        348           0     1  0.667105\n",
      "241        349           0     1  0.833383\n",
      "242        350           1     1  0.963969\n",
      "243        351           0     1  0.760800\n",
      "244        352           1     1  0.746255\n",
      "245        353           0     1  0.654768\n",
      "246        356           0     1  0.793615\n",
      "247        359           1     1  0.722641\n",
      "248        360           1     1  0.651606\n",
      "249        364           1     1  0.614398\n",
      "250        366           1     1  0.958618\n",
      "251        367           1     1  0.941468\n",
      "252        369           1     1  0.882591\n",
      "253        370           1     1  0.785520\n",
      "254        371           1     1  0.646086\n",
      "255        373           0     1  0.757642\n",
      "256        376           0     1  0.982901\n",
      "257        377           0     1  0.558022\n",
      "258        378           0     1  0.643848\n",
      "259        379           0     1  0.685414\n",
      "260        380           0     1  0.770084\n",
      "261        382           0     1  0.563477\n",
      "262        383           1     1  0.890750\n",
      "263        386           1     1  0.753587\n",
      "264        387           0     1  0.647328\n",
      "265        388           0     1  0.892277\n",
      "266        389           0     1  0.740484\n",
      "267        390           0     1  0.515277\n",
      "268        391           0     1  0.938392\n",
      "269        392           0     1  0.686025\n",
      "270        395           0     1  0.715495\n",
      "271        397           0     1  0.734231\n",
      "272        399           0     1  0.667339\n",
      "273        400           1     1  0.542116\n",
      "274        401           0     1  0.551159\n",
      "275        402           0     1  0.746184\n",
      "276        403           1     1  0.933480\n",
      "277        404           1     1  0.537944\n",
      "278        405           0     1  0.662523\n",
      "279        406           1     1  0.550526\n",
      "280        407           0     1  0.746781\n",
      "281        408           1     1  0.622986\n",
      "282        409           1     1  0.583534\n",
      "283        410           0     1  0.632207\n",
      "284        412           0     1  0.763966\n",
      "285        413           1     1  0.644787\n",
      "286        414           0     1  0.645601\n",
      "287        416           1     1  0.801396\n",
      "288        417           0     1  0.616720\n",
      "289        418           0     1  0.830865\n",
      "290        419           0     1  0.790067\n",
      "291        421           0     1  0.610130\n",
      "292        423           0     1  0.689127\n",
      "293        425           1     1  0.638959\n",
      "294        426           1     1  0.697976\n",
      "295        429           1     1  0.739432\n",
      "296        430           0     1  0.654574\n",
      "297        431           1     1  0.696075\n",
      "298        432           0     1  0.594131\n",
      "299        433           0     1  0.560675\n",
      "300        436           1     1  0.648205\n",
      "301        440           1     1  0.512525\n",
      "302        441           0     1  0.689587\n",
      "303        442           1     1  0.877571\n",
      "304        443           1     1  0.696914\n",
      "305        444           0     1  0.799449\n",
      "306        445           0     1  0.799558\n",
      "307        446           0     1  0.922060\n",
      "308        449           1     1  0.718260\n",
      "309        451           1     1  0.834116\n",
      "310        452           0     1  0.716070\n",
      "311        454           0     1  0.599469\n",
      "312        455           0     1  0.633787\n",
      "313        456           1     1  0.766493\n",
      "314        457           1     1  0.940690\n",
      "315        459           0     1  0.803326\n",
      "316        464           0     1  0.692223\n",
      "317        466           1     1  0.573268\n",
      "318        468           1     1  0.611461\n",
      "319        469           0     1  0.568450\n",
      "320        470           1     1  0.596987\n",
      "321        472           1     1  0.676871\n",
      "322        477           0     1  0.680452\n",
      "323        478           1     1  0.652897\n",
      "324        479           1     1  0.725871\n",
      "325        480           1     1  0.561126\n",
      "326        481           0     1  0.620124\n",
      "327        483           1     1  0.580657\n",
      "328        485           1     1  0.585368\n",
      "329        488           1     1  0.634541\n",
      "330        491           1     1  0.596701\n",
      "331        493           1     1  0.564082\n",
      "332        494           1     1  0.552609\n",
      "333        495           0     1  0.646445\n",
      "334        496           0     1  0.713047\n",
      "335        498           0     1  0.650207\n",
      "336        499           1     1  0.818506\n",
      "337        500           1     1  0.516173\n",
      "338        501           1     1  0.557685\n",
      "339        502           1     1  0.729634\n",
      "340        504           1     1  0.567629\n",
      "341        505           1     1  0.589946\n",
      "342        506           1     1  0.561348\n",
      "343        507           0     1  0.639664\n",
      "344        510           0     1  0.622592\n",
      "345        511           1     1  0.778432\n",
      "346        512           0     1  0.624665\n",
      "347        513           1     1  0.535854\n",
      "348        514           0     1  0.646245\n",
      "349        516           1     1  0.757878\n",
      "350        517           1     1  0.596006\n",
      "351        518           0     1  0.612731\n",
      "352        519           0     1  0.726578\n",
      "353        520           1     1  0.539823\n",
      "354        523           1     1  0.573985\n",
      "355        524           1     1  0.616982\n",
      "356        525           1     1  0.560603\n",
      "357        526           1     1  0.851128\n",
      "358        528           1     1  0.586540\n",
      "359        529           1     1  0.695653\n",
      "360        530           0     1  0.559312\n",
      "361        532           1     1  0.570274\n",
      "362        533           0     1  0.742430\n",
      "363        537           1     1  0.863640\n",
      "364        538           0     1  0.546352\n",
      "365        539           1     1  0.550243\n",
      "366        540           0     1  0.561559\n",
      "367        542           1     1  0.620453\n",
      "368        543           1     1  0.597694\n",
      "369        544           1     1  0.533174\n",
      "370        545           0     1  0.523551\n",
      "371        547           0     1  0.583201\n",
      "372        548           1     1  0.603944\n",
      "373        549           1     1  0.690512\n",
      "374        550           1     1  0.554306\n",
      "375        551           1     1  0.680032\n",
      "376        552           1     1  0.591203\n",
      "377        554           1     1  0.591666\n",
      "378        555           0     1  0.676438\n",
      "379        556           1     1  0.714510\n",
      "380        557           1     1  0.570507\n",
      "381        558           1     1  0.536188\n",
      "382        559           1     1  0.636401\n",
      "383        561           1     1  0.655908\n",
      "384        563           0     1  0.658106\n",
      "385        564           1     1  0.657500\n",
      "386        565           0     1  0.654461\n",
      "387        567           0     1  0.717943\n",
      "388        568           0     1  0.941701\n",
      "389        569           0     1  0.772076\n",
      "390        570           1     0  0.442800\n",
      "391        571           0     1  0.724539\n",
      "392        572           0     1  0.881527\n",
      "393        574           0     1  0.729105\n",
      "394        575           0     0  0.442636\n",
      "395        576           1     1  0.876587\n",
      "396        577           1     1  0.711304\n",
      "397        578           0     1  0.650360\n",
      "398        579           1     1  0.651499\n",
      "399        581           0     1  0.787276\n",
      "400        582           1     1  0.800376\n",
      "401        583           1     1  0.685261\n",
      "402        584           1     0  0.446594\n",
      "403        586           1     1  0.727799\n",
      "404        587           0     1  0.859937\n",
      "405        588           0     1  0.770732\n",
      "406        589           0     0  0.374621\n",
      "407        590           1     1  0.662175\n",
      "408        591           0     1  0.733204\n",
      "409        593           1     1  0.771712\n",
      "410        594           1     1  0.839938\n",
      "411        596           0     1  0.690239\n",
      "412        597           1     1  0.687431\n",
      "413        598           1     1  0.737905\n",
      "414        599           1     1  0.719690\n",
      "415        601           0     1  0.697937\n",
      "416        602           1     1  0.651300\n",
      "417        604           1     1  0.522447\n",
      "418        605           0     1  0.676929\n",
      "419        606           1     1  0.729053\n",
      "420        607           1     1  0.676194\n",
      "421        608           1     1  0.600711\n",
      "422        610           1     1  0.703191\n",
      "423        611           1     1  0.521571\n",
      "424        612           1     1  0.610586\n",
      "425        613           1     1  0.586765\n",
      "426        615           1     1  0.623975\n",
      "427        616           0     1  0.773224\n",
      "428        618           1     1  0.754751\n",
      "429        619           0     1  0.728655\n",
      "430        620           0     1  0.606769\n",
      "431        621           1     1  0.623755\n",
      "432        622           1     1  0.590015\n",
      "433        623           0     1  0.566114\n",
      "434        624           0     1  0.553459\n",
      "435        625           1     1  0.654526\n",
      "436        626           1     1  0.592525\n",
      "437        628           1     1  0.604360\n",
      "438        630           0     1  0.578294\n",
      "439        631           1     1  0.652768\n",
      "440        636           0     1  0.750033\n",
      "441        638           1     1  0.521041\n",
      "442        639           1     1  0.581570\n",
      "443        640           1     1  0.530721\n",
      "444        641           0     1  0.771600\n",
      "445        642           0     1  0.597664\n",
      "446        645           0     1  0.610147\n",
      "447        646           1     1  0.682683\n",
      "448        649           0     1  0.595701\n",
      "449        650           1     1  0.787534\n",
      "450        651           0     1  0.633804\n",
      "451        652           1     1  0.861306\n",
      "452        654           0     1  0.553000\n",
      "453        655           1     1  0.642412\n",
      "454        656           1     1  0.982564\n",
      "455        657           0     1  0.624300\n",
      "456        658           1     1  0.569979\n",
      "457        659           1     1  0.537983\n",
      "458        661           1     1  0.591380\n",
      "459        663           0     1  0.665676\n",
      "460        667           0     1  0.618495\n",
      "461        668           0     1  0.590589\n",
      "462        674           1     1  0.572731\n",
      "463        675           1     1  0.569215\n",
      "464        676           1     1  0.606632\n",
      "465        677           1     1  0.733260\n",
      "466        679           1     1  0.979250\n",
      "467        680           1     1  0.674873\n",
      "468        682           0     1  0.593932\n",
      "469        683           0     1  0.755369\n",
      "470        684           0     1  0.651548\n",
      "471        685           0     1  0.599860\n",
      "472        686           0     1  0.561528\n",
      "473        687           0     1  0.546290\n",
      "474        688           0     1  0.572648\n",
      "475        690           1     1  0.533824\n",
      "476        691           1     1  0.781323\n",
      "477        692           1     1  0.774862\n",
      "478        693           1     1  0.566983\n",
      "479        694           1     1  0.734941\n",
      "480        697           1     1  0.691096\n",
      "481        698           1     1  0.578468\n",
      "482        703           0     1  0.535472\n",
      "483        704           1     1  0.589511\n",
      "484        705           1     1  0.530373\n",
      "485        706           0     1  0.640477\n",
      "486        707           1     1  0.720434\n",
      "487        708           1     1  0.531362\n",
      "488        709           0     1  0.595392\n",
      "489        714           1     1  0.638060\n",
      "490        715           1     1  0.596815\n",
      "491        716           1     1  0.599576\n",
      "492        718           1     1  0.694368\n",
      "493        723           0     1  0.542990\n",
      "494        724           0     1  0.538265\n",
      "495        725           1     1  0.653696\n",
      "496        727           0     1  0.787437\n",
      "497        728           0     1  0.532071\n",
      "498        729           0     1  0.662176\n",
      "499        730           0     1  0.573428\n",
      "500        731           1     1  0.591983\n",
      "501        732           1     1  0.605050\n",
      "502        733           0     1  0.687798\n",
      "503        734           0     1  0.764476\n",
      "504        735           0     1  0.664010\n",
      "505        736           1     1  0.633465\n",
      "506        737           1     1  0.659042\n",
      "507        739           1     1  0.790508\n",
      "508        740           1     1  0.560111\n",
      "509        742           0     1  0.568195\n",
      "510        744           0     1  0.614603\n",
      "511        746           1     1  0.577097\n",
      "512        747           0     1  0.669960\n",
      "513        750           1     1  0.569263\n",
      "514        751           0     1  0.805645\n",
      "515        753           0     1  0.892169\n",
      "516        756           0     1  0.799524\n",
      "517        757           1     1  0.558634\n",
      "518        758           1     1  0.749713\n",
      "519        759           0     1  0.862898\n",
      "520        760           1     1  0.666517\n",
      "521        764           0     1  0.849020\n",
      "522        765           1     1  0.570261\n",
      "523        767           0     1  0.702986\n",
      "524        768           1     1  0.622818\n",
      "525        772           1     1  0.584319\n",
      "526        773           1     1  0.667867\n",
      "527        774           0     1  0.686319\n",
      "528        775           1     1  0.696770\n",
      "529        777           1     1  0.574516\n",
      "530        778           0     1  0.591020\n",
      "531        780           0     1  0.650987\n",
      "532        781           1     1  0.744396\n",
      "533        782           1     1  0.622848\n",
      "534        784           1     1  0.643275\n",
      "535        787           1     1  0.669852\n",
      "536        788           0     1  0.628934\n",
      "537        789           1     1  0.616267\n",
      "538        791           1     1  0.744738\n",
      "539        792           0     1  0.709870\n",
      "540        793           1     1  0.806120\n",
      "541        794           1     1  0.541017\n",
      "542        795           1     1  0.638229\n",
      "543        796           0     1  0.701381\n",
      "544        797           0     1  0.710650\n",
      "545        799           0     1  0.796342\n",
      "546        800           0     1  0.582647\n",
      "547        801           1     1  0.670020\n",
      "548        802           0     1  0.562149\n",
      "549        803           0     1  0.687209\n",
      "550        804           0     1  0.513292\n",
      "551        805           0     1  0.655928\n",
      "552        806           0     1  0.542548\n",
      "553        807           1     1  0.809594\n",
      "554        808           1     1  0.724519\n",
      "555        809           0     1  0.683649\n",
      "556        810           0     1  0.650559\n",
      "557        811           1     1  0.547984\n",
      "558        814           0     1  0.616168\n",
      "559        816           1     1  0.695276\n",
      "560        818           0     1  0.697742\n",
      "561        819           1     1  0.733934\n",
      "562        820           0     1  0.789449\n",
      "563        823           1     1  0.645094\n",
      "564        824           0     1  0.655049\n",
      "565        828           1     1  0.777768\n",
      "566        830           0     1  0.730792\n",
      "567        834           0     1  0.546999\n",
      "568        836           0     1  0.722756\n",
      "569        837           0     1  0.715184\n",
      "570        838           1     1  0.642534\n",
      "571        839           0     1  0.697868\n",
      "572        840           1     1  0.675903\n",
      "573        998           1     1  0.941702\n",
      "574        999           1     1  0.913578\n",
      "575       1000           1     1  0.571238\n",
      "576       1001           1     1  0.717592\n",
      "577       1002           1     1  0.598313\n",
      "578       1003           1     1  0.796125\n",
      "579       1004           0     1  0.614541\n",
      "580       1005           1     1  0.622517\n",
      "581       1007           1     1  0.897659\n",
      "582       1008           1     1  0.755171\n",
      "583       1009           0     1  0.876737\n",
      "584       1010           0     1  0.562290\n",
      "Prediction AUC: 0.4509\n",
      "Prediction Accuracy: 0.5282\n",
      "Prediction Specificity: 0.0216\n",
      "Prediction Sensitivity: 0.9870\n",
      "Prediction Precision: 0.5270\n",
      "the score of the fold number 0 and the type T1wCE: 0.45088229090994303\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     0  0.450882  0.528205  0.021583  0.986971  0.526957\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 585/585 [01:04<00:00,  9.11it/s]\n",
      "Fold 1:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            0           1     1  0.621191\n",
      "1            2           1     1  0.622528\n",
      "2            3           0     1  0.616566\n",
      "3            5           1     1  0.623523\n",
      "4            6           1     1  0.622303\n",
      "5            8           1     1  0.598224\n",
      "6            9           0     1  0.617102\n",
      "7           11           1     1  0.616388\n",
      "8           12           1     1  0.607089\n",
      "9           14           1     1  0.604489\n",
      "10          17           0     1  0.600605\n",
      "11          18           0     1  0.612962\n",
      "12          19           0     1  0.614770\n",
      "13          20           1     1  0.611683\n",
      "14          21           0     1  0.616368\n",
      "15          22           0     1  0.611378\n",
      "16          24           0     1  0.610336\n",
      "17          25           1     1  0.611362\n",
      "18          26           1     1  0.611822\n",
      "19          28           1     1  0.618061\n",
      "20          30           0     1  0.611962\n",
      "21          31           1     1  0.615857\n",
      "22          32           0     1  0.610123\n",
      "23          33           1     1  0.610713\n",
      "24          35           1     1  0.614144\n",
      "25          36           0     1  0.615558\n",
      "26          43           1     1  0.607785\n",
      "27          44           0     1  0.614775\n",
      "28          45           0     1  0.614483\n",
      "29          46           1     1  0.615600\n",
      "30          48           1     1  0.598232\n",
      "31          49           0     1  0.611965\n",
      "32          52           1     1  0.608002\n",
      "33          53           0     1  0.601086\n",
      "34          54           1     1  0.617063\n",
      "35          56           1     1  0.620648\n",
      "36          58           1     1  0.615932\n",
      "37          59           1     1  0.610074\n",
      "38          60           1     1  0.600333\n",
      "39          61           0     1  0.602382\n",
      "40          62           1     1  0.614458\n",
      "41          63           1     1  0.608582\n",
      "42          64           0     1  0.611235\n",
      "43          66           1     1  0.606851\n",
      "44          68           1     1  0.615171\n",
      "45          70           1     1  0.605370\n",
      "46          71           1     1  0.604805\n",
      "47          72           0     1  0.599510\n",
      "48          74           1     1  0.601377\n",
      "49          77           1     1  0.613000\n",
      "50          78           1     1  0.618762\n",
      "51          81           0     1  0.615607\n",
      "52          84           0     1  0.617653\n",
      "53          85           1     1  0.605080\n",
      "54          87           1     1  0.614501\n",
      "55          88           0     1  0.614397\n",
      "56          89           1     1  0.622804\n",
      "57          90           0     1  0.609633\n",
      "58          94           1     1  0.607323\n",
      "59          95           0     1  0.603162\n",
      "60          96           1     1  0.613283\n",
      "61          97           0     1  0.612840\n",
      "62          98           1     1  0.619396\n",
      "63          99           0     1  0.616968\n",
      "64         100           1     1  0.623129\n",
      "65         102           0     1  0.616563\n",
      "66         104           0     1  0.629719\n",
      "67         105           1     1  0.618297\n",
      "68         106           1     1  0.612863\n",
      "69         107           1     1  0.622423\n",
      "70         108           0     1  0.606690\n",
      "71         109           1     1  0.628841\n",
      "72         110           0     1  0.603668\n",
      "73         111           0     1  0.603489\n",
      "74         112           0     1  0.622913\n",
      "75         113           0     1  0.614779\n",
      "76         116           0     1  0.614907\n",
      "77         117           1     1  0.606477\n",
      "78         120           1     1  0.615318\n",
      "79         121           0     1  0.607608\n",
      "80         122           0     1  0.632401\n",
      "81         123           0     1  0.613292\n",
      "82         124           0     1  0.627416\n",
      "83         128           1     1  0.624520\n",
      "84         130           0     1  0.602773\n",
      "85         132           0     1  0.610366\n",
      "86         133           0     1  0.601664\n",
      "87         134           1     1  0.625996\n",
      "88         136           1     1  0.626795\n",
      "89         137           0     1  0.625138\n",
      "90         138           1     1  0.601458\n",
      "91         139           1     1  0.614358\n",
      "92         140           1     1  0.626927\n",
      "93         142           0     1  0.604068\n",
      "94         143           1     1  0.625510\n",
      "95         144           1     1  0.603998\n",
      "96         146           1     1  0.604962\n",
      "97         147           0     1  0.603142\n",
      "98         148           0     1  0.612793\n",
      "99         149           0     1  0.615325\n",
      "100        150           0     1  0.630210\n",
      "101        151           0     1  0.615442\n",
      "102        154           0     1  0.588903\n",
      "103        155           1     1  0.584416\n",
      "104        156           1     1  0.604072\n",
      "105        157           0     1  0.586709\n",
      "106        158           0     1  0.592127\n",
      "107        159           1     1  0.578508\n",
      "108        160           1     1  0.585950\n",
      "109        162           0     1  0.585196\n",
      "110        165           0     1  0.578080\n",
      "111        166           1     1  0.611358\n",
      "112        167           0     1  0.584139\n",
      "113        169           0     1  0.616226\n",
      "114        170           0     1  0.589465\n",
      "115        171           1     1  0.589338\n",
      "116        172           0     1  0.586910\n",
      "117        176           0     1  0.593579\n",
      "118        177           1     1  0.580754\n",
      "119        178           1     1  0.589047\n",
      "120        183           0     1  0.591058\n",
      "121        184           0     1  0.580470\n",
      "122        185           1     1  0.589232\n",
      "123        186           1     1  0.568379\n",
      "124        187           1     1  0.600674\n",
      "125        188           1     1  0.580958\n",
      "126        191           0     1  0.580851\n",
      "127        192           0     1  0.579116\n",
      "128        193           0     1  0.621304\n",
      "129        194           0     1  0.591630\n",
      "130        195           0     1  0.588770\n",
      "131        196           1     1  0.588372\n",
      "132        197           1     1  0.582045\n",
      "133        199           1     1  0.582435\n",
      "134        201           0     1  0.582579\n",
      "135        203           1     1  0.587161\n",
      "136        204           1     1  0.600737\n",
      "137        206           0     1  0.614216\n",
      "138        209           0     1  0.577569\n",
      "139        210           1     1  0.589971\n",
      "140        211           0     1  0.580259\n",
      "141        212           1     1  0.576367\n",
      "142        214           0     1  0.611883\n",
      "143        216           0     1  0.596670\n",
      "144        217           0     1  0.577687\n",
      "145        218           0     1  0.573278\n",
      "146        219           0     1  0.589048\n",
      "147        220           1     1  0.585831\n",
      "148        221           0     1  0.583187\n",
      "149        222           1     1  0.584524\n",
      "150        227           0     1  0.589251\n",
      "151        228           0     1  0.609677\n",
      "152        230           1     1  0.577585\n",
      "153        231           0     1  0.587778\n",
      "154        233           1     1  0.582332\n",
      "155        234           1     1  0.576805\n",
      "156        235           1     1  0.594207\n",
      "157        236           0     1  0.583924\n",
      "158        237           0     1  0.582313\n",
      "159        238           0     1  0.582800\n",
      "160        239           0     1  0.584646\n",
      "161        240           1     1  0.588715\n",
      "162        241           0     1  0.582823\n",
      "163        242           0     1  0.585405\n",
      "164        243           0     1  0.616371\n",
      "165        245           1     1  0.578682\n",
      "166        246           1     1  0.587254\n",
      "167        247           0     1  0.593534\n",
      "168        249           0     1  0.579712\n",
      "169        250           1     1  0.601791\n",
      "170        251           0     1  0.577007\n",
      "171        253           1     1  0.581906\n",
      "172        254           1     1  0.582512\n",
      "173        258           0     1  0.614057\n",
      "174        259           0     1  0.586276\n",
      "175        260           1     1  0.586502\n",
      "176        261           0     1  0.582360\n",
      "177        262           0     1  0.615851\n",
      "178        263           1     1  0.573636\n",
      "179        266           0     1  0.598449\n",
      "180        267           0     1  0.582933\n",
      "181        269           0     1  0.595096\n",
      "182        270           1     1  0.621184\n",
      "183        271           1     1  0.588251\n",
      "184        273           1     1  0.584434\n",
      "185        274           0     1  0.590047\n",
      "186        275           0     1  0.609968\n",
      "187        280           0     1  0.586936\n",
      "188        281           1     1  0.605745\n",
      "189        282           1     1  0.581354\n",
      "190        283           0     1  0.581116\n",
      "191        284           1     1  0.581842\n",
      "192        285           1     1  0.582526\n",
      "193        286           0     1  0.581105\n",
      "194        288           0     1  0.584088\n",
      "195        289           0     1  0.587818\n",
      "196        290           0     1  0.577702\n",
      "197        291           1     1  0.607188\n",
      "198        293           1     1  0.582772\n",
      "199        294           1     1  0.609864\n",
      "200        296           1     1  0.586923\n",
      "201        297           0     1  0.584361\n",
      "202        298           0     1  0.581466\n",
      "203        299           1     1  0.577889\n",
      "204        300           0     1  0.587747\n",
      "205        301           0     1  0.577737\n",
      "206        303           1     1  0.587197\n",
      "207        304           1     1  0.584391\n",
      "208        305           1     1  0.612740\n",
      "209        306           1     1  0.586903\n",
      "210        308           0     1  0.582068\n",
      "211        309           0     1  0.582666\n",
      "212        310           0     1  0.587859\n",
      "213        311           1     1  0.579752\n",
      "214        312           0     1  0.600954\n",
      "215        313           1     1  0.581066\n",
      "216        314           0     1  0.578734\n",
      "217        316           0     1  0.584015\n",
      "218        317           1     1  0.625657\n",
      "219        318           0     1  0.574720\n",
      "220        320           0     1  0.582510\n",
      "221        321           1     1  0.584025\n",
      "222        322           1     1  0.583055\n",
      "223        324           0     1  0.579933\n",
      "224        325           0     1  0.606654\n",
      "225        327           0     1  0.581273\n",
      "226        328           1     1  0.578643\n",
      "227        329           1     1  0.585311\n",
      "228        331           1     1  0.575613\n",
      "229        332           1     1  0.596974\n",
      "230        334           1     1  0.582929\n",
      "231        336           0     1  0.586752\n",
      "232        338           1     1  0.621484\n",
      "233        339           0     1  0.589329\n",
      "234        340           1     1  0.580964\n",
      "235        341           0     1  0.586066\n",
      "236        343           0     1  0.586283\n",
      "237        344           1     1  0.590464\n",
      "238        346           0     1  0.589379\n",
      "239        347           0     1  0.579301\n",
      "240        348           0     1  0.590047\n",
      "241        349           0     1  0.604273\n",
      "242        350           1     1  0.584795\n",
      "243        351           0     1  0.598756\n",
      "244        352           1     1  0.592330\n",
      "245        353           0     1  0.582210\n",
      "246        356           0     1  0.586398\n",
      "247        359           1     1  0.612374\n",
      "248        360           1     1  0.588394\n",
      "249        364           1     1  0.591734\n",
      "250        366           1     1  0.586094\n",
      "251        367           1     1  0.612094\n",
      "252        369           1     1  0.601699\n",
      "253        370           1     1  0.586155\n",
      "254        371           1     1  0.585742\n",
      "255        373           0     1  0.582525\n",
      "256        376           0     1  0.572523\n",
      "257        377           0     1  0.618536\n",
      "258        378           0     1  0.583786\n",
      "259        379           0     1  0.581317\n",
      "260        380           0     1  0.585884\n",
      "261        382           0     1  0.583789\n",
      "262        383           1     1  0.601121\n",
      "263        386           1     1  0.579375\n",
      "264        387           0     1  0.590392\n",
      "265        388           0     1  0.607388\n",
      "266        389           0     1  0.612859\n",
      "267        390           0     1  0.587638\n",
      "268        391           0     1  0.575958\n",
      "269        392           0     1  0.598369\n",
      "270        395           0     1  0.579468\n",
      "271        397           0     1  0.591569\n",
      "272        399           0     1  0.585440\n",
      "273        400           1     1  0.584696\n",
      "274        401           0     1  0.588673\n",
      "275        402           0     1  0.579656\n",
      "276        403           1     1  0.594558\n",
      "277        404           1     1  0.583167\n",
      "278        405           0     1  0.626781\n",
      "279        406           1     1  0.584833\n",
      "280        407           0     1  0.585957\n",
      "281        408           1     1  0.585242\n",
      "282        409           1     1  0.578469\n",
      "283        410           0     1  0.591191\n",
      "284        412           0     1  0.582551\n",
      "285        413           1     1  0.593381\n",
      "286        414           0     1  0.582776\n",
      "287        416           1     1  0.579200\n",
      "288        417           0     1  0.587743\n",
      "289        418           0     1  0.587680\n",
      "290        419           0     1  0.578298\n",
      "291        421           0     1  0.584538\n",
      "292        423           0     1  0.580343\n",
      "293        425           1     1  0.575228\n",
      "294        426           1     1  0.590299\n",
      "295        429           1     1  0.610907\n",
      "296        430           0     1  0.582528\n",
      "297        431           1     1  0.583862\n",
      "298        432           0     1  0.596060\n",
      "299        433           0     1  0.585991\n",
      "300        436           1     1  0.617047\n",
      "301        440           1     1  0.587866\n",
      "302        441           0     1  0.614926\n",
      "303        442           1     1  0.601917\n",
      "304        443           1     1  0.612986\n",
      "305        444           0     1  0.597443\n",
      "306        445           0     1  0.596371\n",
      "307        446           0     1  0.599069\n",
      "308        449           1     1  0.604073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309        451           1     1  0.608137\r\n",
      "310        452           0     1  0.590576\r\n",
      "311        454           0     1  0.605554\r\n",
      "312        455           0     1  0.626967\r\n",
      "313        456           1     1  0.604034\r\n",
      "314        457           1     1  0.600131\r\n",
      "315        459           0     1  0.635449\r\n",
      "316        464           0     1  0.602596\r\n",
      "317        466           1     1  0.612758\r\n",
      "318        468           1     1  0.614658\r\n",
      "319        469           0     1  0.615912\r\n",
      "320        470           1     1  0.603378\r\n",
      "321        472           1     1  0.599676\r\n",
      "322        477           0     1  0.609415\r\n",
      "323        478           1     1  0.604147\r\n",
      "324        479           1     1  0.602162\r\n",
      "325        480           1     1  0.602194\r\n",
      "326        481           0     1  0.620727\r\n",
      "327        483           1     1  0.602981\r\n",
      "328        485           1     1  0.608528\r\n",
      "329        488           1     1  0.601872\r\n",
      "330        491           1     1  0.602027\r\n",
      "331        493           1     1  0.609925\r\n",
      "332        494           1     1  0.610711\r\n",
      "333        495           0     1  0.603240\r\n",
      "334        496           0     1  0.602319\r\n",
      "335        498           0     1  0.614518\r\n",
      "336        499           1     1  0.598892\r\n",
      "337        500           1     1  0.602554\r\n",
      "338        501           1     1  0.609277\r\n",
      "339        502           1     1  0.610238\r\n",
      "340        504           1     1  0.603169\r\n",
      "341        505           1     1  0.606565\r\n",
      "342        506           1     1  0.611527\r\n",
      "343        507           0     1  0.620783\r\n",
      "344        510           0     1  0.603880\r\n",
      "345        511           1     1  0.610132\r\n",
      "346        512           0     1  0.613394\r\n",
      "347        513           1     1  0.603592\r\n",
      "348        514           0     1  0.609890\r\n",
      "349        516           1     1  0.603571\r\n",
      "350        517           1     1  0.608913\r\n",
      "351        518           0     1  0.606328\r\n",
      "352        519           0     1  0.601072\r\n",
      "353        520           1     1  0.612843\r\n",
      "354        523           1     1  0.615778\r\n",
      "355        524           1     1  0.607168\r\n",
      "356        525           1     1  0.612170\r\n",
      "357        526           1     1  0.592013\r\n",
      "358        528           1     1  0.597175\r\n",
      "359        529           1     1  0.614573\r\n",
      "360        530           0     1  0.604834\r\n",
      "361        532           1     1  0.619738\r\n",
      "362        533           0     1  0.611777\r\n",
      "363        537           1     1  0.597400\r\n",
      "364        538           0     1  0.613639\r\n",
      "365        539           1     1  0.607012\r\n",
      "366        540           0     1  0.604454\r\n",
      "367        542           1     1  0.607961\r\n",
      "368        543           1     1  0.609817\r\n",
      "369        544           1     1  0.604436\r\n",
      "370        545           0     1  0.608416\r\n",
      "371        547           0     1  0.608297\r\n",
      "372        548           1     1  0.618568\r\n",
      "373        549           1     1  0.609800\r\n",
      "374        550           1     1  0.607720\r\n",
      "375        551           1     1  0.604138\r\n",
      "376        552           1     1  0.613310\r\n",
      "377        554           1     1  0.622188\r\n",
      "378        555           0     1  0.616380\r\n",
      "379        556           1     1  0.599559\r\n",
      "380        557           1     1  0.610807\r\n",
      "381        558           1     1  0.606120\r\n",
      "382        559           1     1  0.611257\r\n",
      "383        561           1     1  0.610842\r\n",
      "384        563           0     1  0.608805\r\n",
      "385        564           1     1  0.617029\r\n",
      "386        565           0     1  0.611097\r\n",
      "387        567           0     1  0.607597\r\n",
      "388        568           0     1  0.599329\r\n",
      "389        569           0     1  0.603782\r\n",
      "390        570           1     1  0.630894\r\n",
      "391        571           0     1  0.626585\r\n",
      "392        572           0     1  0.608226\r\n",
      "393        574           0     1  0.607115\r\n",
      "394        575           0     1  0.619235\r\n",
      "395        576           1     1  0.620184\r\n",
      "396        577           1     1  0.604652\r\n",
      "397        578           0     1  0.620183\r\n",
      "398        579           1     1  0.607819\r\n",
      "399        581           0     1  0.609612\r\n",
      "400        582           1     1  0.586362\r\n",
      "401        583           1     1  0.628891\r\n",
      "402        584           1     1  0.634279\r\n",
      "403        586           1     1  0.609454\r\n",
      "404        587           0     1  0.608087\r\n",
      "405        588           0     1  0.607021\r\n",
      "406        589           0     1  0.633196\r\n",
      "407        590           1     1  0.598797\r\n",
      "408        591           0     1  0.610362\r\n",
      "409        593           1     1  0.606883\r\n",
      "410        594           1     1  0.598697\r\n",
      "411        596           0     1  0.605044\r\n",
      "412        597           1     1  0.604095\r\n",
      "413        598           1     1  0.601111\r\n",
      "414        599           1     1  0.607960\r\n",
      "415        601           0     1  0.603319\r\n",
      "416        602           1     1  0.611902\r\n",
      "417        604           1     1  0.605564\r\n",
      "418        605           0     1  0.605299\r\n",
      "419        606           1     1  0.594212\r\n",
      "420        607           1     1  0.608207\r\n",
      "421        608           1     1  0.602169\r\n",
      "422        610           1     1  0.615846\r\n",
      "423        611           1     1  0.611737\r\n",
      "424        612           1     1  0.607191\r\n",
      "425        613           1     1  0.611838\r\n",
      "426        615           1     1  0.606178\r\n",
      "427        616           0     1  0.602518\r\n",
      "428        618           1     1  0.602203\r\n",
      "429        619           0     1  0.604923\r\n",
      "430        620           0     1  0.603083\r\n",
      "431        621           1     1  0.605423\r\n",
      "432        622           1     1  0.610709\r\n",
      "433        623           0     1  0.604143\r\n",
      "434        624           0     1  0.612341\r\n",
      "435        625           1     1  0.592627\r\n",
      "436        626           1     1  0.606575\r\n",
      "437        628           1     1  0.606098\r\n",
      "438        630           0     1  0.607459\r\n",
      "439        631           1     1  0.602607\r\n",
      "440        636           0     1  0.602385\r\n",
      "441        638           1     1  0.607989\r\n",
      "442        639           1     1  0.603921\r\n",
      "443        640           1     1  0.608773\r\n",
      "444        641           0     1  0.604100\r\n",
      "445        642           0     1  0.606499\r\n",
      "446        645           0     1  0.601400\r\n",
      "447        646           1     1  0.599159\r\n",
      "448        649           0     1  0.603363\r\n",
      "449        650           1     1  0.607951\r\n",
      "450        651           0     1  0.604145\r\n",
      "451        652           1     1  0.600008\r\n",
      "452        654           0     1  0.610421\r\n",
      "453        655           1     1  0.595245\r\n",
      "454        656           1     1  0.602372\r\n",
      "455        657           0     1  0.601347\r\n",
      "456        658           1     1  0.600840\r\n",
      "457        659           1     1  0.607950\r\n",
      "458        661           1     1  0.607420\r\n",
      "459        663           0     1  0.603349\r\n",
      "460        667           0     1  0.602746\r\n",
      "461        668           0     1  0.609621\r\n",
      "462        674           1     1  0.614999\r\n",
      "463        675           1     1  0.613022\r\n",
      "464        676           1     1  0.604833\r\n",
      "465        677           1     1  0.599592\r\n",
      "466        679           1     1  0.608707\r\n",
      "467        680           1     1  0.599805\r\n",
      "468        682           0     1  0.606195\r\n",
      "469        683           0     1  0.609199\r\n",
      "470        684           0     1  0.607415\r\n",
      "471        685           0     1  0.601739\r\n",
      "472        686           0     1  0.608269\r\n",
      "473        687           0     1  0.609746\r\n",
      "474        688           0     1  0.598054\r\n",
      "475        690           1     1  0.608669\r\n",
      "476        691           1     1  0.607586\r\n",
      "477        692           1     1  0.606635\r\n",
      "478        693           1     1  0.613436\r\n",
      "479        694           1     1  0.604669\r\n",
      "480        697           1     1  0.606731\r\n",
      "481        698           1     1  0.600848\r\n",
      "482        703           0     1  0.603212\r\n",
      "483        704           1     1  0.613336\r\n",
      "484        705           1     1  0.599698\r\n",
      "485        706           0     1  0.597561\r\n",
      "486        707           1     1  0.597486\r\n",
      "487        708           1     1  0.607653\r\n",
      "488        709           0     1  0.607137\r\n",
      "489        714           1     1  0.619250\r\n",
      "490        715           1     1  0.607218\r\n",
      "491        716           1     1  0.606123\r\n",
      "492        718           1     1  0.597774\r\n",
      "493        723           0     1  0.608611\r\n",
      "494        724           0     1  0.608088\r\n",
      "495        725           1     1  0.607650\r\n",
      "496        727           0     1  0.602093\r\n",
      "497        728           0     1  0.607323\r\n",
      "498        729           0     1  0.605797\r\n",
      "499        730           0     1  0.615098\r\n",
      "500        731           1     1  0.614788\r\n",
      "501        732           1     1  0.612551\r\n",
      "502        733           0     1  0.603454\r\n",
      "503        734           0     1  0.597309\r\n",
      "504        735           0     1  0.601097\r\n",
      "505        736           1     1  0.607031\r\n",
      "506        737           1     1  0.613756\r\n",
      "507        739           1     1  0.600266\r\n",
      "508        740           1     1  0.609388\r\n",
      "509        742           0     1  0.607031\r\n",
      "510        744           0     1  0.597065\r\n",
      "511        746           1     1  0.609414\r\n",
      "512        747           0     1  0.600853\r\n",
      "513        750           1     1  0.604484\r\n",
      "514        751           0     1  0.596894\r\n",
      "515        753           0     1  0.596175\r\n",
      "516        756           0     1  0.613967\r\n",
      "517        757           1     1  0.606400\r\n",
      "518        758           1     1  0.599790\r\n",
      "519        759           0     1  0.600430\r\n",
      "520        760           1     1  0.598056\r\n",
      "521        764           0     1  0.601022\r\n",
      "522        765           1     1  0.611154\r\n",
      "523        767           0     1  0.610180\r\n",
      "524        768           1     1  0.604033\r\n",
      "525        772           1     1  0.605421\r\n",
      "526        773           1     1  0.607805\r\n",
      "527        774           0     1  0.602883\r\n",
      "528        775           1     1  0.602686\r\n",
      "529        777           1     1  0.612806\r\n",
      "530        778           0     1  0.613891\r\n",
      "531        780           0     1  0.619275\r\n",
      "532        781           1     1  0.584636\r\n",
      "533        782           1     1  0.617283\r\n",
      "534        784           1     1  0.618533\r\n",
      "535        787           1     1  0.615033\r\n",
      "536        788           0     1  0.608372\r\n",
      "537        789           1     1  0.622126\r\n",
      "538        791           1     1  0.614918\r\n",
      "539        792           0     1  0.611712\r\n",
      "540        793           1     1  0.611532\r\n",
      "541        794           1     1  0.618426\r\n",
      "542        795           1     1  0.615992\r\n",
      "543        796           0     1  0.599203\r\n",
      "544        797           0     1  0.621385\r\n",
      "545        799           0     1  0.612345\r\n",
      "546        800           0     1  0.615531\r\n",
      "547        801           1     1  0.613418\r\n",
      "548        802           0     1  0.623965\r\n",
      "549        803           0     1  0.616099\r\n",
      "550        804           0     1  0.623827\r\n",
      "551        805           0     1  0.615485\r\n",
      "552        806           0     1  0.623124\r\n",
      "553        807           1     1  0.608136\r\n",
      "554        808           1     1  0.605243\r\n",
      "555        809           0     1  0.608700\r\n",
      "556        810           0     1  0.622383\r\n",
      "557        811           1     1  0.618975\r\n",
      "558        814           0     1  0.623577\r\n",
      "559        816           1     1  0.611260\r\n",
      "560        818           0     1  0.612741\r\n",
      "561        819           1     1  0.605290\r\n",
      "562        820           0     1  0.589325\r\n",
      "563        823           1     1  0.605955\r\n",
      "564        824           0     1  0.617354\r\n",
      "565        828           1     1  0.601205\r\n",
      "566        830           0     1  0.618143\r\n",
      "567        834           0     1  0.612948\r\n",
      "568        836           0     1  0.609595\r\n",
      "569        837           0     1  0.588183\r\n",
      "570        838           1     1  0.589581\r\n",
      "571        839           0     1  0.588112\r\n",
      "572        840           1     1  0.581337\r\n",
      "573        998           1     1  0.588746\r\n",
      "574        999           1     1  0.620188\r\n",
      "575       1000           1     1  0.575894\r\n",
      "576       1001           1     1  0.580574\r\n",
      "577       1002           1     1  0.621380\r\n",
      "578       1003           1     1  0.617407\r\n",
      "579       1004           0     1  0.585074\r\n",
      "580       1005           1     1  0.623513\r\n",
      "581       1007           1     1  0.600291\r\n",
      "582       1008           1     1  0.586951\r\n",
      "583       1009           0     1  0.625055\r\n",
      "584       1010           0     1  0.588748\r\n",
      "Prediction AUC: 0.5391\r\n",
      "Prediction Accuracy: 0.5248\r\n",
      "Prediction Specificity: 0.0000\r\n",
      "Prediction Sensitivity: 1.0000\r\n",
      "Prediction Precision: 0.5248\r\n",
      "the score of the fold number 1 and the type T1wCE: 0.5390527968504675\r\n",
      "  model       AUC       acc  spec  sens      prec\r\n",
      "0     1  0.539053  0.524786   0.0   1.0  0.524786\r\n",
      "Caulculating the best scans for every case...\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 585/585 [01:05<00:00,  8.90it/s]\n",
      "Fold 2:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            0           1     0  0.449878\n",
      "1            2           1     0  0.462402\n",
      "2            3           0     0  0.472426\n",
      "3            5           1     0  0.440054\n",
      "4            6           1     0  0.458609\n",
      "5            8           1     1  0.502449\n",
      "6            9           0     0  0.488696\n",
      "7           11           1     0  0.322525\n",
      "8           12           1     1  0.508357\n",
      "9           14           1     1  0.504154\n",
      "10          17           0     1  0.508559\n",
      "11          18           0     0  0.455747\n",
      "12          19           0     0  0.472511\n",
      "13          20           1     0  0.418814\n",
      "14          21           0     0  0.452352\n",
      "15          22           0     0  0.482003\n",
      "16          24           0     0  0.402627\n",
      "17          25           1     0  0.459047\n",
      "18          26           1     0  0.458597\n",
      "19          28           1     0  0.438901\n",
      "20          30           0     0  0.445334\n",
      "21          31           1     0  0.494269\n",
      "22          32           0     0  0.482804\n",
      "23          33           1     0  0.412319\n",
      "24          35           1     0  0.470745\n",
      "25          36           0     0  0.472313\n",
      "26          43           1     1  0.506925\n",
      "27          44           0     0  0.399843\n",
      "28          45           0     0  0.454137\n",
      "29          46           1     0  0.457598\n",
      "30          48           1     1  0.514563\n",
      "31          49           0     0  0.400298\n",
      "32          52           1     0  0.458779\n",
      "33          53           0     1  0.519239\n",
      "34          54           1     0  0.491667\n",
      "35          56           1     0  0.444041\n",
      "36          58           1     0  0.469763\n",
      "37          59           1     0  0.439394\n",
      "38          60           1     0  0.458424\n",
      "39          61           0     1  0.512228\n",
      "40          62           1     0  0.455123\n",
      "41          63           1     1  0.500824\n",
      "42          64           0     1  0.501654\n",
      "43          66           1     1  0.504561\n",
      "44          68           1     0  0.396239\n",
      "45          70           1     0  0.439751\n",
      "46          71           1     0  0.471738\n",
      "47          72           0     0  0.415627\n",
      "48          74           1     1  0.509388\n",
      "49          77           1     0  0.462786\n",
      "50          78           1     0  0.445380\n",
      "51          81           0     0  0.436989\n",
      "52          84           0     0  0.448661\n",
      "53          85           1     0  0.469150\n",
      "54          87           1     0  0.474837\n",
      "55          88           0     0  0.425706\n",
      "56          89           1     0  0.472188\n",
      "57          90           0     1  0.504084\n",
      "58          94           1     0  0.447959\n",
      "59          95           0     1  0.507355\n",
      "60          96           1     0  0.470150\n",
      "61          97           0     0  0.475584\n",
      "62          98           1     0  0.489997\n",
      "63          99           0     0  0.449355\n",
      "64         100           1     0  0.117633\n",
      "65         102           0     0  0.126353\n",
      "66         104           0     0  0.290772\n",
      "67         105           1     0  0.248550\n",
      "68         106           1     0  0.414925\n",
      "69         107           1     0  0.177645\n",
      "70         108           0     1  0.510938\n",
      "71         109           1     0  0.237931\n",
      "72         110           0     0  0.401115\n",
      "73         111           0     0  0.461990\n",
      "74         112           0     0  0.085156\n",
      "75         113           0     0  0.099405\n",
      "76         116           0     0  0.293506\n",
      "77         117           1     0  0.434364\n",
      "78         120           1     0  0.351421\n",
      "79         121           0     0  0.187962\n",
      "80         122           0     1  0.622415\n",
      "81         123           0     0  0.159125\n",
      "82         124           0     1  0.693327\n",
      "83         128           1     0  0.294172\n",
      "84         130           0     0  0.491436\n",
      "85         132           0     0  0.286240\n",
      "86         133           0     0  0.260120\n",
      "87         134           1     0  0.411156\n",
      "88         136           1     0  0.288706\n",
      "89         137           0     0  0.396423\n",
      "90         138           1     1  0.577583\n",
      "91         139           1     0  0.102491\n",
      "92         140           1     0  0.326397\n",
      "93         142           0     0  0.284656\n",
      "94         143           1     0  0.455691\n",
      "95         144           1     0  0.230528\n",
      "96         146           1     0  0.315697\n",
      "97         147           0     0  0.361895\n",
      "98         148           0     0  0.399290\n",
      "99         149           0     0  0.129850\n",
      "100        150           0     0  0.310538\n",
      "101        151           0     0  0.214246\n",
      "102        154           0     1  0.519764\n",
      "103        155           1     0  0.464872\n",
      "104        156           1     0  0.454780\n",
      "105        157           0     1  0.518896\n",
      "106        158           0     0  0.465758\n",
      "107        159           1     1  0.515191\n",
      "108        160           1     1  0.508798\n",
      "109        162           0     1  0.507715\n",
      "110        165           0     0  0.473351\n",
      "111        166           1     0  0.485149\n",
      "112        167           0     0  0.469482\n",
      "113        169           0     0  0.253752\n",
      "114        170           0     0  0.494787\n",
      "115        171           1     1  0.504274\n",
      "116        172           0     1  0.509403\n",
      "117        176           0     0  0.497186\n",
      "118        177           1     0  0.497388\n",
      "119        178           1     0  0.490499\n",
      "120        183           0     0  0.484992\n",
      "121        184           0     0  0.496521\n",
      "122        185           1     1  0.514245\n",
      "123        186           1     1  0.526192\n",
      "124        187           1     0  0.467834\n",
      "125        188           1     0  0.493027\n",
      "126        191           0     0  0.478274\n",
      "127        192           0     1  0.511521\n",
      "128        193           0     0  0.218053\n",
      "129        194           0     1  0.505909\n",
      "130        195           0     0  0.463234\n",
      "131        196           1     1  0.503505\n",
      "132        197           1     0  0.481276\n",
      "133        199           1     1  0.512027\n",
      "134        201           0     1  0.511365\n",
      "135        203           1     1  0.510641\n",
      "136        204           1     0  0.401265\n",
      "137        206           0     1  0.512265\n",
      "138        209           0     1  0.524315\n",
      "139        210           1     0  0.484302\n",
      "140        211           0     1  0.502339\n",
      "141        212           1     0  0.479188\n",
      "142        214           0     0  0.311454\n",
      "143        216           0     0  0.445073\n",
      "144        217           0     0  0.426572\n",
      "145        218           0     1  0.512735\n",
      "146        219           0     1  0.508822\n",
      "147        220           1     0  0.484883\n",
      "148        221           0     0  0.489095\n",
      "149        222           1     1  0.523177\n",
      "150        227           0     1  0.521465\n",
      "151        228           0     0  0.331220\n",
      "152        230           1     1  0.508810\n",
      "153        231           0     1  0.513259\n",
      "154        233           1     0  0.486320\n",
      "155        234           1     1  0.506918\n",
      "156        235           1     1  0.508023\n",
      "157        236           0     1  0.518630\n",
      "158        237           0     1  0.519835\n",
      "159        238           0     0  0.458018\n",
      "160        239           0     0  0.465926\n",
      "161        240           1     1  0.518381\n",
      "162        241           0     0  0.496474\n",
      "163        242           0     1  0.506015\n",
      "164        243           0     0  0.230185\n",
      "165        245           1     1  0.518356\n",
      "166        246           1     1  0.517101\n",
      "167        247           0     0  0.467408\n",
      "168        249           0     1  0.515481\n",
      "169        250           1     0  0.471019\n",
      "170        251           0     1  0.528117\n",
      "171        253           1     0  0.496877\n",
      "172        254           1     0  0.494534\n",
      "173        258           0     0  0.349271\n",
      "174        259           0     1  0.503917\n",
      "175        260           1     0  0.484507\n",
      "176        261           0     1  0.519320\n",
      "177        262           0     0  0.325842\n",
      "178        263           1     0  0.490346\n",
      "179        266           0     0  0.481636\n",
      "180        267           0     0  0.449226\n",
      "181        269           0     0  0.490903\n",
      "182        270           1     0  0.202004\n",
      "183        271           1     1  0.505683\n",
      "184        273           1     0  0.460159\n",
      "185        274           0     1  0.500640\n",
      "186        275           0     0  0.339902\n",
      "187        280           0     1  0.505018\n",
      "188        281           1     0  0.407419\n",
      "189        282           1     1  0.516177\n",
      "190        283           0     1  0.510052\n",
      "191        284           1     1  0.512271\n",
      "192        285           1     1  0.509202\n",
      "193        286           0     1  0.517964\n",
      "194        288           0     0  0.464648\n",
      "195        289           0     1  0.504651\n",
      "196        290           0     0  0.488281\n",
      "197        291           1     0  0.305022\n",
      "198        293           1     1  0.508813\n",
      "199        294           1     0  0.244614\n",
      "200        296           1     1  0.515716\n",
      "201        297           0     0  0.475567\n",
      "202        298           0     1  0.505270\n",
      "203        299           1     0  0.431625\n",
      "204        300           0     0  0.480893\n",
      "205        301           0     0  0.476511\n",
      "206        303           1     1  0.515156\n",
      "207        304           1     0  0.479842\n",
      "208        305           1     0  0.458931\n",
      "209        306           1     0  0.463879\n",
      "210        308           0     0  0.451603\n",
      "211        309           0     0  0.499947\n",
      "212        310           0     0  0.469169\n",
      "213        311           1     1  0.521107\n",
      "214        312           0     0  0.498260\n",
      "215        313           1     1  0.514846\n",
      "216        314           0     0  0.487810\n",
      "217        316           0     1  0.509178\n",
      "218        317           1     0  0.302196\n",
      "219        318           0     0  0.480978\n",
      "220        320           0     1  0.516328\n",
      "221        321           1     0  0.487508\n",
      "222        322           1     1  0.506623\n",
      "223        324           0     1  0.520871\n",
      "224        325           0     0  0.453750\n",
      "225        327           0     1  0.527446\n",
      "226        328           1     0  0.445631\n",
      "227        329           1     0  0.491187\n",
      "228        331           1     0  0.495183\n",
      "229        332           1     0  0.464388\n",
      "230        334           1     1  0.507184\n",
      "231        336           0     1  0.502058\n",
      "232        338           1     0  0.394317\n",
      "233        339           0     1  0.507429\n",
      "234        340           1     1  0.501000\n",
      "235        341           0     1  0.513536\n",
      "236        343           0     1  0.509568\n",
      "237        344           1     1  0.502214\n",
      "238        346           0     1  0.505525\n",
      "239        347           0     1  0.515084\n",
      "240        348           0     1  0.510171\n",
      "241        349           0     0  0.363244\n",
      "242        350           1     0  0.254114\n",
      "243        351           0     0  0.495570\n",
      "244        352           1     0  0.435805\n",
      "245        353           0     0  0.483058\n",
      "246        356           0     0  0.483065\n",
      "247        359           1     0  0.477283\n",
      "248        360           1     0  0.493383\n",
      "249        364           1     0  0.489536\n",
      "250        366           1     0  0.407507\n",
      "251        367           1     0  0.231204\n",
      "252        369           1     0  0.456605\n",
      "253        370           1     0  0.477578\n",
      "254        371           1     0  0.497549\n",
      "255        373           0     0  0.493325\n",
      "256        376           0     0  0.408974\n",
      "257        377           0     0  0.292202\n",
      "258        378           0     1  0.515382\n",
      "259        379           0     1  0.519701\n",
      "260        380           0     1  0.500694\n",
      "261        382           0     1  0.520378\n",
      "262        383           1     0  0.431795\n",
      "263        386           1     1  0.521306\n",
      "264        387           0     0  0.498924\n",
      "265        388           0     0  0.244136\n",
      "266        389           0     0  0.473225\n",
      "267        390           0     1  0.526037\n",
      "268        391           0     0  0.418971\n",
      "269        392           0     0  0.472305\n",
      "270        395           0     1  0.502142\n",
      "271        397           0     0  0.499680\n",
      "272        399           0     0  0.477805\n",
      "273        400           1     1  0.523921\n",
      "274        401           0     1  0.511329\n",
      "275        402           0     1  0.500270\n",
      "276        403           1     0  0.389607\n",
      "277        404           1     1  0.510382\n",
      "278        405           0     0  0.337467\n",
      "279        406           1     1  0.504985\n",
      "280        407           0     1  0.504465\n",
      "281        408           1     1  0.513042\n",
      "282        409           1     1  0.527739\n",
      "283        410           0     0  0.493404\n",
      "284        412           0     1  0.501982\n",
      "285        413           1     0  0.495899\n",
      "286        414           0     1  0.515228\n",
      "287        416           1     0  0.476383\n",
      "288        417           0     1  0.505059\n",
      "289        418           0     0  0.435331\n",
      "290        419           0     0  0.484913\n",
      "291        421           0     1  0.507191\n",
      "292        423           0     1  0.500905\n",
      "293        425           1     1  0.512075\n",
      "294        426           1     0  0.461681\n",
      "295        429           1     0  0.324649\n",
      "296        430           0     0  0.499961\n",
      "297        431           1     1  0.503695\n",
      "298        432           0     1  0.504076\n",
      "299        433           0     1  0.510736\n",
      "300        436           1     0  0.211978\n",
      "301        440           1     1  0.506600\n",
      "302        441           0     0  0.308450\n",
      "303        442           1     0  0.477458\n",
      "304        443           1     0  0.449238\n",
      "305        444           0     0  0.477168\n",
      "306        445           0     0  0.494299\n",
      "307        446           0     0  0.389210\n",
      "308        449           1     0  0.485264\n",
      "309        451           1     0  0.470978\n",
      "310        452           0     0  0.492942\n",
      "311        454           0     0  0.492966\n",
      "312        455           0     0  0.225616\n",
      "313        456           1     0  0.468359\n",
      "314        457           1     0  0.410223\n",
      "315        459           0     0  0.272730\n",
      "316        464           0     0  0.372490\n",
      "317        466           1     0  0.497294\n",
      "318        468           1     0  0.486067\n",
      "319        469           0     0  0.487395\n",
      "320        470           1     1  0.505076\n",
      "321        472           1     0  0.497144\n",
      "322        477           0     0  0.491774\n",
      "323        478           1     0  0.494041\n",
      "324        479           1     0  0.486567\n",
      "325        480           1     1  0.506320\n",
      "326        481           0     0  0.433459\n",
      "327        483           1     1  0.512672\n",
      "328        485           1     0  0.493289\n",
      "329        488           1     0  0.493118\n",
      "330        491           1     1  0.514149\n",
      "331        493           1     0  0.493318\n",
      "332        494           1     0  0.496190\n",
      "333        495           0     0  0.484102\n",
      "334        496           0     0  0.498403\n",
      "335        498           0     0  0.456563\n",
      "336        499           1     0  0.473664\n",
      "337        500           1     1  0.510956\n",
      "338        501           1     1  0.500846\n",
      "339        502           1     0  0.457529\n",
      "340        504           1     0  0.499242\n",
      "341        505           1     0  0.492247\n",
      "342        506           1     0  0.457360\n",
      "343        507           0     0  0.449684\n",
      "344        510           0     0  0.494716\n",
      "345        511           1     0  0.441510\n",
      "346        512           0     1  0.504914\n",
      "347        513           1     1  0.510349\n",
      "348        514           0     0  0.483139\n",
      "349        516           1     0  0.482701\n",
      "350        517           1     0  0.464318\n",
      "351        518           0     1  0.501436\n",
      "352        519           0     0  0.480307\n",
      "353        520           1     0  0.497076\n",
      "354        523           1     0  0.448298\n",
      "355        524           1     0  0.490113\n",
      "356        525           1     0  0.442869\n",
      "357        526           1     0  0.491719\n",
      "358        528           1     0  0.488944\n",
      "359        529           1     0  0.432187\n",
      "360        530           0     0  0.459753\n",
      "361        532           1     0  0.473577\n",
      "362        533           0     0  0.452438\n",
      "363        537           1     0  0.491405\n",
      "364        538           0     1  0.500156\n",
      "365        539           1     1  0.502852\n",
      "366        540           0     1  0.508729\n",
      "367        542           1     0  0.493844\n",
      "368        543           1     0  0.429933\n",
      "369        544           1     1  0.505836\n",
      "370        545           0     1  0.501951\n",
      "371        547           0     0  0.449464\n",
      "372        548           1     0  0.458229\n",
      "373        549           1     0  0.425409\n",
      "374        550           1     1  0.508290\n",
      "375        551           1     0  0.498602\n",
      "376        552           1     1  0.500670\n",
      "377        554           1     0  0.483934\n",
      "378        555           0     0  0.469172\n",
      "379        556           1     0  0.475342\n",
      "380        557           1     1  0.501742\n",
      "381        558           1     1  0.507959\n",
      "382        559           1     0  0.418737\n",
      "383        561           1     0  0.424466\n",
      "384        563           0     0  0.400364\n",
      "385        564           1     0  0.432346\n",
      "386        565           0     0  0.425273\n",
      "387        567           0     0  0.461396\n",
      "388        568           0     0  0.330025\n",
      "389        569           0     0  0.467439\n",
      "390        570           1     0  0.301803\n",
      "391        571           0     0  0.071190\n",
      "392        572           0     0  0.393871\n",
      "393        574           0     0  0.463334\n",
      "394        575           0     0  0.156527\n",
      "395        576           1     0  0.244225\n",
      "396        577           1     0  0.448893\n",
      "397        578           0     0  0.326275\n",
      "398        579           1     0  0.465259\n",
      "399        581           0     0  0.376765\n",
      "400        582           1     1  0.501803\n",
      "401        583           1     0  0.318548\n",
      "402        584           1     0  0.189868\n",
      "403        586           1     0  0.427077\n",
      "404        587           0     0  0.411147\n",
      "405        588           0     0  0.456639\n",
      "406        589           0     0  0.169059\n",
      "407        590           1     0  0.436155\n",
      "408        591           0     0  0.435764\n",
      "409        593           1     0  0.460923\n",
      "410        594           1     0  0.359541\n",
      "411        596           0     0  0.438936\n",
      "412        597           1     0  0.467238\n",
      "413        598           1     0  0.441738\n",
      "414        599           1     0  0.459516\n",
      "415        601           0     0  0.495535\n",
      "416        602           1     0  0.487012\n",
      "417        604           1     1  0.503447\n",
      "418        605           0     0  0.467407\n",
      "419        606           1     0  0.469089\n",
      "420        607           1     0  0.482968\n",
      "421        608           1     1  0.500594\n",
      "422        610           1     0  0.472493\n",
      "423        611           1     0  0.498308\n",
      "424        612           1     1  0.506744\n",
      "425        613           1     0  0.477080\n",
      "426        615           1     1  0.503649\n",
      "427        616           0     0  0.478047\n",
      "428        618           1     0  0.490754\n",
      "429        619           0     0  0.491314\n",
      "430        620           0     1  0.508477\n",
      "431        621           1     0  0.493230\n",
      "432        622           1     0  0.489643\n",
      "433        623           0     0  0.494872\n",
      "434        624           0     1  0.501836\n",
      "435        625           1     0  0.499657\n",
      "436        626           1     0  0.492307\n",
      "437        628           1     0  0.490429\n",
      "438        630           0     0  0.497699\n",
      "439        631           1     0  0.496417\n",
      "440        636           0     0  0.499352\n",
      "441        638           1     1  0.502401\n",
      "442        639           1     0  0.496201\n",
      "443        640           1     1  0.505160\n",
      "444        641           0     0  0.481909\n",
      "445        642           0     0  0.470006\n",
      "446        645           0     0  0.498014\n",
      "447        646           1     0  0.493318\n",
      "448        649           0     0  0.486139\n",
      "449        650           1     0  0.475375\n",
      "450        651           0     1  0.501047\n",
      "451        652           1     0  0.442898\n",
      "452        654           0     1  0.503429\n",
      "453        655           1     0  0.495772\n",
      "454        656           1     1  0.507913\n",
      "455        657           0     0  0.494700\n",
      "456        658           1     0  0.485150\n",
      "457        659           1     1  0.509465\n",
      "458        661           1     1  0.501747\n",
      "459        663           0     0  0.496611\n",
      "460        667           0     0  0.494925\n",
      "461        668           0     1  0.501716\n",
      "462        674           1     0  0.493610\n",
      "463        675           1     0  0.495070\n",
      "464        676           1     0  0.488107\n",
      "465        677           1     0  0.488301\n",
      "466        679           1     0  0.408238\n",
      "467        680           1     0  0.491718\n",
      "468        682           0     0  0.447978\n",
      "469        683           0     0  0.476621\n",
      "470        684           0     0  0.497234\n",
      "471        685           0     0  0.490560\n",
      "472        686           0     1  0.505694\n",
      "473        687           0     1  0.501407\n",
      "474        688           0     0  0.495270\n",
      "475        690           1     1  0.501900\n",
      "476        691           1     0  0.496757\n",
      "477        692           1     0  0.482180\n",
      "478        693           1     0  0.496179\n",
      "479        694           1     0  0.490085\n",
      "480        697           1     0  0.486717\n",
      "481        698           1     1  0.508466\n",
      "482        703           0     1  0.509968\n",
      "483        704           1     1  0.504202\n",
      "484        705           1     1  0.512235\n",
      "485        706           0     1  0.505194\n",
      "486        707           1     0  0.483159\n",
      "487        708           1     1  0.504379\n",
      "488        709           0     1  0.502634\n",
      "489        714           1     0  0.468560\n",
      "490        715           1     0  0.496402\n",
      "491        716           1     0  0.481640\n",
      "492        718           1     1  0.504494\n",
      "493        723           0     1  0.505234\n",
      "494        724           0     1  0.507128\n",
      "495        725           1     0  0.480636\n",
      "496        727           0     0  0.487092\n",
      "497        728           0     1  0.506677\n",
      "498        729           0     0  0.483290\n",
      "499        730           0     0  0.495631\n",
      "500        731           1     0  0.473252\n",
      "501        732           1     0  0.492375\n",
      "502        733           0     0  0.425988\n",
      "503        734           0     0  0.475013\n",
      "504        735           0     1  0.501246\n",
      "505        736           1     0  0.482080\n",
      "506        737           1     0  0.486316\n",
      "507        739           1     0  0.488718\n",
      "508        740           1     1  0.501744\n",
      "509        742           0     0  0.494864\n",
      "510        744           0     0  0.489789\n",
      "511        746           1     0  0.496425\n",
      "512        747           0     1  0.507919\n",
      "513        750           1     1  0.500570\n",
      "514        751           0     0  0.495050\n",
      "515        753           0     0  0.404625\n",
      "516        756           0     0  0.460590\n",
      "517        757           1     1  0.501519\n",
      "518        758           1     0  0.480120\n",
      "519        759           0     0  0.470567\n",
      "520        760           1     0  0.497398\n",
      "521        764           0     0  0.460193\n",
      "522        765           1     1  0.503317\n",
      "523        767           0     0  0.493886\n",
      "524        768           1     0  0.491084\n",
      "525        772           1     0  0.493911\n",
      "526        773           1     0  0.477757\n",
      "527        774           0     0  0.467354\n",
      "528        775           1     0  0.496529\n",
      "529        777           1     0  0.479200\n",
      "530        778           0     0  0.483745\n",
      "531        780           0     0  0.213362\n",
      "532        781           1     0  0.350019\n",
      "533        782           1     0  0.280041\n",
      "534        784           1     0  0.317176\n",
      "535        787           1     0  0.304095\n",
      "536        788           0     0  0.249562\n",
      "537        789           1     0  0.301104\n",
      "538        791           1     0  0.331352\n",
      "539        792           0     0  0.349476\n",
      "540        793           1     0  0.354274\n",
      "541        794           1     0  0.257893\n",
      "542        795           1     0  0.263885\n",
      "543        796           0     0  0.396479\n",
      "544        797           0     0  0.285281\n",
      "545        799           0     0  0.343717\n",
      "546        800           0     0  0.241949\n",
      "547        801           1     0  0.280471\n",
      "548        802           0     0  0.299182\n",
      "549        803           0     0  0.290634\n",
      "550        804           0     0  0.269828\n",
      "551        805           0     0  0.262606\n",
      "552        806           0     0  0.272492\n",
      "553        807           1     0  0.385446\n",
      "554        808           1     0  0.227321\n",
      "555        809           0     0  0.302377\n",
      "556        810           0     0  0.300999\n",
      "557        811           1     0  0.192162\n",
      "558        814           0     0  0.288869\n",
      "559        816           1     0  0.333415\n",
      "560        818           0     0  0.310538\n",
      "561        819           1     0  0.308920\n",
      "562        820           0     0  0.359110\n",
      "563        823           1     0  0.256357\n",
      "564        824           0     0  0.272595\n",
      "565        828           1     0  0.385043\n",
      "566        830           0     0  0.362309\n",
      "567        834           0     0  0.430768\n",
      "568        836           0     0  0.342259\n",
      "569        837           0     1  0.509510\n",
      "570        838           1     0  0.492760\n",
      "571        839           0     0  0.496436\n",
      "572        840           1     1  0.507211\n",
      "573        998           1     0  0.118739\n",
      "574        999           1     0  0.217750\n",
      "575       1000           1     1  0.522460\n",
      "576       1001           1     0  0.486761\n",
      "577       1002           1     0  0.213078\n",
      "578       1003           1     0  0.212604\n",
      "579       1004           0     1  0.509426\n",
      "580       1005           1     0  0.194250\n",
      "581       1007           1     0  0.416682\n",
      "582       1008           1     0  0.496303\n",
      "583       1009           0     0  0.308300\n",
      "584       1010           0     1  0.519861\n",
      "Prediction AUC: 0.4990\n",
      "Prediction Accuracy: 0.4667\n",
      "Prediction Specificity: 0.6906\n",
      "Prediction Sensitivity: 0.2638\n",
      "Prediction Precision: 0.4850\n",
      "the score of the fold number 2 and the type T1wCE: 0.49899233707496543\n",
      "  model       AUC       acc      spec      sens     prec\n",
      "0     2  0.498992  0.466667  0.690647  0.263844  0.48503\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 585/585 [01:04<00:00,  9.11it/s]\n",
      "Fold 3:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            0           1     1  0.636878\n",
      "1            2           1     1  0.668743\n",
      "2            3           0     1  0.646783\n",
      "3            5           1     1  0.653103\n",
      "4            6           1     1  0.651306\n",
      "5            8           1     0  0.439447\n",
      "6            9           0     1  0.680222\n",
      "7           11           1     1  0.671669\n",
      "8           12           1     1  0.514821\n",
      "9           14           1     1  0.576396\n",
      "10          17           0     1  0.534890\n",
      "11          18           0     1  0.585877\n",
      "12          19           0     1  0.575325\n",
      "13          20           1     1  0.584514\n",
      "14          21           0     1  0.554388\n",
      "15          22           0     1  0.583501\n",
      "16          24           0     1  0.585522\n",
      "17          25           1     1  0.597495\n",
      "18          26           1     1  0.620427\n",
      "19          28           1     1  0.595646\n",
      "20          30           0     1  0.637334\n",
      "21          31           1     1  0.555035\n",
      "22          32           0     1  0.548819\n",
      "23          33           1     1  0.592484\n",
      "24          35           1     1  0.597530\n",
      "25          36           0     1  0.633299\n",
      "26          43           1     1  0.532449\n",
      "27          44           0     1  0.604122\n",
      "28          45           0     1  0.649403\n",
      "29          46           1     1  0.615766\n",
      "30          48           1     1  0.520310\n",
      "31          49           0     1  0.667833\n",
      "32          52           1     1  0.578468\n",
      "33          53           0     0  0.460049\n",
      "34          54           1     1  0.620797\n",
      "35          56           1     1  0.659569\n",
      "36          58           1     1  0.597183\n",
      "37          59           1     1  0.618416\n",
      "38          60           1     1  0.529349\n",
      "39          61           0     1  0.524138\n",
      "40          62           1     1  0.572940\n",
      "41          63           1     1  0.510417\n",
      "42          64           0     1  0.517296\n",
      "43          66           1     1  0.566839\n",
      "44          68           1     1  0.669613\n",
      "45          70           1     1  0.592435\n",
      "46          71           1     1  0.538236\n",
      "47          72           0     1  0.545800\n",
      "48          74           1     0  0.494741\n",
      "49          77           1     1  0.617493\n",
      "50          78           1     1  0.644611\n",
      "51          81           0     1  0.626901\n",
      "52          84           0     1  0.609997\n",
      "53          85           1     1  0.545305\n",
      "54          87           1     1  0.550911\n",
      "55          88           0     1  0.622087\n",
      "56          89           1     1  0.627570\n",
      "57          90           0     1  0.546466\n",
      "58          94           1     1  0.573834\n",
      "59          95           0     0  0.482293\n",
      "60          96           1     1  0.623766\n",
      "61          97           0     1  0.547549\n",
      "62          98           1     1  0.591587\n",
      "63          99           0     1  0.623134\n",
      "64         100           1     1  0.816267\n",
      "65         102           0     1  0.718250\n",
      "66         104           0     1  0.722461\n",
      "67         105           1     1  0.782041\n",
      "68         106           1     1  0.681497\n",
      "69         107           1     1  0.784810\n",
      "70         108           0     1  0.644126\n",
      "71         109           1     1  0.751621\n",
      "72         110           0     1  0.585299\n",
      "73         111           0     1  0.656230\n",
      "74         112           0     1  0.694772\n",
      "75         113           0     1  0.787532\n",
      "76         116           0     1  0.776444\n",
      "77         117           1     1  0.668521\n",
      "78         120           1     1  0.587652\n",
      "79         121           0     1  0.500677\n",
      "80         122           0     1  0.681093\n",
      "81         123           0     1  0.759459\n",
      "82         124           0     1  0.622609\n",
      "83         128           1     1  0.662411\n",
      "84         130           0     1  0.631140\n",
      "85         132           0     1  0.637475\n",
      "86         133           0     1  0.528768\n",
      "87         134           1     1  0.662068\n",
      "88         136           1     1  0.707400\n",
      "89         137           0     1  0.732856\n",
      "90         138           1     1  0.622208\n",
      "91         139           1     1  0.831591\n",
      "92         140           1     1  0.778446\n",
      "93         142           0     1  0.560105\n",
      "94         143           1     1  0.721963\n",
      "95         144           1     1  0.549926\n",
      "96         146           1     1  0.572069\n",
      "97         147           0     1  0.570208\n",
      "98         148           0     1  0.770258\n",
      "99         149           0     1  0.833337\n",
      "100        150           0     1  0.712840\n",
      "101        151           0     1  0.770567\n",
      "102        154           0     0  0.488149\n",
      "103        155           1     1  0.541340\n",
      "104        156           1     1  0.636836\n",
      "105        157           0     0  0.445910\n",
      "106        158           0     1  0.559280\n",
      "107        159           1     0  0.473486\n",
      "108        160           1     1  0.527239\n",
      "109        162           0     1  0.507574\n",
      "110        165           0     0  0.466243\n",
      "111        166           1     1  0.650829\n",
      "112        167           0     1  0.502644\n",
      "113        169           0     1  0.636518\n",
      "114        170           0     1  0.557781\n",
      "115        171           1     1  0.563914\n",
      "116        172           0     1  0.509323\n",
      "117        176           0     1  0.510217\n",
      "118        177           1     0  0.434054\n",
      "119        178           1     1  0.535672\n",
      "120        183           0     1  0.569476\n",
      "121        184           0     1  0.512010\n",
      "122        185           1     0  0.484769\n",
      "123        186           1     0  0.386996\n",
      "124        187           1     1  0.640107\n",
      "125        188           1     0  0.481916\n",
      "126        191           0     0  0.481199\n",
      "127        192           0     1  0.508337\n",
      "128        193           0     1  0.777361\n",
      "129        194           0     1  0.544812\n",
      "130        195           0     1  0.501107\n",
      "131        196           1     1  0.549431\n",
      "132        197           1     0  0.488837\n",
      "133        199           1     0  0.468849\n",
      "134        201           0     0  0.490800\n",
      "135        203           1     1  0.521318\n",
      "136        204           1     1  0.646924\n",
      "137        206           0     1  0.627566\n",
      "138        209           0     0  0.468155\n",
      "139        210           1     1  0.535414\n",
      "140        211           0     0  0.484046\n",
      "141        212           1     0  0.476735\n",
      "142        214           0     1  0.581881\n",
      "143        216           0     1  0.610935\n",
      "144        217           0     1  0.536145\n",
      "145        218           0     0  0.453594\n",
      "146        219           0     1  0.519510\n",
      "147        220           1     1  0.537446\n",
      "148        221           0     1  0.504589\n",
      "149        222           1     1  0.504511\n",
      "150        227           0     1  0.500667\n",
      "151        228           0     1  0.615888\n",
      "152        230           1     1  0.510714\n",
      "153        231           0     1  0.511610\n",
      "154        233           1     1  0.520090\n",
      "155        234           1     0  0.496332\n",
      "156        235           1     1  0.528872\n",
      "157        236           0     1  0.501723\n",
      "158        237           0     1  0.501465\n",
      "159        238           0     1  0.512572\n",
      "160        239           0     1  0.506634\n",
      "161        240           1     0  0.494561\n",
      "162        241           0     1  0.520978\n",
      "163        242           0     1  0.549201\n",
      "164        243           0     1  0.768602\n",
      "165        245           1     0  0.492765\n",
      "166        246           1     1  0.510676\n",
      "167        247           0     1  0.561374\n",
      "168        249           0     0  0.483888\n",
      "169        250           1     1  0.553644\n",
      "170        251           0     0  0.470446\n",
      "171        253           1     0  0.488414\n",
      "172        254           1     0  0.493450\n",
      "173        258           0     1  0.767558\n",
      "174        259           0     1  0.531145\n",
      "175        260           1     1  0.552827\n",
      "176        261           0     1  0.500724\n",
      "177        262           0     1  0.704259\n",
      "178        263           1     0  0.481795\n",
      "179        266           0     1  0.613781\n",
      "180        267           0     0  0.467587\n",
      "181        269           0     1  0.578726\n",
      "182        270           1     1  0.792870\n",
      "183        271           1     1  0.509825\n",
      "184        273           1     1  0.533972\n",
      "185        274           0     0  0.498509\n",
      "186        275           0     1  0.798861\n",
      "187        280           0     1  0.534035\n",
      "188        281           1     1  0.658729\n",
      "189        282           1     0  0.479640\n",
      "190        283           0     0  0.495415\n",
      "191        284           1     1  0.522655\n",
      "192        285           1     1  0.510826\n",
      "193        286           0     0  0.455084\n",
      "194        288           0     1  0.571258\n",
      "195        289           0     1  0.533796\n",
      "196        290           0     0  0.464781\n",
      "197        291           1     1  0.531098\n",
      "198        293           1     0  0.472108\n",
      "199        294           1     1  0.667304\n",
      "200        296           1     1  0.508193\n",
      "201        297           0     1  0.545302\n",
      "202        298           0     1  0.505831\n",
      "203        299           1     1  0.563452\n",
      "204        300           0     1  0.513929\n",
      "205        301           0     0  0.478669\n",
      "206        303           1     1  0.540453\n",
      "207        304           1     1  0.507116\n",
      "208        305           1     1  0.685984\n",
      "209        306           1     1  0.547659\n",
      "210        308           0     0  0.498056\n",
      "211        309           0     1  0.504339\n",
      "212        310           0     1  0.554590\n",
      "213        311           1     0  0.480932\n",
      "214        312           0     1  0.627186\n",
      "215        313           1     0  0.481117\n",
      "216        314           0     1  0.501595\n",
      "217        316           0     1  0.521221\n",
      "218        317           1     1  0.642867\n",
      "219        318           0     0  0.438715\n",
      "220        320           0     1  0.504081\n",
      "221        321           1     0  0.487121\n",
      "222        322           1     0  0.485210\n",
      "223        324           0     0  0.489477\n",
      "224        325           0     1  0.668773\n",
      "225        327           0     1  0.527518\n",
      "226        328           1     1  0.501053\n",
      "227        329           1     1  0.554261\n",
      "228        331           1     0  0.495162\n",
      "229        332           1     1  0.557725\n",
      "230        334           1     1  0.509143\n",
      "231        336           0     1  0.500801\n",
      "232        338           1     1  0.740802\n",
      "233        339           0     1  0.525529\n",
      "234        340           1     0  0.467789\n",
      "235        341           0     1  0.544177\n",
      "236        343           0     1  0.516275\n",
      "237        344           1     1  0.534960\n",
      "238        346           0     1  0.500282\n",
      "239        347           0     0  0.499076\n",
      "240        348           0     1  0.558104\n",
      "241        349           0     1  0.738597\n",
      "242        350           1     1  0.558090\n",
      "243        351           0     1  0.633001\n",
      "244        352           1     1  0.549522\n",
      "245        353           0     0  0.493882\n",
      "246        356           0     1  0.537322\n",
      "247        359           1     1  0.669530\n",
      "248        360           1     1  0.514608\n",
      "249        364           1     1  0.537657\n",
      "250        366           1     1  0.557248\n",
      "251        367           1     1  0.564087\n",
      "252        369           1     1  0.637477\n",
      "253        370           1     1  0.521390\n",
      "254        371           1     0  0.497325\n",
      "255        373           0     1  0.515799\n",
      "256        376           0     1  0.546204\n",
      "257        377           0     1  0.793961\n",
      "258        378           0     0  0.489771\n",
      "259        379           0     1  0.509447\n",
      "260        380           0     1  0.536369\n",
      "261        382           0     0  0.489101\n",
      "262        383           1     1  0.646516\n",
      "263        386           1     0  0.489356\n",
      "264        387           0     1  0.513052\n",
      "265        388           0     0  0.456970\n",
      "266        389           0     1  0.679169\n",
      "267        390           0     0  0.449954\n",
      "268        391           0     1  0.504817\n",
      "269        392           0     1  0.573958\n",
      "270        395           0     0  0.484707\n",
      "271        397           0     1  0.562373\n",
      "272        399           0     0  0.495368\n",
      "273        400           1     0  0.478211\n",
      "274        401           0     0  0.415095\n",
      "275        402           0     0  0.468875\n",
      "276        403           1     1  0.619538\n",
      "277        404           1     0  0.471215\n",
      "278        405           0     1  0.765481\n",
      "279        406           1     0  0.484358\n",
      "280        407           0     1  0.532171\n",
      "281        408           1     0  0.446576\n",
      "282        409           1     0  0.469831\n",
      "283        410           0     1  0.516147\n",
      "284        412           0     1  0.508567\n",
      "285        413           1     1  0.549267\n",
      "286        414           0     1  0.523247\n",
      "287        416           1     1  0.507132\n",
      "288        417           0     1  0.505571\n",
      "289        418           0     1  0.561229\n",
      "290        419           0     1  0.503127\n",
      "291        421           0     0  0.446493\n",
      "292        423           0     0  0.496050\n",
      "293        425           1     0  0.476126\n",
      "294        426           1     1  0.543564\n",
      "295        429           1     1  0.784535\n",
      "296        430           0     0  0.474746\n",
      "297        431           1     1  0.525434\n",
      "298        432           0     1  0.566180\n",
      "299        433           0     0  0.484119\n",
      "300        436           1     1  0.799446\n",
      "301        440           1     1  0.523887\n",
      "302        441           0     1  0.729096\n",
      "303        442           1     1  0.638297\n",
      "304        443           1     1  0.645225\n",
      "305        444           0     1  0.611853\n",
      "306        445           0     1  0.581817\n",
      "307        446           0     1  0.629472\n",
      "308        449           1     1  0.629526\n",
      "309        451           1     1  0.655550\n",
      "310        452           0     1  0.537379\n",
      "311        454           0     1  0.613836\n",
      "312        455           0     1  0.799407\n",
      "313        456           1     1  0.626659\n",
      "314        457           1     1  0.637341\n",
      "315        459           0     1  0.651454\n",
      "316        464           0     1  0.570518\n",
      "317        466           1     1  0.536158\n",
      "318        468           1     1  0.637880\n",
      "319        469           0     1  0.649415\n",
      "320        470           1     1  0.592861\n",
      "321        472           1     1  0.588367\n",
      "322        477           0     1  0.640431\n",
      "323        478           1     1  0.622484\n",
      "324        479           1     1  0.639123\n",
      "325        480           1     1  0.544929\n",
      "326        481           0     1  0.663977\n",
      "327        483           1     0  0.495441\n",
      "328        485           1     1  0.589193\n",
      "329        488           1     1  0.581058\n",
      "330        491           1     1  0.556864\n",
      "331        493           1     1  0.567222\n",
      "332        494           1     1  0.608798\n",
      "333        495           0     1  0.603059\n",
      "334        496           0     1  0.621160\n",
      "335        498           0     1  0.629079\n",
      "336        499           1     1  0.611089\n",
      "337        500           1     1  0.533421\n",
      "338        501           1     1  0.619897\n",
      "339        502           1     1  0.652183\n",
      "340        504           1     1  0.579471\n",
      "341        505           1     1  0.573134\n",
      "342        506           1     1  0.582117\n",
      "343        507           0     1  0.668213\n",
      "344        510           0     1  0.557043\n",
      "345        511           1     1  0.657392\n",
      "346        512           0     1  0.562543\n",
      "347        513           1     1  0.543823\n",
      "348        514           0     1  0.602104\n",
      "349        516           1     1  0.606188\n",
      "350        517           1     1  0.576715\n",
      "351        518           0     1  0.570983\n",
      "352        519           0     1  0.622212\n",
      "353        520           1     1  0.577858\n",
      "354        523           1     1  0.603298\n",
      "355        524           1     1  0.612033\n",
      "356        525           1     1  0.556187\n",
      "357        526           1     1  0.594232\n",
      "358        528           1     0  0.492806\n",
      "359        529           1     1  0.654561\n",
      "360        530           0     1  0.501374\n",
      "361        532           1     1  0.633891\n",
      "362        533           0     1  0.653150\n",
      "363        537           1     1  0.617390\n",
      "364        538           0     1  0.576356\n",
      "365        539           1     1  0.590549\n",
      "366        540           0     1  0.559277\n",
      "367        542           1     1  0.617361\n",
      "368        543           1     1  0.580416\n",
      "369        544           1     1  0.573782\n",
      "370        545           0     1  0.576837\n",
      "371        547           0     1  0.601203\n",
      "372        548           1     1  0.634788\n",
      "373        549           1     1  0.592766\n",
      "374        550           1     1  0.541275\n",
      "375        551           1     1  0.615764\n",
      "376        552           1     1  0.562470\n",
      "377        554           1     1  0.632578\n",
      "378        555           0     1  0.673057\n",
      "379        556           1     1  0.578783\n",
      "380        557           1     1  0.533114\n",
      "381        558           1     1  0.535312\n",
      "382        559           1     1  0.612927\n",
      "383        561           1     1  0.600596\n",
      "384        563           0     1  0.605951\n",
      "385        564           1     1  0.620948\n",
      "386        565           0     1  0.616337\n",
      "387        567           0     1  0.633179\n",
      "388        568           0     1  0.590839\n",
      "389        569           0     1  0.589262\n",
      "390        570           1     1  0.798868\n",
      "391        571           0     1  0.678095\n",
      "392        572           0     1  0.647609\n",
      "393        574           0     1  0.629537\n",
      "394        575           0     1  0.768402\n",
      "395        576           1     1  0.625490\n",
      "396        577           1     1  0.608852\n",
      "397        578           0     1  0.798203\n",
      "398        579           1     1  0.597468\n",
      "399        581           0     1  0.637224\n",
      "400        582           1     1  0.541419\n",
      "401        583           1     1  0.746653\n",
      "402        584           1     1  0.762572\n",
      "403        586           1     1  0.642528\n",
      "404        587           0     1  0.647395\n",
      "405        588           0     1  0.627807\n",
      "406        589           0     1  0.806791\n",
      "407        590           1     1  0.563660\n",
      "408        591           0     1  0.627892\n",
      "409        593           1     1  0.622621\n",
      "410        594           1     1  0.593412\n",
      "411        596           0     1  0.592137\n",
      "412        597           1     1  0.593952\n",
      "413        598           1     1  0.586008\n",
      "414        599           1     1  0.602154\n",
      "415        601           0     1  0.611391\n",
      "416        602           1     1  0.647469\n",
      "417        604           1     1  0.587330\n",
      "418        605           0     1  0.590255\n",
      "419        606           1     1  0.559018\n",
      "420        607           1     1  0.620068\n",
      "421        608           1     1  0.581877\n",
      "422        610           1     1  0.676797\n",
      "423        611           1     1  0.615547\n",
      "424        612           1     1  0.590950\n",
      "425        613           1     1  0.602755\n",
      "426        615           1     1  0.593074\n",
      "427        616           0     1  0.629618\n",
      "428        618           1     1  0.626776\n",
      "429        619           0     1  0.605723\n",
      "430        620           0     1  0.581678\n",
      "431        621           1     1  0.556253\n",
      "432        622           1     1  0.587381\n",
      "433        623           0     1  0.547228\n",
      "434        624           0     1  0.581972\n",
      "435        625           1     1  0.527201\n",
      "436        626           1     1  0.558585\n",
      "437        628           1     1  0.578996\n",
      "438        630           0     1  0.554236\n",
      "439        631           1     1  0.583825\n",
      "440        636           0     1  0.629162\n",
      "441        638           1     1  0.559203\n",
      "442        639           1     1  0.538116\n",
      "443        640           1     1  0.592990\n",
      "444        641           0     1  0.638061\n",
      "445        642           0     1  0.590376\n",
      "446        645           0     1  0.592451\n",
      "447        646           1     1  0.584939\n",
      "448        649           0     1  0.559863\n",
      "449        650           1     1  0.638851\n",
      "450        651           0     1  0.606242\n",
      "451        652           1     1  0.619957\n",
      "452        654           0     1  0.578709\n",
      "453        655           1     1  0.520770\n",
      "454        656           1     1  0.647084\n",
      "455        657           0     1  0.574289\n",
      "456        658           1     1  0.525137\n",
      "457        659           1     1  0.535407\n",
      "458        661           1     1  0.604928\n",
      "459        663           0     1  0.590595\n",
      "460        667           0     1  0.552158\n",
      "461        668           0     1  0.535856\n",
      "462        674           1     1  0.594703\n",
      "463        675           1     1  0.637785\n",
      "464        676           1     1  0.611107\n",
      "465        677           1     1  0.603969\n",
      "466        679           1     1  0.675556\n",
      "467        680           1     1  0.577404\n",
      "468        682           0     1  0.570451\n",
      "469        683           0     1  0.674823\n",
      "470        684           0     1  0.625781\n",
      "471        685           0     1  0.554672\n",
      "472        686           0     1  0.531775\n",
      "473        687           0     1  0.592067\n",
      "474        688           0     1  0.542031\n",
      "475        690           1     1  0.575270\n",
      "476        691           1     1  0.653325\n",
      "477        692           1     1  0.651457\n",
      "478        693           1     1  0.647881\n",
      "479        694           1     1  0.634995\n",
      "480        697           1     1  0.608386\n",
      "481        698           1     1  0.558272\n",
      "482        703           0     1  0.554392\n",
      "483        704           1     1  0.571961\n",
      "484        705           1     0  0.483876\n",
      "485        706           0     1  0.536599\n",
      "486        707           1     1  0.577940\n",
      "487        708           1     1  0.541728\n",
      "488        709           0     1  0.556768\n",
      "489        714           1     1  0.684644\n",
      "490        715           1     1  0.561971\n",
      "491        716           1     1  0.575924\n",
      "492        718           1     1  0.599023\n",
      "493        723           0     1  0.584068\n",
      "494        724           0     1  0.545726\n",
      "495        725           1     1  0.610526\n",
      "496        727           0     1  0.619126\n",
      "497        728           0     1  0.571967\n",
      "498        729           0     1  0.612547\n",
      "499        730           0     1  0.556853\n",
      "500        731           1     1  0.620050\n",
      "501        732           1     1  0.582682\n",
      "502        733           0     1  0.587866\n",
      "503        734           0     1  0.590077\n",
      "504        735           0     1  0.600134\n",
      "505        736           1     1  0.595954\n",
      "506        737           1     1  0.669469\n",
      "507        739           1     1  0.611051\n",
      "508        740           1     1  0.578844\n",
      "509        742           0     1  0.568704\n",
      "510        744           0     1  0.542768\n",
      "511        746           1     1  0.603085\n",
      "512        747           0     1  0.594574\n",
      "513        750           1     1  0.583672\n",
      "514        751           0     1  0.589053\n",
      "515        753           0     1  0.631004\n",
      "516        756           0     1  0.681390\n",
      "517        757           1     1  0.573448\n",
      "518        758           1     1  0.585595\n",
      "519        759           0     1  0.633674\n",
      "520        760           1     1  0.562012\n",
      "521        764           0     1  0.629091\n",
      "522        765           1     1  0.584536\n",
      "523        767           0     1  0.644989\n",
      "524        768           1     1  0.572273\n",
      "525        772           1     1  0.533281\n",
      "526        773           1     1  0.622056\n",
      "527        774           0     1  0.599237\n",
      "528        775           1     1  0.593743\n",
      "529        777           1     1  0.590409\n",
      "530        778           0     1  0.568860\n",
      "531        780           0     1  0.735396\n",
      "532        781           1     0  0.479083\n",
      "533        782           1     1  0.776201\n",
      "534        784           1     1  0.767684\n",
      "535        787           1     1  0.775187\n",
      "536        788           0     1  0.779780\n",
      "537        789           1     1  0.791026\n",
      "538        791           1     1  0.766368\n",
      "539        792           0     1  0.738601\n",
      "540        793           1     1  0.786920\n",
      "541        794           1     1  0.771836\n",
      "542        795           1     1  0.758486\n",
      "543        796           0     1  0.541837\n",
      "544        797           0     1  0.752216\n",
      "545        799           0     1  0.767766\n",
      "546        800           0     1  0.777385\n",
      "547        801           1     1  0.770359\n",
      "548        802           0     1  0.759514\n",
      "549        803           0     1  0.773736\n",
      "550        804           0     1  0.772434\n",
      "551        805           0     1  0.766796\n",
      "552        806           0     1  0.774736\n",
      "553        807           1     1  0.766911\n",
      "554        808           1     1  0.716765\n",
      "555        809           0     1  0.768249\n",
      "556        810           0     1  0.763567\n",
      "557        811           1     1  0.738230\n",
      "558        814           0     1  0.739817\n",
      "559        816           1     1  0.772509\n",
      "560        818           0     1  0.740836\n",
      "561        819           1     1  0.701333\n",
      "562        820           0     0  0.493046\n",
      "563        823           1     1  0.769907\n",
      "564        824           0     1  0.778850\n",
      "565        828           1     1  0.572101\n",
      "566        830           0     1  0.793189\n",
      "567        834           0     0  0.496077\n",
      "568        836           0     1  0.622107\n",
      "569        837           0     1  0.544203\n",
      "570        838           1     1  0.517817\n",
      "571        839           0     1  0.517329\n",
      "572        840           1     1  0.502199\n",
      "573        998           1     1  0.770435\n",
      "574        999           1     1  0.759195\n",
      "575       1000           1     0  0.449564\n",
      "576       1001           1     0  0.489223\n",
      "577       1002           1     1  0.732162\n",
      "578       1003           1     1  0.756451\n",
      "579       1004           0     1  0.503574\n",
      "580       1005           1     1  0.672044\n",
      "581       1007           1     1  0.613856\n",
      "582       1008           1     1  0.535882\n",
      "583       1009           0     1  0.732722\n",
      "584       1010           0     1  0.513689\n",
      "Prediction AUC: 0.5274\n",
      "Prediction Accuracy: 0.5231\n",
      "Prediction Specificity: 0.1331\n",
      "Prediction Sensitivity: 0.8762\n",
      "Prediction Precision: 0.5275\n",
      "the score of the fold number 3 and the type T1wCE: 0.5274178051695452\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     3  0.527418  0.523077  0.133094  0.876221  0.527451\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 585/585 [01:05<00:00,  8.99it/s]\n",
      "Fold 4:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            0           1     1  0.520211\n",
      "1            2           1     1  0.522239\n",
      "2            3           0     1  0.520381\n",
      "3            5           1     1  0.517078\n",
      "4            6           1     1  0.525472\n",
      "5            8           1     1  0.517886\n",
      "6            9           0     1  0.515122\n",
      "7           11           1     0  0.475387\n",
      "8           12           1     1  0.523180\n",
      "9           14           1     1  0.515410\n",
      "10          17           0     1  0.510188\n",
      "11          18           0     1  0.503522\n",
      "12          19           0     1  0.515236\n",
      "13          20           1     1  0.510984\n",
      "14          21           0     1  0.516436\n",
      "15          22           0     1  0.518480\n",
      "16          24           0     0  0.499119\n",
      "17          25           1     1  0.509448\n",
      "18          26           1     1  0.507544\n",
      "19          28           1     1  0.508662\n",
      "20          30           0     1  0.505449\n",
      "21          31           1     1  0.531020\n",
      "22          32           0     1  0.515930\n",
      "23          33           1     1  0.505667\n",
      "24          35           1     1  0.515665\n",
      "25          36           0     1  0.517666\n",
      "26          43           1     1  0.523410\n",
      "27          44           0     1  0.500934\n",
      "28          45           0     1  0.510852\n",
      "29          46           1     1  0.506225\n",
      "30          48           1     1  0.510329\n",
      "31          49           0     0  0.475094\n",
      "32          52           1     0  0.492384\n",
      "33          53           0     1  0.514218\n",
      "34          54           1     1  0.521972\n",
      "35          56           1     1  0.520647\n",
      "36          58           1     1  0.513205\n",
      "37          59           1     0  0.499772\n",
      "38          60           1     0  0.497520\n",
      "39          61           0     1  0.522873\n",
      "40          62           1     1  0.508252\n",
      "41          63           1     1  0.519558\n",
      "42          64           0     1  0.526644\n",
      "43          66           1     1  0.519779\n",
      "44          68           1     0  0.486121\n",
      "45          70           1     0  0.490735\n",
      "46          71           1     1  0.507980\n",
      "47          72           0     0  0.493288\n",
      "48          74           1     1  0.511510\n",
      "49          77           1     1  0.515801\n",
      "50          78           1     1  0.512985\n",
      "51          81           0     1  0.503132\n",
      "52          84           0     1  0.515774\n",
      "53          85           1     1  0.505797\n",
      "54          87           1     1  0.522737\n",
      "55          88           0     1  0.502630\n",
      "56          89           1     1  0.525884\n",
      "57          90           0     1  0.520847\n",
      "58          94           1     1  0.502120\n",
      "59          95           0     1  0.514053\n",
      "60          96           1     1  0.515294\n",
      "61          97           0     1  0.515469\n",
      "62          98           1     1  0.530723\n",
      "63          99           0     1  0.511547\n",
      "64         100           1     0  0.439290\n",
      "65         102           0     0  0.375153\n",
      "66         104           0     0  0.410308\n",
      "67         105           1     1  0.506462\n",
      "68         106           1     0  0.417068\n",
      "69         107           1     0  0.455094\n",
      "70         108           0     0  0.444200\n",
      "71         109           1     0  0.391665\n",
      "72         110           0     0  0.485923\n",
      "73         111           0     0  0.434503\n",
      "74         112           0     0  0.308768\n",
      "75         113           0     0  0.460457\n",
      "76         116           0     1  0.530306\n",
      "77         117           1     0  0.463690\n",
      "78         120           1     0  0.463392\n",
      "79         121           0     0  0.340677\n",
      "80         122           0     0  0.443974\n",
      "81         123           0     0  0.384321\n",
      "82         124           0     0  0.412962\n",
      "83         128           1     0  0.344647\n",
      "84         130           0     0  0.474168\n",
      "85         132           0     0  0.468391\n",
      "86         133           0     0  0.419389\n",
      "87         134           1     0  0.322980\n",
      "88         136           1     0  0.460505\n",
      "89         137           0     0  0.415437\n",
      "90         138           1     0  0.469508\n",
      "91         139           1     1  0.517559\n",
      "92         140           1     0  0.433386\n",
      "93         142           0     0  0.466108\n",
      "94         143           1     0  0.467975\n",
      "95         144           1     0  0.434564\n",
      "96         146           1     0  0.472218\n",
      "97         147           0     0  0.462269\n",
      "98         148           0     1  0.536004\n",
      "99         149           0     1  0.543442\n",
      "100        150           0     0  0.434617\n",
      "101        151           0     0  0.415045\n",
      "102        154           0     1  0.502227\n",
      "103        155           1     0  0.478316\n",
      "104        156           1     0  0.493066\n",
      "105        157           0     1  0.500964\n",
      "106        158           0     0  0.489717\n",
      "107        159           1     0  0.496513\n",
      "108        160           1     0  0.493342\n",
      "109        162           0     0  0.491973\n",
      "110        165           0     0  0.480190\n",
      "111        166           1     1  0.506775\n",
      "112        167           0     0  0.483034\n",
      "113        169           0     0  0.390032\n",
      "114        170           0     0  0.487487\n",
      "115        171           1     0  0.496256\n",
      "116        172           0     0  0.487761\n",
      "117        176           0     0  0.499110\n",
      "118        177           1     0  0.490617\n",
      "119        178           1     0  0.498801\n",
      "120        183           0     0  0.480151\n",
      "121        184           0     0  0.479908\n",
      "122        185           1     1  0.502381\n",
      "123        186           1     0  0.484072\n",
      "124        187           1     0  0.489907\n",
      "125        188           1     0  0.481015\n",
      "126        191           0     0  0.484273\n",
      "127        192           0     0  0.481732\n",
      "128        193           0     0  0.435992\n",
      "129        194           0     0  0.494676\n",
      "130        195           0     0  0.488084\n",
      "131        196           1     0  0.496863\n",
      "132        197           1     0  0.486525\n",
      "133        199           1     0  0.494823\n",
      "134        201           0     0  0.492582\n",
      "135        203           1     0  0.491688\n",
      "136        204           1     0  0.447737\n",
      "137        206           0     0  0.485155\n",
      "138        209           0     0  0.497542\n",
      "139        210           1     0  0.491465\n",
      "140        211           0     0  0.485603\n",
      "141        212           1     0  0.481036\n",
      "142        214           0     0  0.485273\n",
      "143        216           0     0  0.472460\n",
      "144        217           0     0  0.427885\n",
      "145        218           0     0  0.488857\n",
      "146        219           0     1  0.501480\n",
      "147        220           1     0  0.480417\n",
      "148        221           0     0  0.487600\n",
      "149        222           1     0  0.499873\n",
      "150        227           0     1  0.505209\n",
      "151        228           0     0  0.485714\n",
      "152        230           1     0  0.473822\n",
      "153        231           0     0  0.499184\n",
      "154        233           1     0  0.484604\n",
      "155        234           1     0  0.479776\n",
      "156        235           1     1  0.504393\n",
      "157        236           0     0  0.499071\n",
      "158        237           0     0  0.485763\n",
      "159        238           0     0  0.476114\n",
      "160        239           0     0  0.481076\n",
      "161        240           1     1  0.505134\n",
      "162        241           0     0  0.484319\n",
      "163        242           0     0  0.483949\n",
      "164        243           0     0  0.439309\n",
      "165        245           1     0  0.491901\n",
      "166        246           1     1  0.500855\n",
      "167        247           0     0  0.494407\n",
      "168        249           0     0  0.485273\n",
      "169        250           1     1  0.503511\n",
      "170        251           0     0  0.495281\n",
      "171        253           1     0  0.490869\n",
      "172        254           1     0  0.488684\n",
      "173        258           0     1  0.548155\n",
      "174        259           0     0  0.484850\n",
      "175        260           1     0  0.482677\n",
      "176        261           0     0  0.485758\n",
      "177        262           0     1  0.535452\n",
      "178        263           1     0  0.473207\n",
      "179        266           0     0  0.495170\n",
      "180        267           0     0  0.478764\n",
      "181        269           0     1  0.510438\n",
      "182        270           1     0  0.456374\n",
      "183        271           1     1  0.501925\n",
      "184        273           1     0  0.479831\n",
      "185        274           0     1  0.503391\n",
      "186        275           0     1  0.564015\n",
      "187        280           0     0  0.496612\n",
      "188        281           1     0  0.451606\n",
      "189        282           1     0  0.493931\n",
      "190        283           0     0  0.487251\n",
      "191        284           1     0  0.492542\n",
      "192        285           1     0  0.493677\n",
      "193        286           0     0  0.490425\n",
      "194        288           0     0  0.451669\n",
      "195        289           0     0  0.493331\n",
      "196        290           0     0  0.484438\n",
      "197        291           1     0  0.460828\n",
      "198        293           1     0  0.487285\n",
      "199        294           1     1  0.507489\n",
      "200        296           1     0  0.498961\n",
      "201        297           0     0  0.484199\n",
      "202        298           0     0  0.482979\n",
      "203        299           1     0  0.427346\n",
      "204        300           0     0  0.488184\n",
      "205        301           0     0  0.478967\n",
      "206        303           1     0  0.492932\n",
      "207        304           1     0  0.484216\n",
      "208        305           1     0  0.486010\n",
      "209        306           1     0  0.473445\n",
      "210        308           0     0  0.475327\n",
      "211        309           0     0  0.492098\n",
      "212        310           0     0  0.469325\n",
      "213        311           1     0  0.494458\n",
      "214        312           0     1  0.502548\n",
      "215        313           1     0  0.496221\n",
      "216        314           0     0  0.469597\n",
      "217        316           0     0  0.485817\n",
      "218        317           1     0  0.459113\n",
      "219        318           0     0  0.483547\n",
      "220        320           0     0  0.490084\n",
      "221        321           1     0  0.488307\n",
      "222        322           1     0  0.495000\n",
      "223        324           0     0  0.494844\n",
      "224        325           0     0  0.406525\n",
      "225        327           0     0  0.490794\n",
      "226        328           1     0  0.469993\n",
      "227        329           1     0  0.478067\n",
      "228        331           1     0  0.472571\n",
      "229        332           1     0  0.497711\n",
      "230        334           1     0  0.496967\n",
      "231        336           0     0  0.490586\n",
      "232        338           1     1  0.528172\n",
      "233        339           0     0  0.496067\n",
      "234        340           1     0  0.486345\n",
      "235        341           0     0  0.482784\n",
      "236        343           0     0  0.494761\n",
      "237        344           1     0  0.496349\n",
      "238        346           0     0  0.494349\n",
      "239        347           0     0  0.482105\n",
      "240        348           0     0  0.498268\n",
      "241        349           0     1  0.541453\n",
      "242        350           1     0  0.381054\n",
      "243        351           0     0  0.495392\n",
      "244        352           1     0  0.483304\n",
      "245        353           0     0  0.488732\n",
      "246        356           0     0  0.488228\n",
      "247        359           1     1  0.508790\n",
      "248        360           1     0  0.495309\n",
      "249        364           1     0  0.499865\n",
      "250        366           1     0  0.428083\n",
      "251        367           1     0  0.398330\n",
      "252        369           1     0  0.476299\n",
      "253        370           1     0  0.494090\n",
      "254        371           1     0  0.492864\n",
      "255        373           0     0  0.482137\n",
      "256        376           0     0  0.401541\n",
      "257        377           0     1  0.551935\n",
      "258        378           0     1  0.502308\n",
      "259        379           0     0  0.494316\n",
      "260        380           0     0  0.489576\n",
      "261        382           0     0  0.498238\n",
      "262        383           1     0  0.466793\n",
      "263        386           1     0  0.492701\n",
      "264        387           0     0  0.499407\n",
      "265        388           0     0  0.391760\n",
      "266        389           0     0  0.496416\n",
      "267        390           0     1  0.508529\n",
      "268        391           0     0  0.436994\n",
      "269        392           0     0  0.496629\n",
      "270        395           0     0  0.496294\n",
      "271        397           0     0  0.492961\n",
      "272        399           0     0  0.486042\n",
      "273        400           1     1  0.501024\n",
      "274        401           0     0  0.497133\n",
      "275        402           0     0  0.476711\n",
      "276        403           1     0  0.450140\n",
      "277        404           1     0  0.496809\n",
      "278        405           0     1  0.502574\n",
      "279        406           1     0  0.491005\n",
      "280        407           0     0  0.482418\n",
      "281        408           1     1  0.500552\n",
      "282        409           1     0  0.495741\n",
      "283        410           0     0  0.491059\n",
      "284        412           0     0  0.488160\n",
      "285        413           1     0  0.498429\n",
      "286        414           0     0  0.493370\n",
      "287        416           1     0  0.478473\n",
      "288        417           0     0  0.497107\n",
      "289        418           0     0  0.458554\n",
      "290        419           0     0  0.475139\n",
      "291        421           0     0  0.488654\n",
      "292        423           0     0  0.483700\n",
      "293        425           1     0  0.481739\n",
      "294        426           1     0  0.483127\n",
      "295        429           1     1  0.503543\n",
      "296        430           0     0  0.492555\n",
      "297        431           1     0  0.487532\n",
      "298        432           0     1  0.500676\n",
      "299        433           0     0  0.494469\n",
      "300        436           1     0  0.474002\n",
      "301        440           1     0  0.492410\n",
      "302        441           0     1  0.522215\n",
      "303        442           1     0  0.468819\n",
      "304        443           1     0  0.496160\n",
      "305        444           0     0  0.493214\n",
      "306        445           0     0  0.487249\n",
      "307        446           0     0  0.468785\n",
      "308        449           1     0  0.489778\n",
      "309        451           1     0  0.492568\n",
      "310        452           0     0  0.498461\n",
      "311        454           0     1  0.516197\n",
      "312        455           0     1  0.514650\n",
      "313        456           1     0  0.487086\n",
      "314        457           1     0  0.456560\n",
      "315        459           0     0  0.341273\n",
      "316        464           0     0  0.479501\n",
      "317        466           1     1  0.529584\n",
      "318        468           1     1  0.518960\n",
      "319        469           0     1  0.519915\n",
      "320        470           1     1  0.514318\n",
      "321        472           1     1  0.506013\n",
      "322        477           0     1  0.517049\n",
      "323        478           1     1  0.506237\n",
      "324        479           1     0  0.497485\n",
      "325        480           1     1  0.518943\n",
      "326        481           0     1  0.515284\n",
      "327        483           1     1  0.519412\n",
      "328        485           1     1  0.515851\n",
      "329        488           1     1  0.512806\n",
      "330        491           1     1  0.518699\n",
      "331        493           1     1  0.519064\n",
      "332        494           1     1  0.524735\n",
      "333        495           0     1  0.506652\n",
      "334        496           0     1  0.502024\n",
      "335        498           0     1  0.512959\n",
      "336        499           1     0  0.490080\n",
      "337        500           1     1  0.520520\n",
      "338        501           1     1  0.515962\n",
      "339        502           1     1  0.503820\n",
      "340        504           1     1  0.513237\n",
      "341        505           1     1  0.515938\n",
      "342        506           1     1  0.507576\n",
      "343        507           0     1  0.516119\n",
      "344        510           0     1  0.515492\n",
      "345        511           1     1  0.500532\n",
      "346        512           0     1  0.527722\n",
      "347        513           1     1  0.519804\n",
      "348        514           0     1  0.513847\n",
      "349        516           1     1  0.502827\n",
      "350        517           1     1  0.501725\n",
      "351        518           0     1  0.515001\n",
      "352        519           0     0  0.486598\n",
      "353        520           1     1  0.523787\n",
      "354        523           1     1  0.509068\n",
      "355        524           1     1  0.511935\n",
      "356        525           1     1  0.511867\n",
      "357        526           1     0  0.480041\n",
      "358        528           1     0  0.495348\n",
      "359        529           1     1  0.507296\n",
      "360        530           0     1  0.508300\n",
      "361        532           1     1  0.523414\n",
      "362        533           0     0  0.495829\n",
      "363        537           1     0  0.482113\n",
      "364        538           0     1  0.526611\n",
      "365        539           1     1  0.517167\n",
      "366        540           0     1  0.521210\n",
      "367        542           1     1  0.512042\n",
      "368        543           1     0  0.498689\n",
      "369        544           1     1  0.522358\n",
      "370        545           0     1  0.521851\n",
      "371        547           0     1  0.500185\n",
      "372        548           1     1  0.521763\n",
      "373        549           1     1  0.504209\n",
      "374        550           1     1  0.514672\n",
      "375        551           1     1  0.512514\n",
      "376        552           1     1  0.522588\n",
      "377        554           1     1  0.533542\n",
      "378        555           0     1  0.508314\n",
      "379        556           1     0  0.491312\n",
      "380        557           1     1  0.524169\n",
      "381        558           1     1  0.517276\n",
      "382        559           1     1  0.502477\n",
      "383        561           1     1  0.502424\n",
      "384        563           0     0  0.486770\n",
      "385        564           1     1  0.509214\n",
      "386        565           0     0  0.495579\n",
      "387        567           0     0  0.498909\n",
      "388        568           0     0  0.437936\n",
      "389        569           0     0  0.489761\n",
      "390        570           1     1  0.561487\n",
      "391        571           0     0  0.259213\n",
      "392        572           0     0  0.470743\n",
      "393        574           0     1  0.502887\n",
      "394        575           0     1  0.501667\n",
      "395        576           1     0  0.396204\n",
      "396        577           1     0  0.494960\n",
      "397        578           0     1  0.544723\n",
      "398        579           1     1  0.506559\n",
      "399        581           0     0  0.482068\n",
      "400        582           1     0  0.483334\n",
      "401        583           1     0  0.488904\n",
      "402        584           1     0  0.430296\n",
      "403        586           1     0  0.492798\n",
      "404        587           0     0  0.483524\n",
      "405        588           0     0  0.491753\n",
      "406        589           0     0  0.480342\n",
      "407        590           1     0  0.496572\n",
      "408        591           0     1  0.503005\n",
      "409        593           1     0  0.499295\n",
      "410        594           1     0  0.457155\n",
      "411        596           0     0  0.499758\n",
      "412        597           1     0  0.498353\n",
      "413        598           1     0  0.494670\n",
      "414        599           1     0  0.492037\n",
      "415        601           0     1  0.509871\n",
      "416        602           1     1  0.515713\n",
      "417        604           1     1  0.517471\n",
      "418        605           0     1  0.501132\n",
      "419        606           1     0  0.490609\n",
      "420        607           1     0  0.486660\n",
      "421        608           1     1  0.504694\n",
      "422        610           1     1  0.515759\n",
      "423        611           1     1  0.522914\n",
      "424        612           1     1  0.507255\n",
      "425        613           1     1  0.518150\n",
      "426        615           1     1  0.516749\n",
      "427        616           0     0  0.493662\n",
      "428        618           1     1  0.503185\n",
      "429        619           0     0  0.499388\n",
      "430        620           0     1  0.513341\n",
      "431        621           1     1  0.511446\n",
      "432        622           1     1  0.521945\n",
      "433        623           0     1  0.515565\n",
      "434        624           0     1  0.525605\n",
      "435        625           1     1  0.502882\n",
      "436        626           1     1  0.514494\n",
      "437        628           1     1  0.514177\n",
      "438        630           0     1  0.520387\n",
      "439        631           1     1  0.511409\n",
      "440        636           0     1  0.508584\n",
      "441        638           1     1  0.524320\n",
      "442        639           1     1  0.514580\n",
      "443        640           1     1  0.523017\n",
      "444        641           0     0  0.499949\n",
      "445        642           0     1  0.510068\n",
      "446        645           0     1  0.512123\n",
      "447        646           1     1  0.513018\n",
      "448        649           0     1  0.513316\n",
      "449        650           1     0  0.492449\n",
      "450        651           0     1  0.515090\n",
      "451        652           1     0  0.475057\n",
      "452        654           0     1  0.526448\n",
      "453        655           1     1  0.501376\n",
      "454        656           1     0  0.431372\n",
      "455        657           0     1  0.506348\n",
      "456        658           1     1  0.506064\n",
      "457        659           1     1  0.518862\n",
      "458        661           1     1  0.519142\n",
      "459        663           0     1  0.508518\n",
      "460        667           0     1  0.507831\n",
      "461        668           0     1  0.523505\n",
      "462        674           1     1  0.523283\n",
      "463        675           1     1  0.520033\n",
      "464        676           1     1  0.510684\n",
      "465        677           1     0  0.495503\n",
      "466        679           1     0  0.426846\n",
      "467        680           1     1  0.506730\n",
      "468        682           0     1  0.512220\n",
      "469        683           0     0  0.498325\n",
      "470        684           0     1  0.510645\n",
      "471        685           0     1  0.511257\n",
      "472        686           0     1  0.522386\n",
      "473        687           0     1  0.522216\n",
      "474        688           0     1  0.505232\n",
      "475        690           1     1  0.526027\n",
      "476        691           1     0  0.494272\n",
      "477        692           1     1  0.500480\n",
      "478        693           1     1  0.519109\n",
      "479        694           1     1  0.505890\n",
      "480        697           1     1  0.506567\n",
      "481        698           1     1  0.516734\n",
      "482        703           0     1  0.518607\n",
      "483        704           1     1  0.525575\n",
      "484        705           1     1  0.512297\n",
      "485        706           0     1  0.514971\n",
      "486        707           1     1  0.500936\n",
      "487        708           1     1  0.520544\n",
      "488        709           0     1  0.520613\n",
      "489        714           1     1  0.522171\n",
      "490        715           1     1  0.515452\n",
      "491        716           1     1  0.512305\n",
      "492        718           1     1  0.507888\n",
      "493        723           0     1  0.518086\n",
      "494        724           0     1  0.521601\n",
      "495        725           1     1  0.517373\n",
      "496        727           0     1  0.500958\n",
      "497        728           0     1  0.514187\n",
      "498        729           0     1  0.503836\n",
      "499        730           0     1  0.535414\n",
      "500        731           1     1  0.516136\n",
      "501        732           1     1  0.527171\n",
      "502        733           0     0  0.499737\n",
      "503        734           0     0  0.493198\n",
      "504        735           0     1  0.505925\n",
      "505        736           1     1  0.514717\n",
      "506        737           1     1  0.513381\n",
      "507        739           1     0  0.492600\n",
      "508        740           1     1  0.520989\n",
      "509        742           0     1  0.513311\n",
      "510        744           0     1  0.502169\n",
      "511        746           1     1  0.519659\n",
      "512        747           0     1  0.505040\n",
      "513        750           1     1  0.516310\n",
      "514        751           0     0  0.497866\n",
      "515        753           0     0  0.475485\n",
      "516        756           0     0  0.490720\n",
      "517        757           1     1  0.521925\n",
      "518        758           1     1  0.503219\n",
      "519        759           0     0  0.485299\n",
      "520        760           1     1  0.501630\n",
      "521        764           0     0  0.486291\n",
      "522        765           1     1  0.527703\n",
      "523        767           0     1  0.512446\n",
      "524        768           1     1  0.508554\n",
      "525        772           1     1  0.519269\n",
      "526        773           1     1  0.509928\n",
      "527        774           0     1  0.500448\n",
      "528        775           1     1  0.513675\n",
      "529        777           1     1  0.517759\n",
      "530        778           0     1  0.520866\n",
      "531        780           0     0  0.438358\n",
      "532        781           1     0  0.465208\n",
      "533        782           1     1  0.560299\n",
      "534        784           1     1  0.550168\n",
      "535        787           1     1  0.533632\n",
      "536        788           0     1  0.556193\n",
      "537        789           1     1  0.552365\n",
      "538        791           1     1  0.508273\n",
      "539        792           0     1  0.555179\n",
      "540        793           1     1  0.552099\n",
      "541        794           1     1  0.545874\n",
      "542        795           1     1  0.519106\n",
      "543        796           0     0  0.491915\n",
      "544        797           0     0  0.499600\n",
      "545        799           0     1  0.541600\n",
      "546        800           0     1  0.534963\n",
      "547        801           1     1  0.525099\n",
      "548        802           0     1  0.522888\n",
      "549        803           0     1  0.535918\n",
      "550        804           0     1  0.525122\n",
      "551        805           0     0  0.495067\n",
      "552        806           0     1  0.547243\n",
      "553        807           1     1  0.560495\n",
      "554        808           1     1  0.516642\n",
      "555        809           0     1  0.570684\n",
      "556        810           0     1  0.529966\n",
      "557        811           1     0  0.473460\n",
      "558        814           0     0  0.493445\n",
      "559        816           1     1  0.549156\n",
      "560        818           0     1  0.551914\n",
      "561        819           1     1  0.547480\n",
      "562        820           0     0  0.460759\n",
      "563        823           1     1  0.583043\n",
      "564        824           0     1  0.549832\n",
      "565        828           1     0  0.489322\n",
      "566        830           0     1  0.572264\n",
      "567        834           0     1  0.503032\n",
      "568        836           0     1  0.521075\n",
      "569        837           0     0  0.494416\n",
      "570        838           1     0  0.496923\n",
      "571        839           0     0  0.497470\n",
      "572        840           1     0  0.481499\n",
      "573        998           1     1  0.518915\n",
      "574        999           1     0  0.352177\n",
      "575       1000           1     0  0.493918\n",
      "576       1001           1     0  0.487712\n",
      "577       1002           1     0  0.415432\n",
      "578       1003           1     0  0.420685\n",
      "579       1004           0     0  0.493292\n",
      "580       1005           1     0  0.370884\n",
      "581       1007           1     0  0.465980\n",
      "582       1008           1     0  0.491380\n",
      "583       1009           0     0  0.339901\n",
      "584       1010           0     1  0.502062\n",
      "Prediction AUC: 0.5521\n",
      "Prediction Accuracy: 0.5538\n",
      "Prediction Specificity: 0.5719\n",
      "Prediction Sensitivity: 0.5375\n",
      "Prediction Precision: 0.5810\n",
      "the score of the fold number 4 and the type T1wCE: 0.5521055468328919\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     4  0.552106  0.553846  0.571942  0.537459  0.580986\n",
      "Prediction AUC: 0.5083\n",
      "Prediction Accuracy: 0.0000\n",
      "Prediction Specificity: 0.0000\n",
      "Prediction Sensitivity: 0.0000\n",
      "Prediction Precision: 0.0000\n",
      "the final socre of the type T1wCE\n",
      "0.508307360626157\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0   all  0.508307  0.519316  0.283453  0.732899  0.529042\n",
      "\n",
      "\n",
      "\n",
      "the final score is\n",
      "0.508307360626157\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/validation.py --models_folder tunisiaai_B --csv_file train_fold.csv --full_set True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 585/585 [01:18<00:00,  7.45it/s]\n",
      "Fold 0:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            0           1     1  0.593086\n",
      "1            2           1     1  0.627343\n",
      "2            3           0     1  0.603141\n",
      "3            5           1     1  0.588373\n",
      "4            6           1     1  0.577472\n",
      "5            8           1     1  0.581511\n",
      "6            9           0     1  0.658364\n",
      "7           11           1     1  0.830363\n",
      "8           12           1     1  0.559150\n",
      "9           14           1     1  0.590464\n",
      "10          17           0     1  0.612659\n",
      "11          18           0     1  0.626219\n",
      "12          19           0     1  0.563840\n",
      "13          20           1     1  0.573405\n",
      "14          21           0     1  0.598406\n",
      "15          22           0     1  0.562067\n",
      "16          24           0     1  0.555749\n",
      "17          25           1     1  0.566412\n",
      "18          26           1     1  0.665934\n",
      "19          28           1     1  0.575078\n",
      "20          30           0     1  0.707434\n",
      "21          31           1     1  0.578549\n",
      "22          32           0     1  0.530748\n",
      "23          33           1     1  0.583853\n",
      "24          35           1     1  0.549088\n",
      "25          36           0     1  0.606475\n",
      "26          43           1     1  0.549327\n",
      "27          44           0     1  0.548145\n",
      "28          45           0     1  0.697236\n",
      "29          46           1     1  0.610430\n",
      "30          48           1     1  0.545864\n",
      "31          49           0     1  0.844325\n",
      "32          52           1     1  0.598444\n",
      "33          53           0     1  0.638470\n",
      "34          54           1     1  0.590171\n",
      "35          56           1     1  0.650239\n",
      "36          58           1     1  0.565164\n",
      "37          59           1     1  0.625144\n",
      "38          60           1     1  0.657449\n",
      "39          61           0     1  0.523776\n",
      "40          62           1     1  0.584370\n",
      "41          63           1     1  0.595462\n",
      "42          64           0     1  0.586775\n",
      "43          66           1     1  0.595773\n",
      "44          68           1     1  0.674732\n",
      "45          70           1     1  0.688277\n",
      "46          71           1     1  0.607872\n",
      "47          72           0     1  0.693168\n",
      "48          74           1     1  0.534104\n",
      "49          77           1     1  0.616572\n",
      "50          78           1     1  0.621502\n",
      "51          81           0     1  0.659580\n",
      "52          84           0     1  0.598006\n",
      "53          85           1     1  0.589149\n",
      "54          87           1     1  0.599624\n",
      "55          88           0     1  0.609003\n",
      "56          89           1     1  0.587048\n",
      "57          90           0     1  0.529885\n",
      "58          94           1     1  0.590088\n",
      "59          95           0     1  0.583224\n",
      "60          96           1     1  0.615210\n",
      "61          97           0     1  0.568854\n",
      "62          98           1     1  0.590858\n",
      "63          99           0     1  0.584016\n",
      "64         100           1     0  0.357209\n",
      "65         102           0     1  0.657659\n",
      "66         104           0     1  0.781938\n",
      "67         105           1     1  0.725614\n",
      "68         106           1     1  0.919354\n",
      "69         107           1     1  0.574353\n",
      "70         108           0     1  0.950270\n",
      "71         109           1     1  0.694899\n",
      "72         110           0     1  0.867681\n",
      "73         111           0     1  0.950032\n",
      "74         112           0     0  0.465253\n",
      "75         113           0     1  0.530925\n",
      "76         116           0     1  0.596296\n",
      "77         117           1     1  0.930984\n",
      "78         120           1     1  0.876037\n",
      "79         121           0     1  0.945350\n",
      "80         122           0     1  0.916691\n",
      "81         123           0     1  0.654342\n",
      "82         124           0     1  0.944435\n",
      "83         128           1     1  0.787119\n",
      "84         130           0     1  0.846986\n",
      "85         132           0     1  0.820529\n",
      "86         133           0     1  0.913318\n",
      "87         134           1     1  0.927637\n",
      "88         136           1     1  0.757835\n",
      "89         137           0     1  0.860900\n",
      "90         138           1     1  0.899493\n",
      "91         139           1     0  0.315211\n",
      "92         140           1     1  0.810826\n",
      "93         142           0     1  0.722631\n",
      "94         143           1     1  0.863048\n",
      "95         144           1     1  0.815165\n",
      "96         146           1     1  0.770329\n",
      "97         147           0     1  0.730708\n",
      "98         148           0     1  0.870952\n",
      "99         149           0     0  0.335006\n",
      "100        150           0     1  0.710724\n",
      "101        151           0     1  0.729930\n",
      "102        154           0     1  0.501268\n",
      "103        155           1     1  0.679265\n",
      "104        156           1     1  0.777344\n",
      "105        157           0     0  0.493304\n",
      "106        158           0     1  0.768895\n",
      "107        159           1     1  0.605509\n",
      "108        160           1     1  0.745333\n",
      "109        162           0     1  0.709327\n",
      "110        165           0     1  0.671220\n",
      "111        166           1     1  0.651830\n",
      "112        167           0     1  0.675196\n",
      "113        169           0     1  0.843466\n",
      "114        170           0     1  0.774215\n",
      "115        171           1     1  0.680401\n",
      "116        172           0     1  0.601538\n",
      "117        176           0     1  0.584777\n",
      "118        177           1     1  0.578981\n",
      "119        178           1     1  0.690469\n",
      "120        183           0     1  0.797474\n",
      "121        184           0     1  0.614980\n",
      "122        185           1     1  0.545437\n",
      "123        186           1     1  0.610084\n",
      "124        187           1     1  0.811047\n",
      "125        188           1     1  0.622676\n",
      "126        191           0     1  0.712219\n",
      "127        192           0     1  0.819151\n",
      "128        193           0     1  0.624014\n",
      "129        194           0     1  0.601750\n",
      "130        195           0     1  0.616990\n",
      "131        196           1     1  0.787449\n",
      "132        197           1     1  0.663471\n",
      "133        199           1     1  0.519231\n",
      "134        201           0     1  0.610689\n",
      "135        203           1     1  0.603196\n",
      "136        204           1     1  0.955477\n",
      "137        206           0     1  0.930814\n",
      "138        209           0     1  0.633779\n",
      "139        210           1     1  0.633051\n",
      "140        211           0     1  0.661877\n",
      "141        212           1     1  0.757898\n",
      "142        214           0     1  0.613686\n",
      "143        216           0     1  0.876365\n",
      "144        217           0     1  0.972084\n",
      "145        218           0     1  0.712495\n",
      "146        219           0     1  0.619945\n",
      "147        220           1     1  0.744135\n",
      "148        221           0     1  0.672122\n",
      "149        222           1     1  0.630345\n",
      "150        227           0     1  0.518818\n",
      "151        228           0     1  0.718287\n",
      "152        230           1     1  0.827515\n",
      "153        231           0     1  0.578842\n",
      "154        233           1     1  0.761378\n",
      "155        234           1     1  0.720774\n",
      "156        235           1     1  0.660543\n",
      "157        236           0     1  0.623265\n",
      "158        237           0     1  0.710452\n",
      "159        238           0     1  0.783811\n",
      "160        239           0     1  0.704822\n",
      "161        240           1     1  0.561075\n",
      "162        241           0     1  0.712463\n",
      "163        242           0     1  0.746735\n",
      "164        243           0     1  0.767638\n",
      "165        245           1     1  0.636793\n",
      "166        246           1     1  0.567614\n",
      "167        247           0     1  0.753206\n",
      "168        249           0     1  0.708448\n",
      "169        250           1     1  0.596224\n",
      "170        251           0     1  0.675132\n",
      "171        253           1     1  0.685134\n",
      "172        254           1     1  0.723861\n",
      "173        258           0     1  0.776087\n",
      "174        259           0     1  0.698607\n",
      "175        260           1     1  0.805574\n",
      "176        261           0     1  0.616499\n",
      "177        262           0     1  0.622377\n",
      "178        263           1     1  0.760490\n",
      "179        266           0     1  0.771561\n",
      "180        267           0     1  0.616609\n",
      "181        269           0     1  0.785398\n",
      "182        270           1     1  0.654409\n",
      "183        271           1     1  0.613872\n",
      "184        273           1     1  0.785996\n",
      "185        274           0     1  0.602516\n",
      "186        275           0     1  0.707297\n",
      "187        280           0     1  0.609223\n",
      "188        281           1     1  0.928669\n",
      "189        282           1     1  0.568670\n",
      "190        283           0     1  0.649177\n",
      "191        284           1     1  0.731863\n",
      "192        285           1     1  0.566351\n",
      "193        286           0     0  0.499631\n",
      "194        288           0     1  0.926550\n",
      "195        289           0     1  0.686605\n",
      "196        290           0     1  0.720894\n",
      "197        291           1     1  0.792940\n",
      "198        293           1     1  0.643633\n",
      "199        294           1     1  0.636425\n",
      "200        296           1     1  0.651997\n",
      "201        297           0     1  0.753600\n",
      "202        298           0     1  0.789185\n",
      "203        299           1     1  0.967221\n",
      "204        300           0     1  0.671888\n",
      "205        301           0     1  0.769915\n",
      "206        303           1     1  0.762258\n",
      "207        304           1     1  0.685538\n",
      "208        305           1     1  0.822770\n",
      "209        306           1     1  0.792989\n",
      "210        308           0     1  0.746597\n",
      "211        309           0     1  0.621456\n",
      "212        310           0     1  0.774900\n",
      "213        311           1     1  0.619131\n",
      "214        312           0     1  0.789362\n",
      "215        313           1     1  0.583011\n",
      "216        314           0     1  0.757212\n",
      "217        316           0     1  0.695794\n",
      "218        317           1     1  0.593955\n",
      "219        318           0     1  0.612871\n",
      "220        320           0     1  0.718844\n",
      "221        321           1     1  0.638148\n",
      "222        322           1     1  0.655438\n",
      "223        324           0     1  0.638341\n",
      "224        325           0     1  0.990438\n",
      "225        327           0     1  0.715522\n",
      "226        328           1     1  0.824093\n",
      "227        329           1     1  0.785135\n",
      "228        331           1     1  0.816061\n",
      "229        332           1     1  0.662528\n",
      "230        334           1     1  0.685606\n",
      "231        336           0     1  0.599602\n",
      "232        338           1     1  0.762029\n",
      "233        339           0     1  0.631404\n",
      "234        340           1     1  0.663962\n",
      "235        341           0     1  0.758062\n",
      "236        343           0     1  0.664566\n",
      "237        344           1     1  0.600528\n",
      "238        346           0     1  0.626992\n",
      "239        347           0     1  0.726704\n",
      "240        348           0     1  0.667105\n",
      "241        349           0     1  0.833383\n",
      "242        350           1     1  0.963969\n",
      "243        351           0     1  0.760800\n",
      "244        352           1     1  0.746255\n",
      "245        353           0     1  0.654768\n",
      "246        356           0     1  0.793615\n",
      "247        359           1     1  0.722641\n",
      "248        360           1     1  0.651606\n",
      "249        364           1     1  0.614398\n",
      "250        366           1     1  0.958618\n",
      "251        367           1     1  0.941468\n",
      "252        369           1     1  0.882591\n",
      "253        370           1     1  0.785520\n",
      "254        371           1     1  0.646086\n",
      "255        373           0     1  0.757642\n",
      "256        376           0     1  0.982901\n",
      "257        377           0     1  0.558022\n",
      "258        378           0     1  0.643848\n",
      "259        379           0     1  0.685414\n",
      "260        380           0     1  0.770084\n",
      "261        382           0     1  0.563477\n",
      "262        383           1     1  0.890750\n",
      "263        386           1     1  0.753587\n",
      "264        387           0     1  0.647328\n",
      "265        388           0     1  0.892277\n",
      "266        389           0     1  0.740484\n",
      "267        390           0     1  0.515277\n",
      "268        391           0     1  0.938392\n",
      "269        392           0     1  0.686025\n",
      "270        395           0     1  0.715495\n",
      "271        397           0     1  0.734231\n",
      "272        399           0     1  0.667339\n",
      "273        400           1     1  0.542116\n",
      "274        401           0     1  0.551159\n",
      "275        402           0     1  0.746184\n",
      "276        403           1     1  0.933480\n",
      "277        404           1     1  0.537944\n",
      "278        405           0     1  0.662523\n",
      "279        406           1     1  0.550526\n",
      "280        407           0     1  0.746781\n",
      "281        408           1     1  0.622986\n",
      "282        409           1     1  0.583534\n",
      "283        410           0     1  0.632207\n",
      "284        412           0     1  0.763966\n",
      "285        413           1     1  0.644787\n",
      "286        414           0     1  0.645601\n",
      "287        416           1     1  0.801396\n",
      "288        417           0     1  0.616720\n",
      "289        418           0     1  0.830865\n",
      "290        419           0     1  0.790067\n",
      "291        421           0     1  0.610130\n",
      "292        423           0     1  0.689127\n",
      "293        425           1     1  0.638959\n",
      "294        426           1     1  0.697976\n",
      "295        429           1     1  0.739432\n",
      "296        430           0     1  0.654574\n",
      "297        431           1     1  0.696075\n",
      "298        432           0     1  0.594131\n",
      "299        433           0     1  0.560675\n",
      "300        436           1     1  0.648205\n",
      "301        440           1     1  0.512525\n",
      "302        441           0     1  0.689587\n",
      "303        442           1     1  0.877571\n",
      "304        443           1     1  0.696914\n",
      "305        444           0     1  0.799449\n",
      "306        445           0     1  0.799558\n",
      "307        446           0     1  0.922060\n",
      "308        449           1     1  0.718260\n",
      "309        451           1     1  0.834116\n",
      "310        452           0     1  0.716070\n",
      "311        454           0     1  0.599469\n",
      "312        455           0     1  0.633787\n",
      "313        456           1     1  0.766493\n",
      "314        457           1     1  0.940690\n",
      "315        459           0     1  0.803326\n",
      "316        464           0     1  0.692223\n",
      "317        466           1     1  0.573268\n",
      "318        468           1     1  0.611461\n",
      "319        469           0     1  0.568450\n",
      "320        470           1     1  0.596987\n",
      "321        472           1     1  0.676871\n",
      "322        477           0     1  0.680452\n",
      "323        478           1     1  0.652897\n",
      "324        479           1     1  0.725871\n",
      "325        480           1     1  0.561126\n",
      "326        481           0     1  0.620124\n",
      "327        483           1     1  0.580657\n",
      "328        485           1     1  0.585368\n",
      "329        488           1     1  0.634541\n",
      "330        491           1     1  0.596701\n",
      "331        493           1     1  0.564082\n",
      "332        494           1     1  0.552609\n",
      "333        495           0     1  0.646445\n",
      "334        496           0     1  0.713047\n",
      "335        498           0     1  0.650207\n",
      "336        499           1     1  0.818506\n",
      "337        500           1     1  0.516173\n",
      "338        501           1     1  0.557685\n",
      "339        502           1     1  0.729634\n",
      "340        504           1     1  0.567629\n",
      "341        505           1     1  0.589946\n",
      "342        506           1     1  0.561348\n",
      "343        507           0     1  0.639664\n",
      "344        510           0     1  0.622592\n",
      "345        511           1     1  0.778432\n",
      "346        512           0     1  0.624665\n",
      "347        513           1     1  0.535854\n",
      "348        514           0     1  0.646245\n",
      "349        516           1     1  0.757878\n",
      "350        517           1     1  0.596006\n",
      "351        518           0     1  0.612731\n",
      "352        519           0     1  0.726578\n",
      "353        520           1     1  0.539823\n",
      "354        523           1     1  0.573985\n",
      "355        524           1     1  0.616982\n",
      "356        525           1     1  0.560603\n",
      "357        526           1     1  0.851128\n",
      "358        528           1     1  0.586540\n",
      "359        529           1     1  0.695653\n",
      "360        530           0     1  0.559312\n",
      "361        532           1     1  0.570274\n",
      "362        533           0     1  0.742430\n",
      "363        537           1     1  0.863640\n",
      "364        538           0     1  0.546352\n",
      "365        539           1     1  0.550243\n",
      "366        540           0     1  0.561559\n",
      "367        542           1     1  0.620453\n",
      "368        543           1     1  0.597694\n",
      "369        544           1     1  0.533174\n",
      "370        545           0     1  0.523551\n",
      "371        547           0     1  0.583201\n",
      "372        548           1     1  0.603944\n",
      "373        549           1     1  0.690512\n",
      "374        550           1     1  0.554306\n",
      "375        551           1     1  0.680032\n",
      "376        552           1     1  0.591203\n",
      "377        554           1     1  0.591666\n",
      "378        555           0     1  0.676438\n",
      "379        556           1     1  0.714510\n",
      "380        557           1     1  0.570507\n",
      "381        558           1     1  0.536188\n",
      "382        559           1     1  0.636401\n",
      "383        561           1     1  0.655908\n",
      "384        563           0     1  0.658106\n",
      "385        564           1     1  0.657500\n",
      "386        565           0     1  0.654461\n",
      "387        567           0     1  0.717943\n",
      "388        568           0     1  0.941701\n",
      "389        569           0     1  0.772076\n",
      "390        570           1     0  0.442800\n",
      "391        571           0     1  0.724539\n",
      "392        572           0     1  0.881527\n",
      "393        574           0     1  0.729105\n",
      "394        575           0     0  0.442636\n",
      "395        576           1     1  0.876587\n",
      "396        577           1     1  0.711304\n",
      "397        578           0     1  0.650360\n",
      "398        579           1     1  0.651499\n",
      "399        581           0     1  0.787276\n",
      "400        582           1     1  0.800376\n",
      "401        583           1     1  0.685261\n",
      "402        584           1     0  0.446594\n",
      "403        586           1     1  0.727799\n",
      "404        587           0     1  0.859937\n",
      "405        588           0     1  0.770732\n",
      "406        589           0     0  0.374621\n",
      "407        590           1     1  0.662175\n",
      "408        591           0     1  0.733204\n",
      "409        593           1     1  0.771712\n",
      "410        594           1     1  0.839938\n",
      "411        596           0     1  0.690239\n",
      "412        597           1     1  0.687431\n",
      "413        598           1     1  0.737905\n",
      "414        599           1     1  0.719690\n",
      "415        601           0     1  0.697937\n",
      "416        602           1     1  0.651300\n",
      "417        604           1     1  0.522447\n",
      "418        605           0     1  0.676929\n",
      "419        606           1     1  0.729053\n",
      "420        607           1     1  0.676194\n",
      "421        608           1     1  0.600711\n",
      "422        610           1     1  0.703191\n",
      "423        611           1     1  0.521571\n",
      "424        612           1     1  0.610586\n",
      "425        613           1     1  0.586765\n",
      "426        615           1     1  0.623975\n",
      "427        616           0     1  0.773224\n",
      "428        618           1     1  0.754751\n",
      "429        619           0     1  0.728655\n",
      "430        620           0     1  0.606769\n",
      "431        621           1     1  0.623755\n",
      "432        622           1     1  0.590015\n",
      "433        623           0     1  0.566114\n",
      "434        624           0     1  0.553459\n",
      "435        625           1     1  0.654526\n",
      "436        626           1     1  0.592525\n",
      "437        628           1     1  0.604360\n",
      "438        630           0     1  0.578294\n",
      "439        631           1     1  0.652768\n",
      "440        636           0     1  0.750033\n",
      "441        638           1     1  0.521041\n",
      "442        639           1     1  0.581570\n",
      "443        640           1     1  0.530721\n",
      "444        641           0     1  0.771600\n",
      "445        642           0     1  0.597664\n",
      "446        645           0     1  0.610147\n",
      "447        646           1     1  0.682683\n",
      "448        649           0     1  0.595701\n",
      "449        650           1     1  0.787534\n",
      "450        651           0     1  0.633804\n",
      "451        652           1     1  0.861306\n",
      "452        654           0     1  0.553000\n",
      "453        655           1     1  0.642412\n",
      "454        656           1     1  0.982564\n",
      "455        657           0     1  0.624300\n",
      "456        658           1     1  0.569979\n",
      "457        659           1     1  0.537983\n",
      "458        661           1     1  0.591380\n",
      "459        663           0     1  0.665676\n",
      "460        667           0     1  0.618495\n",
      "461        668           0     1  0.590589\n",
      "462        674           1     1  0.572731\n",
      "463        675           1     1  0.569215\n",
      "464        676           1     1  0.606632\n",
      "465        677           1     1  0.733260\n",
      "466        679           1     1  0.979250\n",
      "467        680           1     1  0.674873\n",
      "468        682           0     1  0.593932\n",
      "469        683           0     1  0.755369\n",
      "470        684           0     1  0.651548\n",
      "471        685           0     1  0.599860\n",
      "472        686           0     1  0.561528\n",
      "473        687           0     1  0.546290\n",
      "474        688           0     1  0.572648\n",
      "475        690           1     1  0.533824\n",
      "476        691           1     1  0.781323\n",
      "477        692           1     1  0.774862\n",
      "478        693           1     1  0.566983\n",
      "479        694           1     1  0.734941\n",
      "480        697           1     1  0.691096\n",
      "481        698           1     1  0.578468\n",
      "482        703           0     1  0.535472\n",
      "483        704           1     1  0.589511\n",
      "484        705           1     1  0.530373\n",
      "485        706           0     1  0.640477\n",
      "486        707           1     1  0.720434\n",
      "487        708           1     1  0.531362\n",
      "488        709           0     1  0.595392\n",
      "489        714           1     1  0.638060\n",
      "490        715           1     1  0.596815\n",
      "491        716           1     1  0.599576\n",
      "492        718           1     1  0.694368\n",
      "493        723           0     1  0.542990\n",
      "494        724           0     1  0.538265\n",
      "495        725           1     1  0.653696\n",
      "496        727           0     1  0.787437\n",
      "497        728           0     1  0.532071\n",
      "498        729           0     1  0.662176\n",
      "499        730           0     1  0.573428\n",
      "500        731           1     1  0.591983\n",
      "501        732           1     1  0.605050\n",
      "502        733           0     1  0.687798\n",
      "503        734           0     1  0.764476\n",
      "504        735           0     1  0.664010\n",
      "505        736           1     1  0.633465\n",
      "506        737           1     1  0.659042\n",
      "507        739           1     1  0.790508\n",
      "508        740           1     1  0.560111\n",
      "509        742           0     1  0.568195\n",
      "510        744           0     1  0.614603\n",
      "511        746           1     1  0.577097\n",
      "512        747           0     1  0.669960\n",
      "513        750           1     1  0.569263\n",
      "514        751           0     1  0.805645\n",
      "515        753           0     1  0.892169\n",
      "516        756           0     1  0.799524\n",
      "517        757           1     1  0.558634\n",
      "518        758           1     1  0.749713\n",
      "519        759           0     1  0.862898\n",
      "520        760           1     1  0.666517\n",
      "521        764           0     1  0.849020\n",
      "522        765           1     1  0.570261\n",
      "523        767           0     1  0.702986\n",
      "524        768           1     1  0.622818\n",
      "525        772           1     1  0.584319\n",
      "526        773           1     1  0.667867\n",
      "527        774           0     1  0.686319\n",
      "528        775           1     1  0.696770\n",
      "529        777           1     1  0.574516\n",
      "530        778           0     1  0.591020\n",
      "531        780           0     1  0.650987\n",
      "532        781           1     1  0.744396\n",
      "533        782           1     1  0.622848\n",
      "534        784           1     1  0.643275\n",
      "535        787           1     1  0.669852\n",
      "536        788           0     1  0.628934\n",
      "537        789           1     1  0.616267\n",
      "538        791           1     1  0.744738\n",
      "539        792           0     1  0.709870\n",
      "540        793           1     1  0.806120\n",
      "541        794           1     1  0.541017\n",
      "542        795           1     1  0.638229\n",
      "543        796           0     1  0.701381\n",
      "544        797           0     1  0.710650\n",
      "545        799           0     1  0.796342\n",
      "546        800           0     1  0.582647\n",
      "547        801           1     1  0.670020\n",
      "548        802           0     1  0.562149\n",
      "549        803           0     1  0.687209\n",
      "550        804           0     1  0.513292\n",
      "551        805           0     1  0.655928\n",
      "552        806           0     1  0.542548\n",
      "553        807           1     1  0.809594\n",
      "554        808           1     1  0.724519\n",
      "555        809           0     1  0.683649\n",
      "556        810           0     1  0.650559\n",
      "557        811           1     1  0.547984\n",
      "558        814           0     1  0.616168\n",
      "559        816           1     1  0.695276\n",
      "560        818           0     1  0.697742\n",
      "561        819           1     1  0.733934\n",
      "562        820           0     1  0.789449\n",
      "563        823           1     1  0.645094\n",
      "564        824           0     1  0.655049\n",
      "565        828           1     1  0.777768\n",
      "566        830           0     1  0.730792\n",
      "567        834           0     1  0.546999\n",
      "568        836           0     1  0.722756\n",
      "569        837           0     1  0.715184\n",
      "570        838           1     1  0.642534\n",
      "571        839           0     1  0.697868\n",
      "572        840           1     1  0.675903\n",
      "573        998           1     1  0.941702\n",
      "574        999           1     1  0.913578\n",
      "575       1000           1     1  0.571238\n",
      "576       1001           1     1  0.717592\n",
      "577       1002           1     1  0.598313\n",
      "578       1003           1     1  0.796125\n",
      "579       1004           0     1  0.614541\n",
      "580       1005           1     1  0.622517\n",
      "581       1007           1     1  0.897659\n",
      "582       1008           1     1  0.755171\n",
      "583       1009           0     1  0.876737\n",
      "584       1010           0     1  0.562290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction AUC: 0.4509\n",
      "Prediction Accuracy: 0.5282\n",
      "Prediction Specificity: 0.0216\n",
      "Prediction Sensitivity: 0.9870\n",
      "Prediction Precision: 0.5270\n",
      "the score of the fold number 0 and the type T1wCE: 0.45088229090994303\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     0  0.450882  0.528205  0.021583  0.986971  0.526957\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 585/585 [01:10<00:00,  8.33it/s]\n",
      "Fold 1:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            0           1     1  0.621191\n",
      "1            2           1     1  0.622528\n",
      "2            3           0     1  0.616566\n",
      "3            5           1     1  0.623523\n",
      "4            6           1     1  0.622303\n",
      "5            8           1     1  0.598224\n",
      "6            9           0     1  0.617102\n",
      "7           11           1     1  0.616388\n",
      "8           12           1     1  0.607089\n",
      "9           14           1     1  0.604489\n",
      "10          17           0     1  0.600605\n",
      "11          18           0     1  0.612962\n",
      "12          19           0     1  0.614770\n",
      "13          20           1     1  0.611683\n",
      "14          21           0     1  0.616368\n",
      "15          22           0     1  0.611378\n",
      "16          24           0     1  0.610336\n",
      "17          25           1     1  0.611362\n",
      "18          26           1     1  0.611822\n",
      "19          28           1     1  0.618061\n",
      "20          30           0     1  0.611962\n",
      "21          31           1     1  0.615857\n",
      "22          32           0     1  0.610123\n",
      "23          33           1     1  0.610713\n",
      "24          35           1     1  0.614144\n",
      "25          36           0     1  0.615558\n",
      "26          43           1     1  0.607785\n",
      "27          44           0     1  0.614775\n",
      "28          45           0     1  0.614483\n",
      "29          46           1     1  0.615600\n",
      "30          48           1     1  0.598232\n",
      "31          49           0     1  0.611965\n",
      "32          52           1     1  0.608002\n",
      "33          53           0     1  0.601086\n",
      "34          54           1     1  0.617063\n",
      "35          56           1     1  0.620648\n",
      "36          58           1     1  0.615932\n",
      "37          59           1     1  0.610074\n",
      "38          60           1     1  0.600333\n",
      "39          61           0     1  0.602382\n",
      "40          62           1     1  0.614458\n",
      "41          63           1     1  0.608582\n",
      "42          64           0     1  0.611235\n",
      "43          66           1     1  0.606851\n",
      "44          68           1     1  0.615171\n",
      "45          70           1     1  0.605370\n",
      "46          71           1     1  0.604805\n",
      "47          72           0     1  0.599510\n",
      "48          74           1     1  0.601377\n",
      "49          77           1     1  0.613000\n",
      "50          78           1     1  0.618762\n",
      "51          81           0     1  0.615607\n",
      "52          84           0     1  0.617653\n",
      "53          85           1     1  0.605080\n",
      "54          87           1     1  0.614501\n",
      "55          88           0     1  0.614397\n",
      "56          89           1     1  0.622804\n",
      "57          90           0     1  0.609633\n",
      "58          94           1     1  0.607323\n",
      "59          95           0     1  0.603162\n",
      "60          96           1     1  0.613283\n",
      "61          97           0     1  0.612840\n",
      "62          98           1     1  0.619396\n",
      "63          99           0     1  0.616968\n",
      "64         100           1     1  0.623129\n",
      "65         102           0     1  0.616563\n",
      "66         104           0     1  0.629719\n",
      "67         105           1     1  0.618297\n",
      "68         106           1     1  0.612863\n",
      "69         107           1     1  0.622423\n",
      "70         108           0     1  0.606690\n",
      "71         109           1     1  0.628841\n",
      "72         110           0     1  0.603668\n",
      "73         111           0     1  0.603489\n",
      "74         112           0     1  0.622913\n",
      "75         113           0     1  0.614779\n",
      "76         116           0     1  0.614907\n",
      "77         117           1     1  0.606477\n",
      "78         120           1     1  0.615318\n",
      "79         121           0     1  0.607608\n",
      "80         122           0     1  0.632401\n",
      "81         123           0     1  0.613292\n",
      "82         124           0     1  0.627416\n",
      "83         128           1     1  0.624520\n",
      "84         130           0     1  0.602773\n",
      "85         132           0     1  0.610366\n",
      "86         133           0     1  0.601664\n",
      "87         134           1     1  0.625996\n",
      "88         136           1     1  0.626795\n",
      "89         137           0     1  0.625138\n",
      "90         138           1     1  0.601458\n",
      "91         139           1     1  0.614358\n",
      "92         140           1     1  0.626927\n",
      "93         142           0     1  0.604068\n",
      "94         143           1     1  0.625510\n",
      "95         144           1     1  0.603998\n",
      "96         146           1     1  0.604962\n",
      "97         147           0     1  0.603142\n",
      "98         148           0     1  0.612793\n",
      "99         149           0     1  0.615325\n",
      "100        150           0     1  0.630210\n",
      "101        151           0     1  0.615442\n",
      "102        154           0     1  0.588903\n",
      "103        155           1     1  0.584416\n",
      "104        156           1     1  0.604072\n",
      "105        157           0     1  0.586709\n",
      "106        158           0     1  0.592127\n",
      "107        159           1     1  0.578508\n",
      "108        160           1     1  0.585950\n",
      "109        162           0     1  0.585196\n",
      "110        165           0     1  0.578080\n",
      "111        166           1     1  0.611358\n",
      "112        167           0     1  0.584139\n",
      "113        169           0     1  0.616226\n",
      "114        170           0     1  0.589465\n",
      "115        171           1     1  0.589338\n",
      "116        172           0     1  0.586910\n",
      "117        176           0     1  0.593579\n",
      "118        177           1     1  0.580754\n",
      "119        178           1     1  0.589047\n",
      "120        183           0     1  0.591058\n",
      "121        184           0     1  0.580470\n",
      "122        185           1     1  0.589232\n",
      "123        186           1     1  0.568379\n",
      "124        187           1     1  0.600674\n",
      "125        188           1     1  0.580958\n",
      "126        191           0     1  0.580851\n",
      "127        192           0     1  0.579116\n",
      "128        193           0     1  0.621304\n",
      "129        194           0     1  0.591630\n",
      "130        195           0     1  0.588770\n",
      "131        196           1     1  0.588372\n",
      "132        197           1     1  0.582045\n",
      "133        199           1     1  0.582435\n",
      "134        201           0     1  0.582579\n",
      "135        203           1     1  0.587161\n",
      "136        204           1     1  0.600737\n",
      "137        206           0     1  0.614216\n",
      "138        209           0     1  0.577569\n",
      "139        210           1     1  0.589971\n",
      "140        211           0     1  0.580259\n",
      "141        212           1     1  0.576367\n",
      "142        214           0     1  0.611883\n",
      "143        216           0     1  0.596670\n",
      "144        217           0     1  0.577687\n",
      "145        218           0     1  0.573278\n",
      "146        219           0     1  0.589048\n",
      "147        220           1     1  0.585831\n",
      "148        221           0     1  0.583187\n",
      "149        222           1     1  0.584524\n",
      "150        227           0     1  0.589251\n",
      "151        228           0     1  0.609677\n",
      "152        230           1     1  0.577585\n",
      "153        231           0     1  0.587778\n",
      "154        233           1     1  0.582332\n",
      "155        234           1     1  0.576805\n",
      "156        235           1     1  0.594207\n",
      "157        236           0     1  0.583924\n",
      "158        237           0     1  0.582313\n",
      "159        238           0     1  0.582800\n",
      "160        239           0     1  0.584646\n",
      "161        240           1     1  0.588715\n",
      "162        241           0     1  0.582823\n",
      "163        242           0     1  0.585405\n",
      "164        243           0     1  0.616371\n",
      "165        245           1     1  0.578682\n",
      "166        246           1     1  0.587254\n",
      "167        247           0     1  0.593534\n",
      "168        249           0     1  0.579712\n",
      "169        250           1     1  0.601791\n",
      "170        251           0     1  0.577007\n",
      "171        253           1     1  0.581906\n",
      "172        254           1     1  0.582512\n",
      "173        258           0     1  0.614057\n",
      "174        259           0     1  0.586276\n",
      "175        260           1     1  0.586502\n",
      "176        261           0     1  0.582360\n",
      "177        262           0     1  0.615851\n",
      "178        263           1     1  0.573636\n",
      "179        266           0     1  0.598449\n",
      "180        267           0     1  0.582933\n",
      "181        269           0     1  0.595096\n",
      "182        270           1     1  0.621184\n",
      "183        271           1     1  0.588251\n",
      "184        273           1     1  0.584434\n",
      "185        274           0     1  0.590047\n",
      "186        275           0     1  0.609968\n",
      "187        280           0     1  0.586936\n",
      "188        281           1     1  0.605745\n",
      "189        282           1     1  0.581354\n",
      "190        283           0     1  0.581116\n",
      "191        284           1     1  0.581842\n",
      "192        285           1     1  0.582526\n",
      "193        286           0     1  0.581105\n",
      "194        288           0     1  0.584088\n",
      "195        289           0     1  0.587818\n",
      "196        290           0     1  0.577702\n",
      "197        291           1     1  0.607188\n",
      "198        293           1     1  0.582772\n",
      "199        294           1     1  0.609864\n",
      "200        296           1     1  0.586923\n",
      "201        297           0     1  0.584361\n",
      "202        298           0     1  0.581466\n",
      "203        299           1     1  0.577889\n",
      "204        300           0     1  0.587747\n",
      "205        301           0     1  0.577737\n",
      "206        303           1     1  0.587197\n",
      "207        304           1     1  0.584391\n",
      "208        305           1     1  0.612740\n",
      "209        306           1     1  0.586903\n",
      "210        308           0     1  0.582068\n",
      "211        309           0     1  0.582666\n",
      "212        310           0     1  0.587859\n",
      "213        311           1     1  0.579752\n",
      "214        312           0     1  0.600954\n",
      "215        313           1     1  0.581066\n",
      "216        314           0     1  0.578734\n",
      "217        316           0     1  0.584015\n",
      "218        317           1     1  0.625657\n",
      "219        318           0     1  0.574720\n",
      "220        320           0     1  0.582510\n",
      "221        321           1     1  0.584025\n",
      "222        322           1     1  0.583055\n",
      "223        324           0     1  0.579933\n",
      "224        325           0     1  0.606654\n",
      "225        327           0     1  0.581273\n",
      "226        328           1     1  0.578643\n",
      "227        329           1     1  0.585311\n",
      "228        331           1     1  0.575613\n",
      "229        332           1     1  0.596974\n",
      "230        334           1     1  0.582929\n",
      "231        336           0     1  0.586752\n",
      "232        338           1     1  0.621484\n",
      "233        339           0     1  0.589329\n",
      "234        340           1     1  0.580964\n",
      "235        341           0     1  0.586066\n",
      "236        343           0     1  0.586283\n",
      "237        344           1     1  0.590464\n",
      "238        346           0     1  0.589379\n",
      "239        347           0     1  0.579301\n",
      "240        348           0     1  0.590047\n",
      "241        349           0     1  0.604273\n",
      "242        350           1     1  0.584795\n",
      "243        351           0     1  0.598756\n",
      "244        352           1     1  0.592330\n",
      "245        353           0     1  0.582210\n",
      "246        356           0     1  0.586398\n",
      "247        359           1     1  0.612374\n",
      "248        360           1     1  0.588394\n",
      "249        364           1     1  0.591734\n",
      "250        366           1     1  0.586094\n",
      "251        367           1     1  0.612094\n",
      "252        369           1     1  0.601699\n",
      "253        370           1     1  0.586155\n",
      "254        371           1     1  0.585742\n",
      "255        373           0     1  0.582525\n",
      "256        376           0     1  0.572523\n",
      "257        377           0     1  0.618536\n",
      "258        378           0     1  0.583786\n",
      "259        379           0     1  0.581317\n",
      "260        380           0     1  0.585884\n",
      "261        382           0     1  0.583789\n",
      "262        383           1     1  0.601121\n",
      "263        386           1     1  0.579375\n",
      "264        387           0     1  0.590392\n",
      "265        388           0     1  0.607388\n",
      "266        389           0     1  0.612859\n",
      "267        390           0     1  0.587638\n",
      "268        391           0     1  0.575958\n",
      "269        392           0     1  0.598369\n",
      "270        395           0     1  0.579468\n",
      "271        397           0     1  0.591569\n",
      "272        399           0     1  0.585440\n",
      "273        400           1     1  0.584696\n",
      "274        401           0     1  0.588673\n",
      "275        402           0     1  0.579656\n",
      "276        403           1     1  0.594558\n",
      "277        404           1     1  0.583167\n",
      "278        405           0     1  0.626781\n",
      "279        406           1     1  0.584833\n",
      "280        407           0     1  0.585957\n",
      "281        408           1     1  0.585242\n",
      "282        409           1     1  0.578469\n",
      "283        410           0     1  0.591191\n",
      "284        412           0     1  0.582551\n",
      "285        413           1     1  0.593381\n",
      "286        414           0     1  0.582776\n",
      "287        416           1     1  0.579200\n",
      "288        417           0     1  0.587743\n",
      "289        418           0     1  0.587680\n",
      "290        419           0     1  0.578298\n",
      "291        421           0     1  0.584538\n",
      "292        423           0     1  0.580343\n",
      "293        425           1     1  0.575228\n",
      "294        426           1     1  0.590299\n",
      "295        429           1     1  0.610907\n",
      "296        430           0     1  0.582528\n",
      "297        431           1     1  0.583862\n",
      "298        432           0     1  0.596060\n",
      "299        433           0     1  0.585991\n",
      "300        436           1     1  0.617047\n",
      "301        440           1     1  0.587866\n",
      "302        441           0     1  0.614926\n",
      "303        442           1     1  0.601917\n",
      "304        443           1     1  0.612986\n",
      "305        444           0     1  0.597443\n",
      "306        445           0     1  0.596371\n",
      "307        446           0     1  0.599069\n",
      "308        449           1     1  0.604073\n",
      "309        451           1     1  0.608137\n",
      "310        452           0     1  0.590576\n",
      "311        454           0     1  0.605554\n",
      "312        455           0     1  0.626967\n",
      "313        456           1     1  0.604034\n",
      "314        457           1     1  0.600131\n",
      "315        459           0     1  0.635449\n",
      "316        464           0     1  0.602596\n",
      "317        466           1     1  0.612758\n",
      "318        468           1     1  0.614658\n",
      "319        469           0     1  0.615912\n",
      "320        470           1     1  0.603378\n",
      "321        472           1     1  0.599676\n",
      "322        477           0     1  0.609415\n",
      "323        478           1     1  0.604147\n",
      "324        479           1     1  0.602162\n",
      "325        480           1     1  0.602194\n",
      "326        481           0     1  0.620727\n",
      "327        483           1     1  0.602981\n",
      "328        485           1     1  0.608528\n",
      "329        488           1     1  0.601872\n",
      "330        491           1     1  0.602027\n",
      "331        493           1     1  0.609925\n",
      "332        494           1     1  0.610711\n",
      "333        495           0     1  0.603240\n",
      "334        496           0     1  0.602319\n",
      "335        498           0     1  0.614518\n",
      "336        499           1     1  0.598892\n",
      "337        500           1     1  0.602554\n",
      "338        501           1     1  0.609277\n",
      "339        502           1     1  0.610238\n",
      "340        504           1     1  0.603169\n",
      "341        505           1     1  0.606565\n",
      "342        506           1     1  0.611527\n",
      "343        507           0     1  0.620783\n",
      "344        510           0     1  0.603880\n",
      "345        511           1     1  0.610132\n",
      "346        512           0     1  0.613394\n",
      "347        513           1     1  0.603592\n",
      "348        514           0     1  0.609890\n",
      "349        516           1     1  0.603571\n",
      "350        517           1     1  0.608913\n",
      "351        518           0     1  0.606328\n",
      "352        519           0     1  0.601072\n",
      "353        520           1     1  0.612843\n",
      "354        523           1     1  0.615778\n",
      "355        524           1     1  0.607168\n",
      "356        525           1     1  0.612170\n",
      "357        526           1     1  0.592013\n",
      "358        528           1     1  0.597175\n",
      "359        529           1     1  0.614573\n",
      "360        530           0     1  0.604834\n",
      "361        532           1     1  0.619738\n",
      "362        533           0     1  0.611777\n",
      "363        537           1     1  0.597400\n",
      "364        538           0     1  0.613639\n",
      "365        539           1     1  0.607012\n",
      "366        540           0     1  0.604454\n",
      "367        542           1     1  0.607961\n",
      "368        543           1     1  0.609817\n",
      "369        544           1     1  0.604436\n",
      "370        545           0     1  0.608416\n",
      "371        547           0     1  0.608297\n",
      "372        548           1     1  0.618568\n",
      "373        549           1     1  0.609800\n",
      "374        550           1     1  0.607720\n",
      "375        551           1     1  0.604138\n",
      "376        552           1     1  0.613310\n",
      "377        554           1     1  0.622188\n",
      "378        555           0     1  0.616380\n",
      "379        556           1     1  0.599559\n",
      "380        557           1     1  0.610807\n",
      "381        558           1     1  0.606120\n",
      "382        559           1     1  0.611257\n",
      "383        561           1     1  0.610842\n",
      "384        563           0     1  0.608805\n",
      "385        564           1     1  0.617029\n",
      "386        565           0     1  0.611097\n",
      "387        567           0     1  0.607597\n",
      "388        568           0     1  0.599329\n",
      "389        569           0     1  0.603782\n",
      "390        570           1     1  0.630894\n",
      "391        571           0     1  0.626585\n",
      "392        572           0     1  0.608226\n",
      "393        574           0     1  0.607115\n",
      "394        575           0     1  0.619235\n",
      "395        576           1     1  0.620184\n",
      "396        577           1     1  0.604652\n",
      "397        578           0     1  0.620183\n",
      "398        579           1     1  0.607819\n",
      "399        581           0     1  0.609612\n",
      "400        582           1     1  0.586362\n",
      "401        583           1     1  0.628891\n",
      "402        584           1     1  0.634279\n",
      "403        586           1     1  0.609454\n",
      "404        587           0     1  0.608087\n",
      "405        588           0     1  0.607021\n",
      "406        589           0     1  0.633196\n",
      "407        590           1     1  0.598797\n",
      "408        591           0     1  0.610362\n",
      "409        593           1     1  0.606883\n",
      "410        594           1     1  0.598697\n",
      "411        596           0     1  0.605044\n",
      "412        597           1     1  0.604095\n",
      "413        598           1     1  0.601111\n",
      "414        599           1     1  0.607960\n",
      "415        601           0     1  0.603319\n",
      "416        602           1     1  0.611902\n",
      "417        604           1     1  0.605564\n",
      "418        605           0     1  0.605299\n",
      "419        606           1     1  0.594212\n",
      "420        607           1     1  0.608207\n",
      "421        608           1     1  0.602169\n",
      "422        610           1     1  0.615846\n",
      "423        611           1     1  0.611737\n",
      "424        612           1     1  0.607191\n",
      "425        613           1     1  0.611838\n",
      "426        615           1     1  0.606178\n",
      "427        616           0     1  0.602518\n",
      "428        618           1     1  0.602203\n",
      "429        619           0     1  0.604923\n",
      "430        620           0     1  0.603083\n",
      "431        621           1     1  0.605423\n",
      "432        622           1     1  0.610709\n",
      "433        623           0     1  0.604143\n",
      "434        624           0     1  0.612341\n",
      "435        625           1     1  0.592627\n",
      "436        626           1     1  0.606575\n",
      "437        628           1     1  0.606098\n",
      "438        630           0     1  0.607459\n",
      "439        631           1     1  0.602607\n",
      "440        636           0     1  0.602385\n",
      "441        638           1     1  0.607989\n",
      "442        639           1     1  0.603921\n",
      "443        640           1     1  0.608773\n",
      "444        641           0     1  0.604100\n",
      "445        642           0     1  0.606499\n",
      "446        645           0     1  0.601400\n",
      "447        646           1     1  0.599159\n",
      "448        649           0     1  0.603363\n",
      "449        650           1     1  0.607951\n",
      "450        651           0     1  0.604145\n",
      "451        652           1     1  0.600008\n",
      "452        654           0     1  0.610421\n",
      "453        655           1     1  0.595245\n",
      "454        656           1     1  0.602372\n",
      "455        657           0     1  0.601347\n",
      "456        658           1     1  0.600840\n",
      "457        659           1     1  0.607950\n",
      "458        661           1     1  0.607420\n",
      "459        663           0     1  0.603349\n",
      "460        667           0     1  0.602746\n",
      "461        668           0     1  0.609621\n",
      "462        674           1     1  0.614999\n",
      "463        675           1     1  0.613022\n",
      "464        676           1     1  0.604833\n",
      "465        677           1     1  0.599592\n",
      "466        679           1     1  0.608707\n",
      "467        680           1     1  0.599805\n",
      "468        682           0     1  0.606195\n",
      "469        683           0     1  0.609199\n",
      "470        684           0     1  0.607415\n",
      "471        685           0     1  0.601739\n",
      "472        686           0     1  0.608269\n",
      "473        687           0     1  0.609746\n",
      "474        688           0     1  0.598054\n",
      "475        690           1     1  0.608669\n",
      "476        691           1     1  0.607586\n",
      "477        692           1     1  0.606635\n",
      "478        693           1     1  0.613436\n",
      "479        694           1     1  0.604669\n",
      "480        697           1     1  0.606731\n",
      "481        698           1     1  0.600848\n",
      "482        703           0     1  0.603212\n",
      "483        704           1     1  0.613336\n",
      "484        705           1     1  0.599698\n",
      "485        706           0     1  0.597561\n",
      "486        707           1     1  0.597486\n",
      "487        708           1     1  0.607653\n",
      "488        709           0     1  0.607137\n",
      "489        714           1     1  0.619250\n",
      "490        715           1     1  0.607218\n",
      "491        716           1     1  0.606123\n",
      "492        718           1     1  0.597774\n",
      "493        723           0     1  0.608611\n",
      "494        724           0     1  0.608088\n",
      "495        725           1     1  0.607650\n",
      "496        727           0     1  0.602093\n",
      "497        728           0     1  0.607323\n",
      "498        729           0     1  0.605797\n",
      "499        730           0     1  0.615098\n",
      "500        731           1     1  0.614788\n",
      "501        732           1     1  0.612551\n",
      "502        733           0     1  0.603454\n",
      "503        734           0     1  0.597309\n",
      "504        735           0     1  0.601097\n",
      "505        736           1     1  0.607031\n",
      "506        737           1     1  0.613756\n",
      "507        739           1     1  0.600266\n",
      "508        740           1     1  0.609388\n",
      "509        742           0     1  0.607031\n",
      "510        744           0     1  0.597065\n",
      "511        746           1     1  0.609414\n",
      "512        747           0     1  0.600853\n",
      "513        750           1     1  0.604484\n",
      "514        751           0     1  0.596894\n",
      "515        753           0     1  0.596175\n",
      "516        756           0     1  0.613967\n",
      "517        757           1     1  0.606400\n",
      "518        758           1     1  0.599790\n",
      "519        759           0     1  0.600430\n",
      "520        760           1     1  0.598056\n",
      "521        764           0     1  0.601022\n",
      "522        765           1     1  0.611154\n",
      "523        767           0     1  0.610180\n",
      "524        768           1     1  0.604033\n",
      "525        772           1     1  0.605421\n",
      "526        773           1     1  0.607805\n",
      "527        774           0     1  0.602883\n",
      "528        775           1     1  0.602686\n",
      "529        777           1     1  0.612806\n",
      "530        778           0     1  0.613891\n",
      "531        780           0     1  0.619275\n",
      "532        781           1     1  0.584636\n",
      "533        782           1     1  0.617283\n",
      "534        784           1     1  0.618533\n",
      "535        787           1     1  0.615033\n",
      "536        788           0     1  0.608372\n",
      "537        789           1     1  0.622126\n",
      "538        791           1     1  0.614918\n",
      "539        792           0     1  0.611712\n",
      "540        793           1     1  0.611532\n",
      "541        794           1     1  0.618426\n",
      "542        795           1     1  0.615992\n",
      "543        796           0     1  0.599203\n",
      "544        797           0     1  0.621385\n",
      "545        799           0     1  0.612345\n",
      "546        800           0     1  0.615531\n",
      "547        801           1     1  0.613418\n",
      "548        802           0     1  0.623965\n",
      "549        803           0     1  0.616099\n",
      "550        804           0     1  0.623827\n",
      "551        805           0     1  0.615485\n",
      "552        806           0     1  0.623124\n",
      "553        807           1     1  0.608136\n",
      "554        808           1     1  0.605243\n",
      "555        809           0     1  0.608700\n",
      "556        810           0     1  0.622383\n",
      "557        811           1     1  0.618975\n",
      "558        814           0     1  0.623577\n",
      "559        816           1     1  0.611260\n",
      "560        818           0     1  0.612741\n",
      "561        819           1     1  0.605290\n",
      "562        820           0     1  0.589325\n",
      "563        823           1     1  0.605955\n",
      "564        824           0     1  0.617354\n",
      "565        828           1     1  0.601205\n",
      "566        830           0     1  0.618143\n",
      "567        834           0     1  0.612948\n",
      "568        836           0     1  0.609595\n",
      "569        837           0     1  0.588183\n",
      "570        838           1     1  0.589581\n",
      "571        839           0     1  0.588112\n",
      "572        840           1     1  0.581337\n",
      "573        998           1     1  0.588746\n",
      "574        999           1     1  0.620188\n",
      "575       1000           1     1  0.575894\n",
      "576       1001           1     1  0.580574\n",
      "577       1002           1     1  0.621380\n",
      "578       1003           1     1  0.617407\n",
      "579       1004           0     1  0.585074\n",
      "580       1005           1     1  0.623513\n",
      "581       1007           1     1  0.600291\n",
      "582       1008           1     1  0.586951\n",
      "583       1009           0     1  0.625055\n",
      "584       1010           0     1  0.588748\n",
      "Prediction AUC: 0.5391\n",
      "Prediction Accuracy: 0.5248\n",
      "Prediction Specificity: 0.0000\n",
      "Prediction Sensitivity: 1.0000\n",
      "Prediction Precision: 0.5248\n",
      "the score of the fold number 1 and the type T1wCE: 0.5390527968504675\n",
      "  model       AUC       acc  spec  sens      prec\n",
      "0     1  0.539053  0.524786   0.0   1.0  0.524786\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 585/585 [01:10<00:00,  8.27it/s]\n",
      "Fold 2:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            0           1     0  0.449878\n",
      "1            2           1     0  0.462402\n",
      "2            3           0     0  0.472426\n",
      "3            5           1     0  0.440054\n",
      "4            6           1     0  0.458609\n",
      "5            8           1     1  0.502449\n",
      "6            9           0     0  0.488696\n",
      "7           11           1     0  0.322525\n",
      "8           12           1     1  0.508357\n",
      "9           14           1     1  0.504154\n",
      "10          17           0     1  0.508559\n",
      "11          18           0     0  0.455747\n",
      "12          19           0     0  0.472511\n",
      "13          20           1     0  0.418814\n",
      "14          21           0     0  0.452352\n",
      "15          22           0     0  0.482003\n",
      "16          24           0     0  0.402627\n",
      "17          25           1     0  0.459047\n",
      "18          26           1     0  0.458597\n",
      "19          28           1     0  0.438901\n",
      "20          30           0     0  0.445334\n",
      "21          31           1     0  0.494269\n",
      "22          32           0     0  0.482804\n",
      "23          33           1     0  0.412319\n",
      "24          35           1     0  0.470745\n",
      "25          36           0     0  0.472313\n",
      "26          43           1     1  0.506925\n",
      "27          44           0     0  0.399843\n",
      "28          45           0     0  0.454137\n",
      "29          46           1     0  0.457598\n",
      "30          48           1     1  0.514563\n",
      "31          49           0     0  0.400298\n",
      "32          52           1     0  0.458779\n",
      "33          53           0     1  0.519239\n",
      "34          54           1     0  0.491667\n",
      "35          56           1     0  0.444041\n",
      "36          58           1     0  0.469763\n",
      "37          59           1     0  0.439394\n",
      "38          60           1     0  0.458424\n",
      "39          61           0     1  0.512228\n",
      "40          62           1     0  0.455123\n",
      "41          63           1     1  0.500824\n",
      "42          64           0     1  0.501654\n",
      "43          66           1     1  0.504561\n",
      "44          68           1     0  0.396239\n",
      "45          70           1     0  0.439751\n",
      "46          71           1     0  0.471738\n",
      "47          72           0     0  0.415627\n",
      "48          74           1     1  0.509388\n",
      "49          77           1     0  0.462786\n",
      "50          78           1     0  0.445380\n",
      "51          81           0     0  0.436989\n",
      "52          84           0     0  0.448661\n",
      "53          85           1     0  0.469150\n",
      "54          87           1     0  0.474837\n",
      "55          88           0     0  0.425706\n",
      "56          89           1     0  0.472188\n",
      "57          90           0     1  0.504084\n",
      "58          94           1     0  0.447959\n",
      "59          95           0     1  0.507355\n",
      "60          96           1     0  0.470150\n",
      "61          97           0     0  0.475584\n",
      "62          98           1     0  0.489997\n",
      "63          99           0     0  0.449355\n",
      "64         100           1     0  0.117633\n",
      "65         102           0     0  0.126353\n",
      "66         104           0     0  0.290772\n",
      "67         105           1     0  0.248550\n",
      "68         106           1     0  0.414925\n",
      "69         107           1     0  0.177645\n",
      "70         108           0     1  0.510938\n",
      "71         109           1     0  0.237931\n",
      "72         110           0     0  0.401115\n",
      "73         111           0     0  0.461990\n",
      "74         112           0     0  0.085156\n",
      "75         113           0     0  0.099405\n",
      "76         116           0     0  0.293506\n",
      "77         117           1     0  0.434364\n",
      "78         120           1     0  0.351421\n",
      "79         121           0     0  0.187962\n",
      "80         122           0     1  0.622415\n",
      "81         123           0     0  0.159125\n",
      "82         124           0     1  0.693327\n",
      "83         128           1     0  0.294172\n",
      "84         130           0     0  0.491436\n",
      "85         132           0     0  0.286240\n",
      "86         133           0     0  0.260120\n",
      "87         134           1     0  0.411156\n",
      "88         136           1     0  0.288706\n",
      "89         137           0     0  0.396423\n",
      "90         138           1     1  0.577583\n",
      "91         139           1     0  0.102491\n",
      "92         140           1     0  0.326397\n",
      "93         142           0     0  0.284656\n",
      "94         143           1     0  0.455691\n",
      "95         144           1     0  0.230528\n",
      "96         146           1     0  0.315697\n",
      "97         147           0     0  0.361895\n",
      "98         148           0     0  0.399290\n",
      "99         149           0     0  0.129850\n",
      "100        150           0     0  0.310538\n",
      "101        151           0     0  0.214246\n",
      "102        154           0     1  0.519764\n",
      "103        155           1     0  0.464872\n",
      "104        156           1     0  0.454780\n",
      "105        157           0     1  0.518896\n",
      "106        158           0     0  0.465758\n",
      "107        159           1     1  0.515191\n",
      "108        160           1     1  0.508798\n",
      "109        162           0     1  0.507715\n",
      "110        165           0     0  0.473351\n",
      "111        166           1     0  0.485149\n",
      "112        167           0     0  0.469482\n",
      "113        169           0     0  0.253752\n",
      "114        170           0     0  0.494787\n",
      "115        171           1     1  0.504274\n",
      "116        172           0     1  0.509403\n",
      "117        176           0     0  0.497186\n",
      "118        177           1     0  0.497388\n",
      "119        178           1     0  0.490499\n",
      "120        183           0     0  0.484992\n",
      "121        184           0     0  0.496521\n",
      "122        185           1     1  0.514245\n",
      "123        186           1     1  0.526192\n",
      "124        187           1     0  0.467834\n",
      "125        188           1     0  0.493027\n",
      "126        191           0     0  0.478274\n",
      "127        192           0     1  0.511521\n",
      "128        193           0     0  0.218053\n",
      "129        194           0     1  0.505909\n",
      "130        195           0     0  0.463234\n",
      "131        196           1     1  0.503505\n",
      "132        197           1     0  0.481276\n",
      "133        199           1     1  0.512027\n",
      "134        201           0     1  0.511365\n",
      "135        203           1     1  0.510641\n",
      "136        204           1     0  0.401265\n",
      "137        206           0     1  0.512265\n",
      "138        209           0     1  0.524315\n",
      "139        210           1     0  0.484302\n",
      "140        211           0     1  0.502339\n",
      "141        212           1     0  0.479188\n",
      "142        214           0     0  0.311454\n",
      "143        216           0     0  0.445073\n",
      "144        217           0     0  0.426572\n",
      "145        218           0     1  0.512735\n",
      "146        219           0     1  0.508822\n",
      "147        220           1     0  0.484883\n",
      "148        221           0     0  0.489095\n",
      "149        222           1     1  0.523177\n",
      "150        227           0     1  0.521465\n",
      "151        228           0     0  0.331220\n",
      "152        230           1     1  0.508810\n",
      "153        231           0     1  0.513259\n",
      "154        233           1     0  0.486320\n",
      "155        234           1     1  0.506918\n",
      "156        235           1     1  0.508023\n",
      "157        236           0     1  0.518630\n",
      "158        237           0     1  0.519835\n",
      "159        238           0     0  0.458018\n",
      "160        239           0     0  0.465926\n",
      "161        240           1     1  0.518381\n",
      "162        241           0     0  0.496474\n",
      "163        242           0     1  0.506015\n",
      "164        243           0     0  0.230185\n",
      "165        245           1     1  0.518356\n",
      "166        246           1     1  0.517101\n",
      "167        247           0     0  0.467408\n",
      "168        249           0     1  0.515481\n",
      "169        250           1     0  0.471019\n",
      "170        251           0     1  0.528117\n",
      "171        253           1     0  0.496877\n",
      "172        254           1     0  0.494534\n",
      "173        258           0     0  0.349271\n",
      "174        259           0     1  0.503917\n",
      "175        260           1     0  0.484507\n",
      "176        261           0     1  0.519320\n",
      "177        262           0     0  0.325842\n",
      "178        263           1     0  0.490346\n",
      "179        266           0     0  0.481636\n",
      "180        267           0     0  0.449226\n",
      "181        269           0     0  0.490903\n",
      "182        270           1     0  0.202004\n",
      "183        271           1     1  0.505683\n",
      "184        273           1     0  0.460159\n",
      "185        274           0     1  0.500640\n",
      "186        275           0     0  0.339902\n",
      "187        280           0     1  0.505018\n",
      "188        281           1     0  0.407419\n",
      "189        282           1     1  0.516177\n",
      "190        283           0     1  0.510052\n",
      "191        284           1     1  0.512271\n",
      "192        285           1     1  0.509202\n",
      "193        286           0     1  0.517964\n",
      "194        288           0     0  0.464648\n",
      "195        289           0     1  0.504651\n",
      "196        290           0     0  0.488281\n",
      "197        291           1     0  0.305022\n",
      "198        293           1     1  0.508813\n",
      "199        294           1     0  0.244614\n",
      "200        296           1     1  0.515716\n",
      "201        297           0     0  0.475567\n",
      "202        298           0     1  0.505270\n",
      "203        299           1     0  0.431625\n",
      "204        300           0     0  0.480893\n",
      "205        301           0     0  0.476511\n",
      "206        303           1     1  0.515156\n",
      "207        304           1     0  0.479842\n",
      "208        305           1     0  0.458931\n",
      "209        306           1     0  0.463879\n",
      "210        308           0     0  0.451603\n",
      "211        309           0     0  0.499947\n",
      "212        310           0     0  0.469169\n",
      "213        311           1     1  0.521107\n",
      "214        312           0     0  0.498260\n",
      "215        313           1     1  0.514846\n",
      "216        314           0     0  0.487810\n",
      "217        316           0     1  0.509178\n",
      "218        317           1     0  0.302196\n",
      "219        318           0     0  0.480978\n",
      "220        320           0     1  0.516328\n",
      "221        321           1     0  0.487508\n",
      "222        322           1     1  0.506623\n",
      "223        324           0     1  0.520871\n",
      "224        325           0     0  0.453750\n",
      "225        327           0     1  0.527446\n",
      "226        328           1     0  0.445631\n",
      "227        329           1     0  0.491187\n",
      "228        331           1     0  0.495183\n",
      "229        332           1     0  0.464388\n",
      "230        334           1     1  0.507184\n",
      "231        336           0     1  0.502058\n",
      "232        338           1     0  0.394317\n",
      "233        339           0     1  0.507429\n",
      "234        340           1     1  0.501000\n",
      "235        341           0     1  0.513536\n",
      "236        343           0     1  0.509568\n",
      "237        344           1     1  0.502214\n",
      "238        346           0     1  0.505525\n",
      "239        347           0     1  0.515084\n",
      "240        348           0     1  0.510171\n",
      "241        349           0     0  0.363244\n",
      "242        350           1     0  0.254114\n",
      "243        351           0     0  0.495570\n",
      "244        352           1     0  0.435805\n",
      "245        353           0     0  0.483058\n",
      "246        356           0     0  0.483065\n",
      "247        359           1     0  0.477283\n",
      "248        360           1     0  0.493383\n",
      "249        364           1     0  0.489536\n",
      "250        366           1     0  0.407507\n",
      "251        367           1     0  0.231204\n",
      "252        369           1     0  0.456605\n",
      "253        370           1     0  0.477578\n",
      "254        371           1     0  0.497549\n",
      "255        373           0     0  0.493325\n",
      "256        376           0     0  0.408974\n",
      "257        377           0     0  0.292202\n",
      "258        378           0     1  0.515382\n",
      "259        379           0     1  0.519701\n",
      "260        380           0     1  0.500694\n",
      "261        382           0     1  0.520378\n",
      "262        383           1     0  0.431795\n",
      "263        386           1     1  0.521306\n",
      "264        387           0     0  0.498924\n",
      "265        388           0     0  0.244136\n",
      "266        389           0     0  0.473225\n",
      "267        390           0     1  0.526037\n",
      "268        391           0     0  0.418971\n",
      "269        392           0     0  0.472305\n",
      "270        395           0     1  0.502142\n",
      "271        397           0     0  0.499680\n",
      "272        399           0     0  0.477805\n",
      "273        400           1     1  0.523921\n",
      "274        401           0     1  0.511329\n",
      "275        402           0     1  0.500270\n",
      "276        403           1     0  0.389607\n",
      "277        404           1     1  0.510382\n",
      "278        405           0     0  0.337467\n",
      "279        406           1     1  0.504985\n",
      "280        407           0     1  0.504465\n",
      "281        408           1     1  0.513042\n",
      "282        409           1     1  0.527739\n",
      "283        410           0     0  0.493404\n",
      "284        412           0     1  0.501982\n",
      "285        413           1     0  0.495899\n",
      "286        414           0     1  0.515228\n",
      "287        416           1     0  0.476383\n",
      "288        417           0     1  0.505059\n",
      "289        418           0     0  0.435331\n",
      "290        419           0     0  0.484913\n",
      "291        421           0     1  0.507191\n",
      "292        423           0     1  0.500905\n",
      "293        425           1     1  0.512075\n",
      "294        426           1     0  0.461681\n",
      "295        429           1     0  0.324649\n",
      "296        430           0     0  0.499961\n",
      "297        431           1     1  0.503695\n",
      "298        432           0     1  0.504076\n",
      "299        433           0     1  0.510736\n",
      "300        436           1     0  0.211978\n",
      "301        440           1     1  0.506600\n",
      "302        441           0     0  0.308450\n",
      "303        442           1     0  0.477458\n",
      "304        443           1     0  0.449238\n",
      "305        444           0     0  0.477168\n",
      "306        445           0     0  0.494299\n",
      "307        446           0     0  0.389210\n",
      "308        449           1     0  0.485264\n",
      "309        451           1     0  0.470978\n",
      "310        452           0     0  0.492942\n",
      "311        454           0     0  0.492966\n",
      "312        455           0     0  0.225616\n",
      "313        456           1     0  0.468359\n",
      "314        457           1     0  0.410223\n",
      "315        459           0     0  0.272730\n",
      "316        464           0     0  0.372490\n",
      "317        466           1     0  0.497294\n",
      "318        468           1     0  0.486067\n",
      "319        469           0     0  0.487395\n",
      "320        470           1     1  0.505076\n",
      "321        472           1     0  0.497144\n",
      "322        477           0     0  0.491774\n",
      "323        478           1     0  0.494041\n",
      "324        479           1     0  0.486567\n",
      "325        480           1     1  0.506320\n",
      "326        481           0     0  0.433459\n",
      "327        483           1     1  0.512672\n",
      "328        485           1     0  0.493289\n",
      "329        488           1     0  0.493118\n",
      "330        491           1     1  0.514149\n",
      "331        493           1     0  0.493318\n",
      "332        494           1     0  0.496190\n",
      "333        495           0     0  0.484102\n",
      "334        496           0     0  0.498403\n",
      "335        498           0     0  0.456563\n",
      "336        499           1     0  0.473664\n",
      "337        500           1     1  0.510956\n",
      "338        501           1     1  0.500846\n",
      "339        502           1     0  0.457529\n",
      "340        504           1     0  0.499242\n",
      "341        505           1     0  0.492247\n",
      "342        506           1     0  0.457360\n",
      "343        507           0     0  0.449684\n",
      "344        510           0     0  0.494716\n",
      "345        511           1     0  0.441510\n",
      "346        512           0     1  0.504914\n",
      "347        513           1     1  0.510349\n",
      "348        514           0     0  0.483139\n",
      "349        516           1     0  0.482701\n",
      "350        517           1     0  0.464318\n",
      "351        518           0     1  0.501436\n",
      "352        519           0     0  0.480307\n",
      "353        520           1     0  0.497076\n",
      "354        523           1     0  0.448298\n",
      "355        524           1     0  0.490113\n",
      "356        525           1     0  0.442869\n",
      "357        526           1     0  0.491719\n",
      "358        528           1     0  0.488944\n",
      "359        529           1     0  0.432187\n",
      "360        530           0     0  0.459753\n",
      "361        532           1     0  0.473577\n",
      "362        533           0     0  0.452438\n",
      "363        537           1     0  0.491405\n",
      "364        538           0     1  0.500156\n",
      "365        539           1     1  0.502852\n",
      "366        540           0     1  0.508729\n",
      "367        542           1     0  0.493844\n",
      "368        543           1     0  0.429933\n",
      "369        544           1     1  0.505836\n",
      "370        545           0     1  0.501951\n",
      "371        547           0     0  0.449464\n",
      "372        548           1     0  0.458229\n",
      "373        549           1     0  0.425409\n",
      "374        550           1     1  0.508290\n",
      "375        551           1     0  0.498602\n",
      "376        552           1     1  0.500670\n",
      "377        554           1     0  0.483934\n",
      "378        555           0     0  0.469172\n",
      "379        556           1     0  0.475342\n",
      "380        557           1     1  0.501742\n",
      "381        558           1     1  0.507959\n",
      "382        559           1     0  0.418737\n",
      "383        561           1     0  0.424466\n",
      "384        563           0     0  0.400364\n",
      "385        564           1     0  0.432346\n",
      "386        565           0     0  0.425273\n",
      "387        567           0     0  0.461396\n",
      "388        568           0     0  0.330025\n",
      "389        569           0     0  0.467439\n",
      "390        570           1     0  0.301803\n",
      "391        571           0     0  0.071190\n",
      "392        572           0     0  0.393871\n",
      "393        574           0     0  0.463334\n",
      "394        575           0     0  0.156527\n",
      "395        576           1     0  0.244225\n",
      "396        577           1     0  0.448893\n",
      "397        578           0     0  0.326275\n",
      "398        579           1     0  0.465259\n",
      "399        581           0     0  0.376765\n",
      "400        582           1     1  0.501803\n",
      "401        583           1     0  0.318548\n",
      "402        584           1     0  0.189868\n",
      "403        586           1     0  0.427077\n",
      "404        587           0     0  0.411147\n",
      "405        588           0     0  0.456639\n",
      "406        589           0     0  0.169059\n",
      "407        590           1     0  0.436155\n",
      "408        591           0     0  0.435764\n",
      "409        593           1     0  0.460923\n",
      "410        594           1     0  0.359541\n",
      "411        596           0     0  0.438936\n",
      "412        597           1     0  0.467238\n",
      "413        598           1     0  0.441738\n",
      "414        599           1     0  0.459516\n",
      "415        601           0     0  0.495535\n",
      "416        602           1     0  0.487012\n",
      "417        604           1     1  0.503447\n",
      "418        605           0     0  0.467407\n",
      "419        606           1     0  0.469089\n",
      "420        607           1     0  0.482968\n",
      "421        608           1     1  0.500594\n",
      "422        610           1     0  0.472493\n",
      "423        611           1     0  0.498308\n",
      "424        612           1     1  0.506744\n",
      "425        613           1     0  0.477080\n",
      "426        615           1     1  0.503649\n",
      "427        616           0     0  0.478047\n",
      "428        618           1     0  0.490754\n",
      "429        619           0     0  0.491314\n",
      "430        620           0     1  0.508477\n",
      "431        621           1     0  0.493230\n",
      "432        622           1     0  0.489643\n",
      "433        623           0     0  0.494872\n",
      "434        624           0     1  0.501836\n",
      "435        625           1     0  0.499657\n",
      "436        626           1     0  0.492307\n",
      "437        628           1     0  0.490429\n",
      "438        630           0     0  0.497699\n",
      "439        631           1     0  0.496417\n",
      "440        636           0     0  0.499352\n",
      "441        638           1     1  0.502401\n",
      "442        639           1     0  0.496201\n",
      "443        640           1     1  0.505160\n",
      "444        641           0     0  0.481909\n",
      "445        642           0     0  0.470006\n",
      "446        645           0     0  0.498014\n",
      "447        646           1     0  0.493318\n",
      "448        649           0     0  0.486139\n",
      "449        650           1     0  0.475375\n",
      "450        651           0     1  0.501047\n",
      "451        652           1     0  0.442898\n",
      "452        654           0     1  0.503429\n",
      "453        655           1     0  0.495772\n",
      "454        656           1     1  0.507913\n",
      "455        657           0     0  0.494700\n",
      "456        658           1     0  0.485150\n",
      "457        659           1     1  0.509465\n",
      "458        661           1     1  0.501747\n",
      "459        663           0     0  0.496611\n",
      "460        667           0     0  0.494925\n",
      "461        668           0     1  0.501716\n",
      "462        674           1     0  0.493610\n",
      "463        675           1     0  0.495070\n",
      "464        676           1     0  0.488107\n",
      "465        677           1     0  0.488301\n",
      "466        679           1     0  0.408238\n",
      "467        680           1     0  0.491718\n",
      "468        682           0     0  0.447978\n",
      "469        683           0     0  0.476621\n",
      "470        684           0     0  0.497234\n",
      "471        685           0     0  0.490560\n",
      "472        686           0     1  0.505694\n",
      "473        687           0     1  0.501407\n",
      "474        688           0     0  0.495270\n",
      "475        690           1     1  0.501900\n",
      "476        691           1     0  0.496757\n",
      "477        692           1     0  0.482180\n",
      "478        693           1     0  0.496179\n",
      "479        694           1     0  0.490085\n",
      "480        697           1     0  0.486717\n",
      "481        698           1     1  0.508466\n",
      "482        703           0     1  0.509968\n",
      "483        704           1     1  0.504202\n",
      "484        705           1     1  0.512235\n",
      "485        706           0     1  0.505194\n",
      "486        707           1     0  0.483159\n",
      "487        708           1     1  0.504379\n",
      "488        709           0     1  0.502634\n",
      "489        714           1     0  0.468560\n",
      "490        715           1     0  0.496402\n",
      "491        716           1     0  0.481640\n",
      "492        718           1     1  0.504494\n",
      "493        723           0     1  0.505234\n",
      "494        724           0     1  0.507128\n",
      "495        725           1     0  0.480636\n",
      "496        727           0     0  0.487092\n",
      "497        728           0     1  0.506677\n",
      "498        729           0     0  0.483290\n",
      "499        730           0     0  0.495631\n",
      "500        731           1     0  0.473252\n",
      "501        732           1     0  0.492375\n",
      "502        733           0     0  0.425988\n",
      "503        734           0     0  0.475013\n",
      "504        735           0     1  0.501246\n",
      "505        736           1     0  0.482080\n",
      "506        737           1     0  0.486316\n",
      "507        739           1     0  0.488718\n",
      "508        740           1     1  0.501744\n",
      "509        742           0     0  0.494864\n",
      "510        744           0     0  0.489789\n",
      "511        746           1     0  0.496425\n",
      "512        747           0     1  0.507919\n",
      "513        750           1     1  0.500570\n",
      "514        751           0     0  0.495050\n",
      "515        753           0     0  0.404625\n",
      "516        756           0     0  0.460590\n",
      "517        757           1     1  0.501519\n",
      "518        758           1     0  0.480120\n",
      "519        759           0     0  0.470567\n",
      "520        760           1     0  0.497398\n",
      "521        764           0     0  0.460193\n",
      "522        765           1     1  0.503317\n",
      "523        767           0     0  0.493886\n",
      "524        768           1     0  0.491084\n",
      "525        772           1     0  0.493911\n",
      "526        773           1     0  0.477757\n",
      "527        774           0     0  0.467354\n",
      "528        775           1     0  0.496529\n",
      "529        777           1     0  0.479200\n",
      "530        778           0     0  0.483745\n",
      "531        780           0     0  0.213362\n",
      "532        781           1     0  0.350019\n",
      "533        782           1     0  0.280041\n",
      "534        784           1     0  0.317176\n",
      "535        787           1     0  0.304095\n",
      "536        788           0     0  0.249562\n",
      "537        789           1     0  0.301104\n",
      "538        791           1     0  0.331352\n",
      "539        792           0     0  0.349476\n",
      "540        793           1     0  0.354274\n",
      "541        794           1     0  0.257893\n",
      "542        795           1     0  0.263885\n",
      "543        796           0     0  0.396479\n",
      "544        797           0     0  0.285281\n",
      "545        799           0     0  0.343717\n",
      "546        800           0     0  0.241949\n",
      "547        801           1     0  0.280471\n",
      "548        802           0     0  0.299182\n",
      "549        803           0     0  0.290634\n",
      "550        804           0     0  0.269828\n",
      "551        805           0     0  0.262606\n",
      "552        806           0     0  0.272492\n",
      "553        807           1     0  0.385446\n",
      "554        808           1     0  0.227321\n",
      "555        809           0     0  0.302377\n",
      "556        810           0     0  0.300999\n",
      "557        811           1     0  0.192162\n",
      "558        814           0     0  0.288869\n",
      "559        816           1     0  0.333415\n",
      "560        818           0     0  0.310538\n",
      "561        819           1     0  0.308920\n",
      "562        820           0     0  0.359110\n",
      "563        823           1     0  0.256357\n",
      "564        824           0     0  0.272595\n",
      "565        828           1     0  0.385043\n",
      "566        830           0     0  0.362309\n",
      "567        834           0     0  0.430768\n",
      "568        836           0     0  0.342259\n",
      "569        837           0     1  0.509510\n",
      "570        838           1     0  0.492760\n",
      "571        839           0     0  0.496436\n",
      "572        840           1     1  0.507211\n",
      "573        998           1     0  0.118739\n",
      "574        999           1     0  0.217750\n",
      "575       1000           1     1  0.522460\n",
      "576       1001           1     0  0.486761\n",
      "577       1002           1     0  0.213078\n",
      "578       1003           1     0  0.212604\n",
      "579       1004           0     1  0.509426\n",
      "580       1005           1     0  0.194250\n",
      "581       1007           1     0  0.416682\n",
      "582       1008           1     0  0.496303\n",
      "583       1009           0     0  0.308300\n",
      "584       1010           0     1  0.519861\n",
      "Prediction AUC: 0.4990\n",
      "Prediction Accuracy: 0.4667\n",
      "Prediction Specificity: 0.6906\n",
      "Prediction Sensitivity: 0.2638\n",
      "Prediction Precision: 0.4850\n",
      "the score of the fold number 2 and the type T1wCE: 0.49899233707496543\n",
      "  model       AUC       acc      spec      sens     prec\n",
      "0     2  0.498992  0.466667  0.690647  0.263844  0.48503\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 585/585 [01:05<00:00,  8.97it/s]\n",
      "Fold 3:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            0           1     1  0.636878\n",
      "1            2           1     1  0.668743\n",
      "2            3           0     1  0.646783\n",
      "3            5           1     1  0.653103\n",
      "4            6           1     1  0.651306\n",
      "5            8           1     0  0.439447\n",
      "6            9           0     1  0.680222\n",
      "7           11           1     1  0.671669\n",
      "8           12           1     1  0.514821\n",
      "9           14           1     1  0.576396\n",
      "10          17           0     1  0.534890\n",
      "11          18           0     1  0.585877\n",
      "12          19           0     1  0.575325\n",
      "13          20           1     1  0.584514\n",
      "14          21           0     1  0.554388\n",
      "15          22           0     1  0.583501\n",
      "16          24           0     1  0.585522\n",
      "17          25           1     1  0.597495\n",
      "18          26           1     1  0.620427\n",
      "19          28           1     1  0.595646\n",
      "20          30           0     1  0.637334\n",
      "21          31           1     1  0.555035\n",
      "22          32           0     1  0.548819\n",
      "23          33           1     1  0.592484\n",
      "24          35           1     1  0.597530\n",
      "25          36           0     1  0.633299\n",
      "26          43           1     1  0.532449\n",
      "27          44           0     1  0.604122\n",
      "28          45           0     1  0.649403\n",
      "29          46           1     1  0.615766\n",
      "30          48           1     1  0.520310\n",
      "31          49           0     1  0.667833\n",
      "32          52           1     1  0.578468\n",
      "33          53           0     0  0.460049\n",
      "34          54           1     1  0.620797\n",
      "35          56           1     1  0.659569\n",
      "36          58           1     1  0.597183\n",
      "37          59           1     1  0.618416\n",
      "38          60           1     1  0.529349\n",
      "39          61           0     1  0.524138\n",
      "40          62           1     1  0.572940\n",
      "41          63           1     1  0.510417\n",
      "42          64           0     1  0.517296\n",
      "43          66           1     1  0.566839\n",
      "44          68           1     1  0.669613\n",
      "45          70           1     1  0.592435\n",
      "46          71           1     1  0.538236\n",
      "47          72           0     1  0.545800\n",
      "48          74           1     0  0.494741\n",
      "49          77           1     1  0.617493\n",
      "50          78           1     1  0.644611\n",
      "51          81           0     1  0.626901\n",
      "52          84           0     1  0.609997\n",
      "53          85           1     1  0.545305\n",
      "54          87           1     1  0.550911\n",
      "55          88           0     1  0.622087\n",
      "56          89           1     1  0.627570\n",
      "57          90           0     1  0.546466\n",
      "58          94           1     1  0.573834\n",
      "59          95           0     0  0.482293\n",
      "60          96           1     1  0.623766\n",
      "61          97           0     1  0.547549\n",
      "62          98           1     1  0.591587\n",
      "63          99           0     1  0.623134\n",
      "64         100           1     1  0.816267\n",
      "65         102           0     1  0.718250\n",
      "66         104           0     1  0.722461\n",
      "67         105           1     1  0.782041\n",
      "68         106           1     1  0.681497\n",
      "69         107           1     1  0.784810\n",
      "70         108           0     1  0.644126\n",
      "71         109           1     1  0.751621\n",
      "72         110           0     1  0.585299\n",
      "73         111           0     1  0.656230\n",
      "74         112           0     1  0.694772\n",
      "75         113           0     1  0.787532\n",
      "76         116           0     1  0.776444\n",
      "77         117           1     1  0.668521\n",
      "78         120           1     1  0.587652\n",
      "79         121           0     1  0.500677\n",
      "80         122           0     1  0.681093\n",
      "81         123           0     1  0.759459\n",
      "82         124           0     1  0.622609\n",
      "83         128           1     1  0.662411\n",
      "84         130           0     1  0.631140\n",
      "85         132           0     1  0.637475\n",
      "86         133           0     1  0.528768\n",
      "87         134           1     1  0.662068\n",
      "88         136           1     1  0.707400\n",
      "89         137           0     1  0.732856\n",
      "90         138           1     1  0.622208\n",
      "91         139           1     1  0.831591\n",
      "92         140           1     1  0.778446\n",
      "93         142           0     1  0.560105\n",
      "94         143           1     1  0.721963\n",
      "95         144           1     1  0.549926\n",
      "96         146           1     1  0.572069\n",
      "97         147           0     1  0.570208\n",
      "98         148           0     1  0.770258\n",
      "99         149           0     1  0.833337\n",
      "100        150           0     1  0.712840\n",
      "101        151           0     1  0.770567\n",
      "102        154           0     0  0.488149\n",
      "103        155           1     1  0.541340\n",
      "104        156           1     1  0.636836\n",
      "105        157           0     0  0.445910\n",
      "106        158           0     1  0.559280\n",
      "107        159           1     0  0.473486\n",
      "108        160           1     1  0.527239\n",
      "109        162           0     1  0.507574\n",
      "110        165           0     0  0.466243\n",
      "111        166           1     1  0.650829\n",
      "112        167           0     1  0.502644\n",
      "113        169           0     1  0.636518\n",
      "114        170           0     1  0.557781\n",
      "115        171           1     1  0.563914\n",
      "116        172           0     1  0.509323\n",
      "117        176           0     1  0.510217\n",
      "118        177           1     0  0.434054\n",
      "119        178           1     1  0.535672\n",
      "120        183           0     1  0.569476\n",
      "121        184           0     1  0.512010\n",
      "122        185           1     0  0.484769\n",
      "123        186           1     0  0.386996\n",
      "124        187           1     1  0.640107\n",
      "125        188           1     0  0.481916\n",
      "126        191           0     0  0.481199\n",
      "127        192           0     1  0.508337\n",
      "128        193           0     1  0.777361\n",
      "129        194           0     1  0.544812\n",
      "130        195           0     1  0.501107\n",
      "131        196           1     1  0.549431\n",
      "132        197           1     0  0.488837\n",
      "133        199           1     0  0.468849\n",
      "134        201           0     0  0.490800\n",
      "135        203           1     1  0.521318\n",
      "136        204           1     1  0.646924\n",
      "137        206           0     1  0.627566\n",
      "138        209           0     0  0.468155\n",
      "139        210           1     1  0.535414\n",
      "140        211           0     0  0.484046\n",
      "141        212           1     0  0.476735\n",
      "142        214           0     1  0.581881\n",
      "143        216           0     1  0.610935\n",
      "144        217           0     1  0.536145\n",
      "145        218           0     0  0.453594\n",
      "146        219           0     1  0.519510\n",
      "147        220           1     1  0.537446\n",
      "148        221           0     1  0.504589\n",
      "149        222           1     1  0.504511\n",
      "150        227           0     1  0.500667\n",
      "151        228           0     1  0.615888\n",
      "152        230           1     1  0.510714\n",
      "153        231           0     1  0.511610\n",
      "154        233           1     1  0.520090\n",
      "155        234           1     0  0.496332\n",
      "156        235           1     1  0.528872\n",
      "157        236           0     1  0.501723\n",
      "158        237           0     1  0.501465\n",
      "159        238           0     1  0.512572\n",
      "160        239           0     1  0.506634\n",
      "161        240           1     0  0.494561\n",
      "162        241           0     1  0.520978\n",
      "163        242           0     1  0.549201\n",
      "164        243           0     1  0.768602\n",
      "165        245           1     0  0.492765\n",
      "166        246           1     1  0.510676\n",
      "167        247           0     1  0.561374\n",
      "168        249           0     0  0.483888\n",
      "169        250           1     1  0.553644\n",
      "170        251           0     0  0.470446\n",
      "171        253           1     0  0.488414\n",
      "172        254           1     0  0.493450\n",
      "173        258           0     1  0.767558\n",
      "174        259           0     1  0.531145\n",
      "175        260           1     1  0.552827\n",
      "176        261           0     1  0.500724\n",
      "177        262           0     1  0.704259\n",
      "178        263           1     0  0.481795\n",
      "179        266           0     1  0.613781\n",
      "180        267           0     0  0.467587\n",
      "181        269           0     1  0.578726\n",
      "182        270           1     1  0.792870\n",
      "183        271           1     1  0.509825\n",
      "184        273           1     1  0.533972\n",
      "185        274           0     0  0.498509\n",
      "186        275           0     1  0.798861\n",
      "187        280           0     1  0.534035\n",
      "188        281           1     1  0.658729\n",
      "189        282           1     0  0.479640\n",
      "190        283           0     0  0.495415\n",
      "191        284           1     1  0.522655\n",
      "192        285           1     1  0.510826\n",
      "193        286           0     0  0.455084\n",
      "194        288           0     1  0.571258\n",
      "195        289           0     1  0.533796\n",
      "196        290           0     0  0.464781\n",
      "197        291           1     1  0.531098\n",
      "198        293           1     0  0.472108\n",
      "199        294           1     1  0.667304\n",
      "200        296           1     1  0.508193\n",
      "201        297           0     1  0.545302\n",
      "202        298           0     1  0.505831\n",
      "203        299           1     1  0.563452\n",
      "204        300           0     1  0.513929\n",
      "205        301           0     0  0.478669\n",
      "206        303           1     1  0.540453\n",
      "207        304           1     1  0.507116\n",
      "208        305           1     1  0.685984\n",
      "209        306           1     1  0.547659\n",
      "210        308           0     0  0.498056\n",
      "211        309           0     1  0.504339\n",
      "212        310           0     1  0.554590\n",
      "213        311           1     0  0.480932\n",
      "214        312           0     1  0.627186\n",
      "215        313           1     0  0.481117\n",
      "216        314           0     1  0.501595\n",
      "217        316           0     1  0.521221\n",
      "218        317           1     1  0.642867\n",
      "219        318           0     0  0.438715\n",
      "220        320           0     1  0.504081\n",
      "221        321           1     0  0.487121\n",
      "222        322           1     0  0.485210\n",
      "223        324           0     0  0.489477\n",
      "224        325           0     1  0.668773\n",
      "225        327           0     1  0.527518\n",
      "226        328           1     1  0.501053\n",
      "227        329           1     1  0.554261\n",
      "228        331           1     0  0.495162\n",
      "229        332           1     1  0.557725\n",
      "230        334           1     1  0.509143\n",
      "231        336           0     1  0.500801\n",
      "232        338           1     1  0.740802\n",
      "233        339           0     1  0.525529\n",
      "234        340           1     0  0.467789\n",
      "235        341           0     1  0.544177\n",
      "236        343           0     1  0.516275\n",
      "237        344           1     1  0.534960\n",
      "238        346           0     1  0.500282\n",
      "239        347           0     0  0.499076\n",
      "240        348           0     1  0.558104\n",
      "241        349           0     1  0.738597\n",
      "242        350           1     1  0.558090\n",
      "243        351           0     1  0.633001\n",
      "244        352           1     1  0.549522\n",
      "245        353           0     0  0.493882\n",
      "246        356           0     1  0.537322\n",
      "247        359           1     1  0.669530\n",
      "248        360           1     1  0.514608\n",
      "249        364           1     1  0.537657\n",
      "250        366           1     1  0.557248\n",
      "251        367           1     1  0.564087\n",
      "252        369           1     1  0.637477\n",
      "253        370           1     1  0.521390\n",
      "254        371           1     0  0.497325\n",
      "255        373           0     1  0.515799\n",
      "256        376           0     1  0.546204\n",
      "257        377           0     1  0.793961\n",
      "258        378           0     0  0.489771\n",
      "259        379           0     1  0.509447\n",
      "260        380           0     1  0.536369\n",
      "261        382           0     0  0.489101\n",
      "262        383           1     1  0.646516\n",
      "263        386           1     0  0.489356\n",
      "264        387           0     1  0.513052\n",
      "265        388           0     0  0.456970\n",
      "266        389           0     1  0.679169\n",
      "267        390           0     0  0.449954\n",
      "268        391           0     1  0.504817\n",
      "269        392           0     1  0.573958\n",
      "270        395           0     0  0.484707\n",
      "271        397           0     1  0.562373\n",
      "272        399           0     0  0.495368\n",
      "273        400           1     0  0.478211\n",
      "274        401           0     0  0.415095\n",
      "275        402           0     0  0.468875\n",
      "276        403           1     1  0.619538\n",
      "277        404           1     0  0.471215\n",
      "278        405           0     1  0.765481\n",
      "279        406           1     0  0.484358\n",
      "280        407           0     1  0.532171\n",
      "281        408           1     0  0.446576\n",
      "282        409           1     0  0.469831\n",
      "283        410           0     1  0.516147\n",
      "284        412           0     1  0.508567\n",
      "285        413           1     1  0.549267\n",
      "286        414           0     1  0.523247\n",
      "287        416           1     1  0.507132\n",
      "288        417           0     1  0.505571\n",
      "289        418           0     1  0.561229\n",
      "290        419           0     1  0.503127\n",
      "291        421           0     0  0.446493\n",
      "292        423           0     0  0.496050\n",
      "293        425           1     0  0.476126\n",
      "294        426           1     1  0.543564\n",
      "295        429           1     1  0.784535\n",
      "296        430           0     0  0.474746\n",
      "297        431           1     1  0.525434\n",
      "298        432           0     1  0.566180\n",
      "299        433           0     0  0.484119\n",
      "300        436           1     1  0.799446\n",
      "301        440           1     1  0.523887\n",
      "302        441           0     1  0.729096\n",
      "303        442           1     1  0.638297\n",
      "304        443           1     1  0.645225\n",
      "305        444           0     1  0.611853\n",
      "306        445           0     1  0.581817\n",
      "307        446           0     1  0.629472\n",
      "308        449           1     1  0.629526\n",
      "309        451           1     1  0.655550\n",
      "310        452           0     1  0.537379\n",
      "311        454           0     1  0.613836\n",
      "312        455           0     1  0.799407\n",
      "313        456           1     1  0.626659\n",
      "314        457           1     1  0.637341\n",
      "315        459           0     1  0.651454\n",
      "316        464           0     1  0.570518\n",
      "317        466           1     1  0.536158\n",
      "318        468           1     1  0.637880\n",
      "319        469           0     1  0.649415\n",
      "320        470           1     1  0.592861\n",
      "321        472           1     1  0.588367\n",
      "322        477           0     1  0.640431\n",
      "323        478           1     1  0.622484\n",
      "324        479           1     1  0.639123\n",
      "325        480           1     1  0.544929\n",
      "326        481           0     1  0.663977\n",
      "327        483           1     0  0.495441\n",
      "328        485           1     1  0.589193\n",
      "329        488           1     1  0.581058\n",
      "330        491           1     1  0.556864\n",
      "331        493           1     1  0.567222\n",
      "332        494           1     1  0.608798\n",
      "333        495           0     1  0.603059\n",
      "334        496           0     1  0.621160\n",
      "335        498           0     1  0.629079\n",
      "336        499           1     1  0.611089\n",
      "337        500           1     1  0.533421\n",
      "338        501           1     1  0.619897\n",
      "339        502           1     1  0.652183\n",
      "340        504           1     1  0.579471\n",
      "341        505           1     1  0.573134\n",
      "342        506           1     1  0.582117\n",
      "343        507           0     1  0.668213\n",
      "344        510           0     1  0.557043\n",
      "345        511           1     1  0.657392\n",
      "346        512           0     1  0.562543\n",
      "347        513           1     1  0.543823\n",
      "348        514           0     1  0.602104\n",
      "349        516           1     1  0.606188\n",
      "350        517           1     1  0.576715\n",
      "351        518           0     1  0.570983\n",
      "352        519           0     1  0.622212\n",
      "353        520           1     1  0.577858\n",
      "354        523           1     1  0.603298\n",
      "355        524           1     1  0.612033\n",
      "356        525           1     1  0.556187\n",
      "357        526           1     1  0.594232\n",
      "358        528           1     0  0.492806\n",
      "359        529           1     1  0.654561\n",
      "360        530           0     1  0.501374\n",
      "361        532           1     1  0.633891\n",
      "362        533           0     1  0.653150\n",
      "363        537           1     1  0.617390\n",
      "364        538           0     1  0.576356\n",
      "365        539           1     1  0.590549\n",
      "366        540           0     1  0.559277\n",
      "367        542           1     1  0.617361\n",
      "368        543           1     1  0.580416\n",
      "369        544           1     1  0.573782\n",
      "370        545           0     1  0.576837\n",
      "371        547           0     1  0.601203\n",
      "372        548           1     1  0.634788\n",
      "373        549           1     1  0.592766\n",
      "374        550           1     1  0.541275\n",
      "375        551           1     1  0.615764\n",
      "376        552           1     1  0.562470\n",
      "377        554           1     1  0.632578\n",
      "378        555           0     1  0.673057\n",
      "379        556           1     1  0.578783\n",
      "380        557           1     1  0.533114\n",
      "381        558           1     1  0.535312\n",
      "382        559           1     1  0.612927\n",
      "383        561           1     1  0.600596\n",
      "384        563           0     1  0.605951\n",
      "385        564           1     1  0.620948\n",
      "386        565           0     1  0.616337\n",
      "387        567           0     1  0.633179\n",
      "388        568           0     1  0.590839\n",
      "389        569           0     1  0.589262\n",
      "390        570           1     1  0.798868\n",
      "391        571           0     1  0.678095\n",
      "392        572           0     1  0.647609\n",
      "393        574           0     1  0.629537\n",
      "394        575           0     1  0.768402\n",
      "395        576           1     1  0.625490\n",
      "396        577           1     1  0.608852\n",
      "397        578           0     1  0.798203\n",
      "398        579           1     1  0.597468\n",
      "399        581           0     1  0.637224\n",
      "400        582           1     1  0.541419\n",
      "401        583           1     1  0.746653\n",
      "402        584           1     1  0.762572\n",
      "403        586           1     1  0.642528\n",
      "404        587           0     1  0.647395\n",
      "405        588           0     1  0.627807\n",
      "406        589           0     1  0.806791\n",
      "407        590           1     1  0.563660\n",
      "408        591           0     1  0.627892\n",
      "409        593           1     1  0.622621\n",
      "410        594           1     1  0.593412\n",
      "411        596           0     1  0.592137\n",
      "412        597           1     1  0.593952\n",
      "413        598           1     1  0.586008\n",
      "414        599           1     1  0.602154\n",
      "415        601           0     1  0.611391\n",
      "416        602           1     1  0.647469\n",
      "417        604           1     1  0.587330\n",
      "418        605           0     1  0.590255\n",
      "419        606           1     1  0.559018\n",
      "420        607           1     1  0.620068\n",
      "421        608           1     1  0.581877\n",
      "422        610           1     1  0.676797\n",
      "423        611           1     1  0.615547\n",
      "424        612           1     1  0.590950\n",
      "425        613           1     1  0.602755\n",
      "426        615           1     1  0.593074\n",
      "427        616           0     1  0.629618\n",
      "428        618           1     1  0.626776\n",
      "429        619           0     1  0.605723\n",
      "430        620           0     1  0.581678\n",
      "431        621           1     1  0.556253\n",
      "432        622           1     1  0.587381\n",
      "433        623           0     1  0.547228\n",
      "434        624           0     1  0.581972\n",
      "435        625           1     1  0.527201\n",
      "436        626           1     1  0.558585\n",
      "437        628           1     1  0.578996\n",
      "438        630           0     1  0.554236\n",
      "439        631           1     1  0.583825\n",
      "440        636           0     1  0.629162\n",
      "441        638           1     1  0.559203\n",
      "442        639           1     1  0.538116\n",
      "443        640           1     1  0.592990\n",
      "444        641           0     1  0.638061\n",
      "445        642           0     1  0.590376\n",
      "446        645           0     1  0.592451\n",
      "447        646           1     1  0.584939\n",
      "448        649           0     1  0.559863\n",
      "449        650           1     1  0.638851\n",
      "450        651           0     1  0.606242\n",
      "451        652           1     1  0.619957\n",
      "452        654           0     1  0.578709\n",
      "453        655           1     1  0.520770\n",
      "454        656           1     1  0.647084\n",
      "455        657           0     1  0.574289\n",
      "456        658           1     1  0.525137\n",
      "457        659           1     1  0.535407\n",
      "458        661           1     1  0.604928\n",
      "459        663           0     1  0.590595\n",
      "460        667           0     1  0.552158\n",
      "461        668           0     1  0.535856\n",
      "462        674           1     1  0.594703\n",
      "463        675           1     1  0.637785\n",
      "464        676           1     1  0.611107\n",
      "465        677           1     1  0.603969\n",
      "466        679           1     1  0.675556\n",
      "467        680           1     1  0.577404\n",
      "468        682           0     1  0.570451\n",
      "469        683           0     1  0.674823\n",
      "470        684           0     1  0.625781\n",
      "471        685           0     1  0.554672\n",
      "472        686           0     1  0.531775\n",
      "473        687           0     1  0.592067\n",
      "474        688           0     1  0.542031\n",
      "475        690           1     1  0.575270\n",
      "476        691           1     1  0.653325\n",
      "477        692           1     1  0.651457\n",
      "478        693           1     1  0.647881\n",
      "479        694           1     1  0.634995\n",
      "480        697           1     1  0.608386\n",
      "481        698           1     1  0.558272\n",
      "482        703           0     1  0.554392\n",
      "483        704           1     1  0.571961\n",
      "484        705           1     0  0.483876\n",
      "485        706           0     1  0.536599\n",
      "486        707           1     1  0.577940\n",
      "487        708           1     1  0.541728\n",
      "488        709           0     1  0.556768\n",
      "489        714           1     1  0.684644\n",
      "490        715           1     1  0.561971\n",
      "491        716           1     1  0.575924\n",
      "492        718           1     1  0.599023\n",
      "493        723           0     1  0.584068\n",
      "494        724           0     1  0.545726\n",
      "495        725           1     1  0.610526\n",
      "496        727           0     1  0.619126\n",
      "497        728           0     1  0.571967\n",
      "498        729           0     1  0.612547\n",
      "499        730           0     1  0.556853\n",
      "500        731           1     1  0.620050\n",
      "501        732           1     1  0.582682\n",
      "502        733           0     1  0.587866\n",
      "503        734           0     1  0.590077\n",
      "504        735           0     1  0.600134\n",
      "505        736           1     1  0.595954\n",
      "506        737           1     1  0.669469\n",
      "507        739           1     1  0.611051\n",
      "508        740           1     1  0.578844\n",
      "509        742           0     1  0.568704\n",
      "510        744           0     1  0.542768\n",
      "511        746           1     1  0.603085\n",
      "512        747           0     1  0.594574\n",
      "513        750           1     1  0.583672\n",
      "514        751           0     1  0.589053\n",
      "515        753           0     1  0.631004\n",
      "516        756           0     1  0.681390\n",
      "517        757           1     1  0.573448\n",
      "518        758           1     1  0.585595\n",
      "519        759           0     1  0.633674\n",
      "520        760           1     1  0.562012\n",
      "521        764           0     1  0.629091\n",
      "522        765           1     1  0.584536\n",
      "523        767           0     1  0.644989\n",
      "524        768           1     1  0.572273\n",
      "525        772           1     1  0.533281\n",
      "526        773           1     1  0.622056\n",
      "527        774           0     1  0.599237\n",
      "528        775           1     1  0.593743\n",
      "529        777           1     1  0.590409\n",
      "530        778           0     1  0.568860\n",
      "531        780           0     1  0.735396\n",
      "532        781           1     0  0.479083\n",
      "533        782           1     1  0.776201\n",
      "534        784           1     1  0.767684\n",
      "535        787           1     1  0.775187\n",
      "536        788           0     1  0.779780\n",
      "537        789           1     1  0.791026\n",
      "538        791           1     1  0.766368\n",
      "539        792           0     1  0.738601\n",
      "540        793           1     1  0.786920\n",
      "541        794           1     1  0.771836\n",
      "542        795           1     1  0.758486\n",
      "543        796           0     1  0.541837\n",
      "544        797           0     1  0.752216\n",
      "545        799           0     1  0.767766\n",
      "546        800           0     1  0.777385\n",
      "547        801           1     1  0.770359\n",
      "548        802           0     1  0.759514\n",
      "549        803           0     1  0.773736\n",
      "550        804           0     1  0.772434\n",
      "551        805           0     1  0.766796\n",
      "552        806           0     1  0.774736\n",
      "553        807           1     1  0.766911\n",
      "554        808           1     1  0.716765\n",
      "555        809           0     1  0.768249\n",
      "556        810           0     1  0.763567\n",
      "557        811           1     1  0.738230\n",
      "558        814           0     1  0.739817\n",
      "559        816           1     1  0.772509\n",
      "560        818           0     1  0.740836\n",
      "561        819           1     1  0.701333\n",
      "562        820           0     0  0.493046\n",
      "563        823           1     1  0.769907\n",
      "564        824           0     1  0.778850\n",
      "565        828           1     1  0.572101\n",
      "566        830           0     1  0.793189\n",
      "567        834           0     0  0.496077\n",
      "568        836           0     1  0.622107\n",
      "569        837           0     1  0.544203\n",
      "570        838           1     1  0.517817\n",
      "571        839           0     1  0.517329\n",
      "572        840           1     1  0.502199\n",
      "573        998           1     1  0.770435\n",
      "574        999           1     1  0.759195\n",
      "575       1000           1     0  0.449564\n",
      "576       1001           1     0  0.489223\n",
      "577       1002           1     1  0.732162\n",
      "578       1003           1     1  0.756451\n",
      "579       1004           0     1  0.503574\n",
      "580       1005           1     1  0.672044\n",
      "581       1007           1     1  0.613856\n",
      "582       1008           1     1  0.535882\n",
      "583       1009           0     1  0.732722\n",
      "584       1010           0     1  0.513689\n",
      "Prediction AUC: 0.5274\n",
      "Prediction Accuracy: 0.5231\n",
      "Prediction Specificity: 0.1331\n",
      "Prediction Sensitivity: 0.8762\n",
      "Prediction Precision: 0.5275\n",
      "the score of the fold number 3 and the type T1wCE: 0.5274178051695452\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     3  0.527418  0.523077  0.133094  0.876221  0.527451\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 585/585 [01:06<00:00,  8.84it/s]\n",
      "Fold 4:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            0           1     1  0.520211\n",
      "1            2           1     1  0.522239\n",
      "2            3           0     1  0.520381\n",
      "3            5           1     1  0.517078\n",
      "4            6           1     1  0.525472\n",
      "5            8           1     1  0.517886\n",
      "6            9           0     1  0.515122\n",
      "7           11           1     0  0.475387\n",
      "8           12           1     1  0.523180\n",
      "9           14           1     1  0.515410\n",
      "10          17           0     1  0.510188\n",
      "11          18           0     1  0.503522\n",
      "12          19           0     1  0.515236\n",
      "13          20           1     1  0.510984\n",
      "14          21           0     1  0.516436\n",
      "15          22           0     1  0.518480\n",
      "16          24           0     0  0.499119\n",
      "17          25           1     1  0.509448\n",
      "18          26           1     1  0.507544\n",
      "19          28           1     1  0.508662\n",
      "20          30           0     1  0.505449\n",
      "21          31           1     1  0.531020\n",
      "22          32           0     1  0.515930\n",
      "23          33           1     1  0.505667\n",
      "24          35           1     1  0.515665\n",
      "25          36           0     1  0.517666\n",
      "26          43           1     1  0.523410\n",
      "27          44           0     1  0.500934\n",
      "28          45           0     1  0.510852\n",
      "29          46           1     1  0.506225\n",
      "30          48           1     1  0.510329\n",
      "31          49           0     0  0.475094\n",
      "32          52           1     0  0.492384\n",
      "33          53           0     1  0.514218\n",
      "34          54           1     1  0.521972\n",
      "35          56           1     1  0.520647\n",
      "36          58           1     1  0.513205\n",
      "37          59           1     0  0.499772\n",
      "38          60           1     0  0.497520\n",
      "39          61           0     1  0.522873\n",
      "40          62           1     1  0.508252\n",
      "41          63           1     1  0.519558\n",
      "42          64           0     1  0.526644\n",
      "43          66           1     1  0.519779\n",
      "44          68           1     0  0.486121\n",
      "45          70           1     0  0.490735\n",
      "46          71           1     1  0.507980\n",
      "47          72           0     0  0.493288\n",
      "48          74           1     1  0.511510\n",
      "49          77           1     1  0.515801\n",
      "50          78           1     1  0.512985\n",
      "51          81           0     1  0.503132\n",
      "52          84           0     1  0.515774\n",
      "53          85           1     1  0.505797\n",
      "54          87           1     1  0.522737\n",
      "55          88           0     1  0.502630\n",
      "56          89           1     1  0.525884\n",
      "57          90           0     1  0.520847\n",
      "58          94           1     1  0.502120\n",
      "59          95           0     1  0.514053\n",
      "60          96           1     1  0.515294\n",
      "61          97           0     1  0.515469\n",
      "62          98           1     1  0.530723\n",
      "63          99           0     1  0.511547\n",
      "64         100           1     0  0.439290\n",
      "65         102           0     0  0.375153\n",
      "66         104           0     0  0.410308\n",
      "67         105           1     1  0.506462\n",
      "68         106           1     0  0.417068\n",
      "69         107           1     0  0.455094\n",
      "70         108           0     0  0.444200\n",
      "71         109           1     0  0.391665\n",
      "72         110           0     0  0.485923\n",
      "73         111           0     0  0.434503\n",
      "74         112           0     0  0.308768\n",
      "75         113           0     0  0.460457\n",
      "76         116           0     1  0.530306\n",
      "77         117           1     0  0.463690\n",
      "78         120           1     0  0.463392\n",
      "79         121           0     0  0.340677\n",
      "80         122           0     0  0.443974\n",
      "81         123           0     0  0.384321\n",
      "82         124           0     0  0.412962\n",
      "83         128           1     0  0.344647\n",
      "84         130           0     0  0.474168\n",
      "85         132           0     0  0.468391\n",
      "86         133           0     0  0.419389\n",
      "87         134           1     0  0.322980\n",
      "88         136           1     0  0.460505\n",
      "89         137           0     0  0.415437\n",
      "90         138           1     0  0.469508\n",
      "91         139           1     1  0.517559\n",
      "92         140           1     0  0.433386\n",
      "93         142           0     0  0.466108\n",
      "94         143           1     0  0.467975\n",
      "95         144           1     0  0.434564\n",
      "96         146           1     0  0.472218\n",
      "97         147           0     0  0.462269\n",
      "98         148           0     1  0.536004\n",
      "99         149           0     1  0.543442\n",
      "100        150           0     0  0.434617\n",
      "101        151           0     0  0.415045\n",
      "102        154           0     1  0.502227\n",
      "103        155           1     0  0.478316\n",
      "104        156           1     0  0.493066\n",
      "105        157           0     1  0.500964\n",
      "106        158           0     0  0.489717\n",
      "107        159           1     0  0.496513\n",
      "108        160           1     0  0.493342\n",
      "109        162           0     0  0.491973\n",
      "110        165           0     0  0.480190\n",
      "111        166           1     1  0.506775\n",
      "112        167           0     0  0.483034\n",
      "113        169           0     0  0.390032\n",
      "114        170           0     0  0.487487\n",
      "115        171           1     0  0.496256\n",
      "116        172           0     0  0.487761\n",
      "117        176           0     0  0.499110\n",
      "118        177           1     0  0.490617\n",
      "119        178           1     0  0.498801\n",
      "120        183           0     0  0.480151\n",
      "121        184           0     0  0.479908\n",
      "122        185           1     1  0.502381\n",
      "123        186           1     0  0.484072\n",
      "124        187           1     0  0.489907\n",
      "125        188           1     0  0.481015\n",
      "126        191           0     0  0.484273\n",
      "127        192           0     0  0.481732\n",
      "128        193           0     0  0.435992\n",
      "129        194           0     0  0.494676\n",
      "130        195           0     0  0.488084\n",
      "131        196           1     0  0.496863\n",
      "132        197           1     0  0.486525\n",
      "133        199           1     0  0.494823\n",
      "134        201           0     0  0.492582\n",
      "135        203           1     0  0.491688\n",
      "136        204           1     0  0.447737\n",
      "137        206           0     0  0.485155\n",
      "138        209           0     0  0.497542\n",
      "139        210           1     0  0.491465\n",
      "140        211           0     0  0.485603\n",
      "141        212           1     0  0.481036\n",
      "142        214           0     0  0.485273\n",
      "143        216           0     0  0.472460\n",
      "144        217           0     0  0.427885\n",
      "145        218           0     0  0.488857\n",
      "146        219           0     1  0.501480\n",
      "147        220           1     0  0.480417\n",
      "148        221           0     0  0.487600\n",
      "149        222           1     0  0.499873\n",
      "150        227           0     1  0.505209\n",
      "151        228           0     0  0.485714\n",
      "152        230           1     0  0.473822\n",
      "153        231           0     0  0.499184\n",
      "154        233           1     0  0.484604\n",
      "155        234           1     0  0.479776\n",
      "156        235           1     1  0.504393\n",
      "157        236           0     0  0.499071\n",
      "158        237           0     0  0.485763\n",
      "159        238           0     0  0.476114\n",
      "160        239           0     0  0.481076\n",
      "161        240           1     1  0.505134\n",
      "162        241           0     0  0.484319\n",
      "163        242           0     0  0.483949\n",
      "164        243           0     0  0.439309\n",
      "165        245           1     0  0.491901\n",
      "166        246           1     1  0.500855\n",
      "167        247           0     0  0.494407\n",
      "168        249           0     0  0.485273\n",
      "169        250           1     1  0.503511\n",
      "170        251           0     0  0.495281\n",
      "171        253           1     0  0.490869\n",
      "172        254           1     0  0.488684\n",
      "173        258           0     1  0.548155\n",
      "174        259           0     0  0.484850\n",
      "175        260           1     0  0.482677\n",
      "176        261           0     0  0.485758\n",
      "177        262           0     1  0.535452\n",
      "178        263           1     0  0.473207\n",
      "179        266           0     0  0.495170\n",
      "180        267           0     0  0.478764\n",
      "181        269           0     1  0.510438\n",
      "182        270           1     0  0.456374\n",
      "183        271           1     1  0.501925\n",
      "184        273           1     0  0.479831\n",
      "185        274           0     1  0.503391\n",
      "186        275           0     1  0.564015\n",
      "187        280           0     0  0.496612\n",
      "188        281           1     0  0.451606\n",
      "189        282           1     0  0.493931\n",
      "190        283           0     0  0.487251\n",
      "191        284           1     0  0.492542\n",
      "192        285           1     0  0.493677\n",
      "193        286           0     0  0.490425\n",
      "194        288           0     0  0.451669\n",
      "195        289           0     0  0.493331\n",
      "196        290           0     0  0.484438\n",
      "197        291           1     0  0.460828\n",
      "198        293           1     0  0.487285\n",
      "199        294           1     1  0.507489\n",
      "200        296           1     0  0.498961\n",
      "201        297           0     0  0.484199\n",
      "202        298           0     0  0.482979\n",
      "203        299           1     0  0.427346\n",
      "204        300           0     0  0.488184\n",
      "205        301           0     0  0.478967\n",
      "206        303           1     0  0.492932\n",
      "207        304           1     0  0.484216\n",
      "208        305           1     0  0.486010\n",
      "209        306           1     0  0.473445\n",
      "210        308           0     0  0.475327\n",
      "211        309           0     0  0.492098\n",
      "212        310           0     0  0.469325\n",
      "213        311           1     0  0.494458\n",
      "214        312           0     1  0.502548\n",
      "215        313           1     0  0.496221\n",
      "216        314           0     0  0.469597\n",
      "217        316           0     0  0.485817\n",
      "218        317           1     0  0.459113\n",
      "219        318           0     0  0.483547\n",
      "220        320           0     0  0.490084\n",
      "221        321           1     0  0.488307\n",
      "222        322           1     0  0.495000\n",
      "223        324           0     0  0.494844\n",
      "224        325           0     0  0.406525\n",
      "225        327           0     0  0.490794\n",
      "226        328           1     0  0.469993\n",
      "227        329           1     0  0.478067\n",
      "228        331           1     0  0.472571\n",
      "229        332           1     0  0.497711\n",
      "230        334           1     0  0.496967\n",
      "231        336           0     0  0.490586\n",
      "232        338           1     1  0.528172\n",
      "233        339           0     0  0.496067\n",
      "234        340           1     0  0.486345\n",
      "235        341           0     0  0.482784\n",
      "236        343           0     0  0.494761\n",
      "237        344           1     0  0.496349\n",
      "238        346           0     0  0.494349\n",
      "239        347           0     0  0.482105\n",
      "240        348           0     0  0.498268\n",
      "241        349           0     1  0.541453\n",
      "242        350           1     0  0.381054\n",
      "243        351           0     0  0.495392\n",
      "244        352           1     0  0.483304\n",
      "245        353           0     0  0.488732\n",
      "246        356           0     0  0.488228\n",
      "247        359           1     1  0.508790\n",
      "248        360           1     0  0.495309\n",
      "249        364           1     0  0.499865\n",
      "250        366           1     0  0.428083\n",
      "251        367           1     0  0.398330\n",
      "252        369           1     0  0.476299\n",
      "253        370           1     0  0.494090\n",
      "254        371           1     0  0.492864\n",
      "255        373           0     0  0.482137\n",
      "256        376           0     0  0.401541\n",
      "257        377           0     1  0.551935\n",
      "258        378           0     1  0.502308\n",
      "259        379           0     0  0.494316\n",
      "260        380           0     0  0.489576\n",
      "261        382           0     0  0.498238\n",
      "262        383           1     0  0.466793\n",
      "263        386           1     0  0.492701\n",
      "264        387           0     0  0.499407\n",
      "265        388           0     0  0.391760\n",
      "266        389           0     0  0.496416\n",
      "267        390           0     1  0.508529\n",
      "268        391           0     0  0.436994\n",
      "269        392           0     0  0.496629\n",
      "270        395           0     0  0.496294\n",
      "271        397           0     0  0.492961\n",
      "272        399           0     0  0.486042\n",
      "273        400           1     1  0.501024\n",
      "274        401           0     0  0.497133\n",
      "275        402           0     0  0.476711\n",
      "276        403           1     0  0.450140\n",
      "277        404           1     0  0.496809\n",
      "278        405           0     1  0.502574\n",
      "279        406           1     0  0.491005\n",
      "280        407           0     0  0.482418\n",
      "281        408           1     1  0.500552\n",
      "282        409           1     0  0.495741\n",
      "283        410           0     0  0.491059\n",
      "284        412           0     0  0.488160\n",
      "285        413           1     0  0.498429\n",
      "286        414           0     0  0.493370\n",
      "287        416           1     0  0.478473\n",
      "288        417           0     0  0.497107\n",
      "289        418           0     0  0.458554\n",
      "290        419           0     0  0.475139\n",
      "291        421           0     0  0.488654\n",
      "292        423           0     0  0.483700\n",
      "293        425           1     0  0.481739\n",
      "294        426           1     0  0.483127\n",
      "295        429           1     1  0.503543\n",
      "296        430           0     0  0.492555\n",
      "297        431           1     0  0.487532\n",
      "298        432           0     1  0.500676\n",
      "299        433           0     0  0.494469\n",
      "300        436           1     0  0.474002\n",
      "301        440           1     0  0.492410\n",
      "302        441           0     1  0.522215\n",
      "303        442           1     0  0.468819\n",
      "304        443           1     0  0.496160\n",
      "305        444           0     0  0.493214\n",
      "306        445           0     0  0.487249\n",
      "307        446           0     0  0.468785\n",
      "308        449           1     0  0.489778\n",
      "309        451           1     0  0.492568\n",
      "310        452           0     0  0.498461\n",
      "311        454           0     1  0.516197\n",
      "312        455           0     1  0.514650\n",
      "313        456           1     0  0.487086\n",
      "314        457           1     0  0.456560\n",
      "315        459           0     0  0.341273\n",
      "316        464           0     0  0.479501\n",
      "317        466           1     1  0.529584\n",
      "318        468           1     1  0.518960\n",
      "319        469           0     1  0.519915\n",
      "320        470           1     1  0.514318\n",
      "321        472           1     1  0.506013\n",
      "322        477           0     1  0.517049\n",
      "323        478           1     1  0.506237\n",
      "324        479           1     0  0.497485\n",
      "325        480           1     1  0.518943\n",
      "326        481           0     1  0.515284\n",
      "327        483           1     1  0.519412\n",
      "328        485           1     1  0.515851\n",
      "329        488           1     1  0.512806\n",
      "330        491           1     1  0.518699\n",
      "331        493           1     1  0.519064\n",
      "332        494           1     1  0.524735\n",
      "333        495           0     1  0.506652\n",
      "334        496           0     1  0.502024\n",
      "335        498           0     1  0.512959\n",
      "336        499           1     0  0.490080\n",
      "337        500           1     1  0.520520\n",
      "338        501           1     1  0.515962\n",
      "339        502           1     1  0.503820\n",
      "340        504           1     1  0.513237\n",
      "341        505           1     1  0.515938\n",
      "342        506           1     1  0.507576\n",
      "343        507           0     1  0.516119\n",
      "344        510           0     1  0.515492\n",
      "345        511           1     1  0.500532\n",
      "346        512           0     1  0.527722\n",
      "347        513           1     1  0.519804\n",
      "348        514           0     1  0.513847\n",
      "349        516           1     1  0.502827\n",
      "350        517           1     1  0.501725\n",
      "351        518           0     1  0.515001\n",
      "352        519           0     0  0.486598\n",
      "353        520           1     1  0.523787\n",
      "354        523           1     1  0.509068\n",
      "355        524           1     1  0.511935\n",
      "356        525           1     1  0.511867\n",
      "357        526           1     0  0.480041\n",
      "358        528           1     0  0.495348\n",
      "359        529           1     1  0.507296\n",
      "360        530           0     1  0.508300\n",
      "361        532           1     1  0.523414\n",
      "362        533           0     0  0.495829\n",
      "363        537           1     0  0.482113\n",
      "364        538           0     1  0.526611\n",
      "365        539           1     1  0.517167\n",
      "366        540           0     1  0.521210\n",
      "367        542           1     1  0.512042\n",
      "368        543           1     0  0.498689\n",
      "369        544           1     1  0.522358\n",
      "370        545           0     1  0.521851\n",
      "371        547           0     1  0.500185\n",
      "372        548           1     1  0.521763\n",
      "373        549           1     1  0.504209\n",
      "374        550           1     1  0.514672\n",
      "375        551           1     1  0.512514\n",
      "376        552           1     1  0.522588\n",
      "377        554           1     1  0.533542\n",
      "378        555           0     1  0.508314\n",
      "379        556           1     0  0.491312\n",
      "380        557           1     1  0.524169\n",
      "381        558           1     1  0.517276\n",
      "382        559           1     1  0.502477\n",
      "383        561           1     1  0.502424\n",
      "384        563           0     0  0.486770\n",
      "385        564           1     1  0.509214\n",
      "386        565           0     0  0.495579\n",
      "387        567           0     0  0.498909\n",
      "388        568           0     0  0.437936\n",
      "389        569           0     0  0.489761\n",
      "390        570           1     1  0.561487\n",
      "391        571           0     0  0.259213\n",
      "392        572           0     0  0.470743\n",
      "393        574           0     1  0.502887\n",
      "394        575           0     1  0.501667\n",
      "395        576           1     0  0.396204\n",
      "396        577           1     0  0.494960\n",
      "397        578           0     1  0.544723\n",
      "398        579           1     1  0.506559\n",
      "399        581           0     0  0.482068\n",
      "400        582           1     0  0.483334\n",
      "401        583           1     0  0.488904\n",
      "402        584           1     0  0.430296\n",
      "403        586           1     0  0.492798\n",
      "404        587           0     0  0.483524\n",
      "405        588           0     0  0.491753\n",
      "406        589           0     0  0.480342\n",
      "407        590           1     0  0.496572\n",
      "408        591           0     1  0.503005\n",
      "409        593           1     0  0.499295\n",
      "410        594           1     0  0.457155\n",
      "411        596           0     0  0.499758\n",
      "412        597           1     0  0.498353\n",
      "413        598           1     0  0.494670\n",
      "414        599           1     0  0.492037\n",
      "415        601           0     1  0.509871\n",
      "416        602           1     1  0.515713\n",
      "417        604           1     1  0.517471\n",
      "418        605           0     1  0.501132\n",
      "419        606           1     0  0.490609\n",
      "420        607           1     0  0.486660\n",
      "421        608           1     1  0.504694\n",
      "422        610           1     1  0.515759\n",
      "423        611           1     1  0.522914\n",
      "424        612           1     1  0.507255\n",
      "425        613           1     1  0.518150\n",
      "426        615           1     1  0.516749\n",
      "427        616           0     0  0.493662\n",
      "428        618           1     1  0.503185\n",
      "429        619           0     0  0.499388\n",
      "430        620           0     1  0.513341\n",
      "431        621           1     1  0.511446\n",
      "432        622           1     1  0.521945\n",
      "433        623           0     1  0.515565\n",
      "434        624           0     1  0.525605\n",
      "435        625           1     1  0.502882\n",
      "436        626           1     1  0.514494\n",
      "437        628           1     1  0.514177\n",
      "438        630           0     1  0.520387\n",
      "439        631           1     1  0.511409\n",
      "440        636           0     1  0.508584\n",
      "441        638           1     1  0.524320\n",
      "442        639           1     1  0.514580\n",
      "443        640           1     1  0.523017\n",
      "444        641           0     0  0.499949\n",
      "445        642           0     1  0.510068\n",
      "446        645           0     1  0.512123\n",
      "447        646           1     1  0.513018\n",
      "448        649           0     1  0.513316\n",
      "449        650           1     0  0.492449\n",
      "450        651           0     1  0.515090\n",
      "451        652           1     0  0.475057\n",
      "452        654           0     1  0.526448\n",
      "453        655           1     1  0.501376\n",
      "454        656           1     0  0.431372\n",
      "455        657           0     1  0.506348\n",
      "456        658           1     1  0.506064\n",
      "457        659           1     1  0.518862\n",
      "458        661           1     1  0.519142\n",
      "459        663           0     1  0.508518\n",
      "460        667           0     1  0.507831\n",
      "461        668           0     1  0.523505\n",
      "462        674           1     1  0.523283\n",
      "463        675           1     1  0.520033\n",
      "464        676           1     1  0.510684\n",
      "465        677           1     0  0.495503\n",
      "466        679           1     0  0.426846\n",
      "467        680           1     1  0.506730\n",
      "468        682           0     1  0.512220\n",
      "469        683           0     0  0.498325\n",
      "470        684           0     1  0.510645\n",
      "471        685           0     1  0.511257\n",
      "472        686           0     1  0.522386\n",
      "473        687           0     1  0.522216\n",
      "474        688           0     1  0.505232\n",
      "475        690           1     1  0.526027\n",
      "476        691           1     0  0.494272\n",
      "477        692           1     1  0.500480\n",
      "478        693           1     1  0.519109\n",
      "479        694           1     1  0.505890\n",
      "480        697           1     1  0.506567\n",
      "481        698           1     1  0.516734\n",
      "482        703           0     1  0.518607\n",
      "483        704           1     1  0.525575\n",
      "484        705           1     1  0.512297\n",
      "485        706           0     1  0.514971\n",
      "486        707           1     1  0.500936\n",
      "487        708           1     1  0.520544\n",
      "488        709           0     1  0.520613\n",
      "489        714           1     1  0.522171\n",
      "490        715           1     1  0.515452\n",
      "491        716           1     1  0.512305\n",
      "492        718           1     1  0.507888\n",
      "493        723           0     1  0.518086\n",
      "494        724           0     1  0.521601\n",
      "495        725           1     1  0.517373\n",
      "496        727           0     1  0.500958\n",
      "497        728           0     1  0.514187\n",
      "498        729           0     1  0.503836\n",
      "499        730           0     1  0.535414\n",
      "500        731           1     1  0.516136\n",
      "501        732           1     1  0.527171\n",
      "502        733           0     0  0.499737\n",
      "503        734           0     0  0.493198\n",
      "504        735           0     1  0.505925\n",
      "505        736           1     1  0.514717\n",
      "506        737           1     1  0.513381\n",
      "507        739           1     0  0.492600\n",
      "508        740           1     1  0.520989\n",
      "509        742           0     1  0.513311\n",
      "510        744           0     1  0.502169\n",
      "511        746           1     1  0.519659\n",
      "512        747           0     1  0.505040\n",
      "513        750           1     1  0.516310\n",
      "514        751           0     0  0.497866\n",
      "515        753           0     0  0.475485\n",
      "516        756           0     0  0.490720\n",
      "517        757           1     1  0.521925\n",
      "518        758           1     1  0.503219\n",
      "519        759           0     0  0.485299\n",
      "520        760           1     1  0.501630\n",
      "521        764           0     0  0.486291\n",
      "522        765           1     1  0.527703\n",
      "523        767           0     1  0.512446\n",
      "524        768           1     1  0.508554\n",
      "525        772           1     1  0.519269\n",
      "526        773           1     1  0.509928\n",
      "527        774           0     1  0.500448\n",
      "528        775           1     1  0.513675\n",
      "529        777           1     1  0.517759\n",
      "530        778           0     1  0.520866\n",
      "531        780           0     0  0.438358\n",
      "532        781           1     0  0.465208\n",
      "533        782           1     1  0.560299\n",
      "534        784           1     1  0.550168\n",
      "535        787           1     1  0.533632\n",
      "536        788           0     1  0.556193\n",
      "537        789           1     1  0.552365\n",
      "538        791           1     1  0.508273\n",
      "539        792           0     1  0.555179\n",
      "540        793           1     1  0.552099\n",
      "541        794           1     1  0.545874\n",
      "542        795           1     1  0.519106\n",
      "543        796           0     0  0.491915\n",
      "544        797           0     0  0.499600\n",
      "545        799           0     1  0.541600\n",
      "546        800           0     1  0.534963\n",
      "547        801           1     1  0.525099\n",
      "548        802           0     1  0.522888\n",
      "549        803           0     1  0.535918\n",
      "550        804           0     1  0.525122\n",
      "551        805           0     0  0.495067\n",
      "552        806           0     1  0.547243\n",
      "553        807           1     1  0.560495\n",
      "554        808           1     1  0.516642\n",
      "555        809           0     1  0.570684\n",
      "556        810           0     1  0.529966\n",
      "557        811           1     0  0.473460\n",
      "558        814           0     0  0.493445\n",
      "559        816           1     1  0.549156\n",
      "560        818           0     1  0.551914\n",
      "561        819           1     1  0.547480\n",
      "562        820           0     0  0.460759\n",
      "563        823           1     1  0.583043\n",
      "564        824           0     1  0.549832\n",
      "565        828           1     0  0.489322\n",
      "566        830           0     1  0.572264\n",
      "567        834           0     1  0.503032\n",
      "568        836           0     1  0.521075\n",
      "569        837           0     0  0.494416\n",
      "570        838           1     0  0.496923\n",
      "571        839           0     0  0.497470\n",
      "572        840           1     0  0.481499\n",
      "573        998           1     1  0.518915\n",
      "574        999           1     0  0.352177\n",
      "575       1000           1     0  0.493918\n",
      "576       1001           1     0  0.487712\n",
      "577       1002           1     0  0.415432\n",
      "578       1003           1     0  0.420685\n",
      "579       1004           0     0  0.493292\n",
      "580       1005           1     0  0.370884\n",
      "581       1007           1     0  0.465980\n",
      "582       1008           1     0  0.491380\n",
      "583       1009           0     0  0.339901\n",
      "584       1010           0     1  0.502062\n",
      "Prediction AUC: 0.5521\n",
      "Prediction Accuracy: 0.5538\n",
      "Prediction Specificity: 0.5719\n",
      "Prediction Sensitivity: 0.5375\n",
      "Prediction Precision: 0.5810\n",
      "the score of the fold number 4 and the type T1wCE: 0.5521055468328919\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     4  0.552106  0.553846  0.571942  0.537459  0.580986\n",
      "Prediction AUC: 0.5083\n",
      "Prediction Accuracy: 0.0000\n",
      "Prediction Specificity: 0.0000\n",
      "Prediction Sensitivity: 0.0000\n",
      "Prediction Precision: 0.0000\n",
      "the final socre of the type T1wCE\n",
      "0.508307360626157\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0   all  0.508307  0.519316  0.283453  0.732899  0.529042\n",
      "\n",
      "\n",
      "\n",
      "the final score is\n",
      "0.508307360626157\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/validation.py --models_folder tunisiaai_B --csv_file train_fold.csv --full_set True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kfm-admin/GBM-MGMT-Detection/notebooks/../rsnaresnet10/working/validation.py:47: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data1.append(data2, ignore_index=True)\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 157/157 [00:24<00:00,  6.46it/s]\n",
      "Fold 0:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            3           0     1  0.595111\n",
      "1           12           1     1  0.589631\n",
      "2           21           0     1  0.571918\n",
      "3           22           0     1  0.582914\n",
      "4           25           1     1  0.567604\n",
      "5           28           1     1  0.586445\n",
      "6           32           0     1  0.572056\n",
      "7           33           1     1  0.555618\n",
      "8           46           1     1  0.590620\n",
      "9           48           1     1  0.575131\n",
      "10          49           0     1  0.578886\n",
      "11          58           1     1  0.585758\n",
      "12          72           0     1  0.542734\n",
      "13          84           0     1  0.586522\n",
      "14          87           1     1  0.589111\n",
      "15          90           0     1  0.588565\n",
      "16          94           1     1  0.557477\n",
      "17          96           1     1  0.587485\n",
      "18          97           0     1  0.587068\n",
      "19         124           0     1  0.579681\n",
      "20         133           0     1  0.532485\n",
      "21         137           0     1  0.591936\n",
      "22         140           1     1  0.622009\n",
      "23         148           0     1  0.573386\n",
      "24         156           1     1  0.576671\n",
      "25         162           0     1  0.523969\n",
      "26         172           0     1  0.535761\n",
      "27         178           1     1  0.540152\n",
      "28         183           0     1  0.541514\n",
      "29         211           0     1  0.527184\n",
      "30         214           0     1  0.555843\n",
      "31         243           0     1  0.611865\n",
      "32         249           0     1  0.528671\n",
      "33         260           1     1  0.529551\n",
      "34         266           0     1  0.562844\n",
      "35         270           1     1  0.632029\n",
      "36         283           0     1  0.523372\n",
      "37         285           1     1  0.534725\n",
      "38         299           1     1  0.526123\n",
      "39         304           1     1  0.527234\n",
      "40         306           1     1  0.530173\n",
      "41         310           0     1  0.545837\n",
      "42         312           0     1  0.570058\n",
      "43         317           1     1  0.579914\n",
      "44         332           1     1  0.544416\n",
      "45         339           0     1  0.535519\n",
      "46         340           1     1  0.527102\n",
      "47         344           1     1  0.545796\n",
      "48         347           0     1  0.526307\n",
      "49         351           0     1  0.576692\n",
      "50         359           1     1  0.585946\n",
      "51         377           0     1  0.613714\n",
      "52         379           0     1  0.541372\n",
      "53         390           0     1  0.533946\n",
      "54         401           0     1  0.532085\n",
      "55         414           0     1  0.527781\n",
      "56         416           1     1  0.521928\n",
      "57         419           0     1  0.508388\n",
      "58         425           1     1  0.520335\n",
      "59         432           0     1  0.551032\n",
      "60         442           1     1  0.574115\n",
      "61         446           0     1  0.546506\n",
      "62         454           0     1  0.575501\n",
      "63         456           1     1  0.565585\n",
      "64         466           1     1  0.601674\n",
      "65         472           1     1  0.560484\n",
      "66         493           1     1  0.583731\n",
      "67         494           1     1  0.594761\n",
      "68         499           1     1  0.554937\n",
      "69         505           1     1  0.577660\n",
      "70         511           1     1  0.570723\n",
      "71         519           0     1  0.564245\n",
      "72         524           1     1  0.577707\n",
      "73         526           1     1  0.553626\n",
      "74         538           0     1  0.595806\n",
      "75         540           0     1  0.582760\n",
      "76         547           0     1  0.575431\n",
      "77         549           1     1  0.558810\n",
      "78         552           1     1  0.603775\n",
      "79         558           1     1  0.582648\n",
      "80         575           0     1  0.592274\n",
      "81         577           1     1  0.567731\n",
      "82         582           1     1  0.534877\n",
      "83         583           1     1  0.625627\n",
      "84         586           1     1  0.560164\n",
      "85         594           1     1  0.538077\n",
      "86         596           0     1  0.555084\n",
      "87         597           1     1  0.556325\n",
      "88         598           1     1  0.557624\n",
      "89         616           0     1  0.569393\n",
      "90         641           0     1  0.575123\n",
      "91         642           0     1  0.570153\n",
      "92         651           0     1  0.571783\n",
      "93         652           1     1  0.552731\n",
      "94         656           1     1  0.569590\n",
      "95         657           0     1  0.575502\n",
      "96         667           0     1  0.566193\n",
      "97         690           1     1  0.583641\n",
      "98         704           1     1  0.604504\n",
      "99         706           0     1  0.568035\n",
      "100        714           1     1  0.592474\n",
      "101        716           1     1  0.577206\n",
      "102        744           0     1  0.551177\n",
      "103        747           0     1  0.568223\n",
      "104        750           1     1  0.584829\n",
      "105        753           0     1  0.562312\n",
      "106        759           0     1  0.565264\n",
      "107        773           1     1  0.574287\n",
      "108        777           1     1  0.579579\n",
      "109        784           1     1  0.599052\n",
      "110        789           1     1  0.614987\n",
      "111        794           1     1  0.590411\n",
      "112        837           0     1  0.536855\n",
      "113        838           1     1  0.533781\n",
      "114        840           1     1  0.528118\n",
      "115       1005           1     1  0.603450\n",
      "116       1009           0     1  0.616574\n",
      "117     100091           0     0  0.436495\n",
      "118     100130           1     0  0.450453\n",
      "119     100132           0     0  0.452453\n",
      "120     100139           0     0  0.426304\n",
      "121     100143           0     0  0.437663\n",
      "122     100148           0     0  0.419045\n",
      "123     100148           0     0  0.419045\n",
      "124     100197           1     0  0.422024\n",
      "125     100260           1     0  0.465612\n",
      "126     100263           1     0  0.464603\n",
      "127     100301           0     0  0.448448\n",
      "128     100304           0     0  0.449342\n",
      "129     100312           0     0  0.434898\n",
      "130     100316           0     0  0.445187\n",
      "131     100354           0     0  0.448539\n",
      "132     100359           1     0  0.461034\n",
      "133     100360           1     0  0.436606\n",
      "134     100363           1     1  0.538660\n",
      "135     100367           0     0  0.448742\n",
      "136     100369           0     1  0.545787\n",
      "137     100375           0     0  0.492048\n",
      "138     100391           0     0  0.432954\n",
      "139     100395           1     0  0.442100\n",
      "140     100398           0     0  0.436718\n",
      "141     100401           1     0  0.489967\n",
      "142     100411           0     0  0.429468\n",
      "143     100413           0     0  0.430262\n",
      "144     100414           1     1  0.524289\n",
      "145     100419           0     0  0.440147\n",
      "146     100424           1     0  0.457293\n",
      "147     100446           1     0  0.436779\n",
      "148     100455           1     0  0.456170\n",
      "149     100457           0     0  0.467795\n",
      "150     100458           0     0  0.431768\n",
      "151     100463           0     0  0.480421\n",
      "152     100470           0     0  0.423317\n",
      "153     100486           0     0  0.482297\n",
      "154     100494           1     0  0.497494\n",
      "155     100496           1     1  0.558080\n",
      "156     100499           0     0  0.429689\n",
      "Prediction AUC: 0.6195\n",
      "Prediction Accuracy: 0.5669\n",
      "Prediction Specificity: 0.3000\n",
      "Prediction Sensitivity: 0.8442\n",
      "Prediction Precision: 0.5372\n",
      "the score of the fold number 0 and the type T1wCE: 0.6194805194805195\n",
      "  model       AUC       acc  spec      sens     prec\n",
      "0     0  0.619481  0.566879   0.3  0.844156  0.53719\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 153/153 [00:22<00:00,  6.81it/s]\n",
      "Fold 1:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            0           1     1  0.534692\n",
      "1            5           1     1  0.531503\n",
      "2            8           1     1  0.540111\n",
      "3           17           0     1  0.531323\n",
      "4           19           0     1  0.534001\n",
      "5           20           1     1  0.500863\n",
      "6           24           0     1  0.501035\n",
      "7           36           0     1  0.531318\n",
      "8           62           1     1  0.521540\n",
      "9           66           1     1  0.539712\n",
      "10          85           1     1  0.512140\n",
      "11         102           0     0  0.358362\n",
      "12         111           0     1  0.589062\n",
      "13         116           0     0  0.406193\n",
      "14         117           1     1  0.574539\n",
      "15         130           0     1  0.569729\n",
      "16         132           0     0  0.461013\n",
      "17         136           1     0  0.457343\n",
      "18         144           1     0  0.477279\n",
      "19         146           1     0  0.495329\n",
      "20         154           0     1  0.505659\n",
      "21         165           0     0  0.496719\n",
      "22         166           1     1  0.535800\n",
      "23         167           0     1  0.508236\n",
      "24         188           1     1  0.509111\n",
      "25         192           0     1  0.528596\n",
      "26         206           0     0  0.419517\n",
      "27         222           1     1  0.511972\n",
      "28         234           1     1  0.512553\n",
      "29         236           0     1  0.510240\n",
      "30         237           0     1  0.524588\n",
      "31         245           1     1  0.506054\n",
      "32         247           0     1  0.513222\n",
      "33         251           0     1  0.501462\n",
      "34         258           0     0  0.413892\n",
      "35         259           0     1  0.521608\n",
      "36         261           0     1  0.516210\n",
      "37         269           0     1  0.519753\n",
      "38         275           0     0  0.424101\n",
      "39         281           1     1  0.574993\n",
      "40         286           0     0  0.499141\n",
      "41         289           0     1  0.526055\n",
      "42         293           1     0  0.496244\n",
      "43         294           1     0  0.413991\n",
      "44         311           1     1  0.506791\n",
      "45         331           1     1  0.506254\n",
      "46         338           1     0  0.443931\n",
      "47         349           0     0  0.391785\n",
      "48         366           1     1  0.562377\n",
      "49         370           1     1  0.511738\n",
      "50         371           1     1  0.507869\n",
      "51         373           0     1  0.520870\n",
      "52         388           0     0  0.387320\n",
      "53         389           0     1  0.543528\n",
      "54         402           0     1  0.510138\n",
      "55         403           1     1  0.561110\n",
      "56         417           0     1  0.512070\n",
      "57         418           0     1  0.543579\n",
      "58         426           1     1  0.519869\n",
      "59         430           0     1  0.503857\n",
      "60         451           1     1  0.554928\n",
      "61         469           0     1  0.537901\n",
      "62         470           1     1  0.539220\n",
      "63         479           1     1  0.535237\n",
      "64         491           1     1  0.533342\n",
      "65         516           1     1  0.537460\n",
      "66         520           1     1  0.547234\n",
      "67         532           1     1  0.534376\n",
      "68         548           1     1  0.529284\n",
      "69         551           1     1  0.542153\n",
      "70         557           1     1  0.554033\n",
      "71         565           0     1  0.518448\n",
      "72         572           0     1  0.539938\n",
      "73         576           1     0  0.450520\n",
      "74         590           1     1  0.516789\n",
      "75         599           1     1  0.534185\n",
      "76         601           0     1  0.535809\n",
      "77         604           1     1  0.529337\n",
      "78         608           1     1  0.526858\n",
      "79         621           1     1  0.521422\n",
      "80         645           0     1  0.533892\n",
      "81         646           1     1  0.534149\n",
      "82         649           0     1  0.519965\n",
      "83         650           1     1  0.548684\n",
      "84         654           0     1  0.553261\n",
      "85         655           1     1  0.509195\n",
      "86         658           1     1  0.513272\n",
      "87         661           1     1  0.538702\n",
      "88         674           1     1  0.549719\n",
      "89         676           1     1  0.534797\n",
      "90         683           0     1  0.548056\n",
      "91         684           0     1  0.542489\n",
      "92         687           0     1  0.538846\n",
      "93         698           1     1  0.529804\n",
      "94         705           1     1  0.535444\n",
      "95         724           0     1  0.542075\n",
      "96         729           0     1  0.535310\n",
      "97         732           1     1  0.542669\n",
      "98         734           0     1  0.528213\n",
      "99         736           1     1  0.527410\n",
      "100        742           0     1  0.523966\n",
      "101        756           0     1  0.555675\n",
      "102        757           1     1  0.539074\n",
      "103        758           1     1  0.526816\n",
      "104        764           0     1  0.549227\n",
      "105        791           1     0  0.405627\n",
      "106        800           0     0  0.397444\n",
      "107        801           1     0  0.394985\n",
      "108        803           0     0  0.391688\n",
      "109        804           0     0  0.426754\n",
      "110        811           1     0  0.412542\n",
      "111        823           1     0  0.382322\n",
      "112        824           0     0  0.408005\n",
      "113        834           0     0  0.496411\n",
      "114        839           0     1  0.512794\n",
      "115        999           1     0  0.370137\n",
      "116       1008           1     1  0.517922\n",
      "117     100092           0     0  0.459444\n",
      "118     100093           0     0  0.428301\n",
      "119     100095           1     0  0.435172\n",
      "120     100120           0     0  0.435210\n",
      "121     100128           1     0  0.445570\n",
      "122     100129           0     0  0.476420\n",
      "123     100131           1     1  0.534402\n",
      "124     100138           1     0  0.457510\n",
      "125     100145           0     0  0.394719\n",
      "126     100147           0     0  0.419863\n",
      "127     100151           0     0  0.447218\n",
      "128     100197           1     1  0.511176\n",
      "129     100262           0     0  0.442447\n",
      "130     100269           1     0  0.392222\n",
      "131     100282           0     0  0.388801\n",
      "132     100294           0     0  0.396484\n",
      "133     100352           0     0  0.408739\n",
      "134     100355           0     0  0.461799\n",
      "135     100373           0     0  0.439408\n",
      "136     100381           1     0  0.471233\n",
      "137     100387           0     0  0.403621\n",
      "138     100388           1     1  0.515773\n",
      "139     100390           0     0  0.424402\n",
      "140     100393           0     0  0.475824\n",
      "141     100399           1     0  0.401006\n",
      "142     100408           0     0  0.381109\n",
      "143     100409           1     0  0.473430\n",
      "144     100410           0     0  0.422812\n",
      "145     100420           1     1  0.509062\n",
      "146     100443           1     1  0.512016\n",
      "147     100444           1     1  0.535194\n",
      "148     100448           0     0  0.343343\n",
      "149     100449           0     0  0.439382\n",
      "150     100476           0     0  0.383965\n",
      "151     100482           0     0  0.313840\n",
      "152     100490           0     0  0.470686\n",
      "Prediction AUC: 0.6318\n",
      "Prediction Accuracy: 0.6144\n",
      "Prediction Specificity: 0.4872\n",
      "Prediction Sensitivity: 0.7467\n",
      "Prediction Precision: 0.5833\n",
      "the score of the fold number 1 and the type T1wCE: 0.6317948717948717\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     1  0.631795  0.614379  0.487179  0.746667  0.583333\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 149/149 [00:20<00:00,  7.12it/s]\n",
      "Fold 2:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            2           1     1  0.515666\n",
      "1            6           1     1  0.536052\n",
      "2           11           1     0  0.413185\n",
      "3           30           0     0  0.457440\n",
      "4           59           1     0  0.470049\n",
      "5           61           0     1  0.537358\n",
      "6           64           0     1  0.555510\n",
      "7           77           1     0  0.488060\n",
      "8           78           1     1  0.506545\n",
      "9          105           1     0  0.333872\n",
      "10         107           1     0  0.313567\n",
      "11         108           0     0  0.356019\n",
      "12         110           0     0  0.337215\n",
      "13         112           0     0  0.271278\n",
      "14         121           0     0  0.155555\n",
      "15         122           0     0  0.363689\n",
      "16         123           0     0  0.238796\n",
      "17         138           1     0  0.427949\n",
      "18         142           0     0  0.300434\n",
      "19         157           0     0  0.489431\n",
      "20         159           1     0  0.428595\n",
      "21         169           0     0  0.277661\n",
      "22         185           1     0  0.477392\n",
      "23         186           1     0  0.418156\n",
      "24         187           1     0  0.429645\n",
      "25         194           0     0  0.441401\n",
      "26         199           1     0  0.430873\n",
      "27         201           0     0  0.431146\n",
      "28         218           0     0  0.429159\n",
      "29         227           0     0  0.472465\n",
      "30         230           1     0  0.376827\n",
      "31         239           0     0  0.416751\n",
      "32         241           0     0  0.420801\n",
      "33         262           0     0  0.292502\n",
      "34         271           1     0  0.461790\n",
      "35         282           1     0  0.439410\n",
      "36         284           1     0  0.399995\n",
      "37         291           1     0  0.233393\n",
      "38         298           0     0  0.419659\n",
      "39         300           0     0  0.421306\n",
      "40         303           1     0  0.429227\n",
      "41         308           0     0  0.418906\n",
      "42         321           1     0  0.445585\n",
      "43         327           0     0  0.411361\n",
      "44         334           1     0  0.432334\n",
      "45         336           0     0  0.394642\n",
      "46         346           0     0  0.468301\n",
      "47         350           1     0  0.247855\n",
      "48         352           1     0  0.417924\n",
      "49         356           0     0  0.406054\n",
      "50         367           1     0  0.203590\n",
      "51         369           1     0  0.411483\n",
      "52         383           1     0  0.390748\n",
      "53         386           1     0  0.410423\n",
      "54         392           0     0  0.429167\n",
      "55         397           0     0  0.428935\n",
      "56         400           1     0  0.464891\n",
      "57         406           1     0  0.434901\n",
      "58         412           0     0  0.423539\n",
      "59         423           0     0  0.434889\n",
      "60         433           0     0  0.457237\n",
      "61         436           1     0  0.266820\n",
      "62         441           0     0  0.265657\n",
      "63         449           1     0  0.455140\n",
      "64         452           0     0  0.453557\n",
      "65         459           0     0  0.354922\n",
      "66         468           1     1  0.536689\n",
      "67         480           1     1  0.521762\n",
      "68         481           0     1  0.504068\n",
      "69         483           1     1  0.542891\n",
      "70         495           0     0  0.482438\n",
      "71         510           0     1  0.508559\n",
      "72         525           1     1  0.503625\n",
      "73         528           1     1  0.508748\n",
      "74         530           0     1  0.511342\n",
      "75         533           0     0  0.443168\n",
      "76         537           1     0  0.424480\n",
      "77         542           1     1  0.501054\n",
      "78         544           1     1  0.532810\n",
      "79         556           1     0  0.477153\n",
      "80         567           0     0  0.435298\n",
      "81         569           0     0  0.416391\n",
      "82         571           0     0  0.291240\n",
      "83         588           0     0  0.416845\n",
      "84         589           0     0  0.390430\n",
      "85         593           1     0  0.457237\n",
      "86         602           1     1  0.500729\n",
      "87         610           1     0  0.496126\n",
      "88         613           1     1  0.512905\n",
      "89         618           1     0  0.461774\n",
      "90         623           0     1  0.531135\n",
      "91         624           0     1  0.536461\n",
      "92         679           1     0  0.314352\n",
      "93         688           0     1  0.512354\n",
      "94         691           1     0  0.455775\n",
      "95         692           1     0  0.458165\n",
      "96         694           1     0  0.461541\n",
      "97         703           0     1  0.522706\n",
      "98         709           0     1  0.532670\n",
      "99         731           1     1  0.516072\n",
      "100        735           0     0  0.477864\n",
      "101        751           0     0  0.456524\n",
      "102        765           1     1  0.532437\n",
      "103        772           1     1  0.526996\n",
      "104        781           1     0  0.378500\n",
      "105        782           1     0  0.320171\n",
      "106        787           1     0  0.320217\n",
      "107        795           1     0  0.293696\n",
      "108        796           0     0  0.419426\n",
      "109        799           0     0  0.297900\n",
      "110        802           0     0  0.374455\n",
      "111        816           1     0  0.279503\n",
      "112        818           0     0  0.299877\n",
      "113        998           1     0  0.086350\n",
      "114       1002           1     0  0.302610\n",
      "115       1003           1     0  0.286957\n",
      "116       1010           0     0  0.468754\n",
      "117     100022           0     0  0.110017\n",
      "118     100098           1     0  0.284781\n",
      "119     100128           1     0  0.151570\n",
      "120     100134           0     0  0.001335\n",
      "121     100140           0     0  0.075630\n",
      "122     100141           0     0  0.105347\n",
      "123     100150           0     0  0.102016\n",
      "124     100240           1     0  0.124822\n",
      "125     100244           0     0  0.021041\n",
      "126     100246           0     0  0.084950\n",
      "127     100264           1     0  0.129469\n",
      "128     100302           1     0  0.185514\n",
      "129     100336           0     0  0.081778\n",
      "130     100344           1     0  0.082952\n",
      "131     100356           1     0  0.135585\n",
      "132     100370           0     0  0.087371\n",
      "133     100403           1     0  0.105983\n",
      "134     100418           1     0  0.051923\n",
      "135     100426           0     0  0.064670\n",
      "136     100428           0     0  0.121640\n",
      "137     100434           0     0  0.177903\n",
      "138     100435           0     0  0.097706\n",
      "139     100447           0     0  0.093990\n",
      "140     100453           1     0  0.115833\n",
      "141     100456           0     0  0.060648\n",
      "142     100471           0     0  0.138774\n",
      "143     100473           0     0  0.108822\n",
      "144     100483           0     0  0.184788\n",
      "145     100484           0     0  0.179329\n",
      "146     100489           0     0  0.068158\n",
      "147     100495           0     0  0.222031\n",
      "148     100498           1     0  0.185343\n",
      "Prediction AUC: 0.5913\n",
      "Prediction Accuracy: 0.5503\n",
      "Prediction Specificity: 0.8701\n",
      "Prediction Sensitivity: 0.2083\n",
      "Prediction Precision: 0.6000\n",
      "the score of the fold number 2 and the type T1wCE: 0.5912698412698414\n",
      "  model      AUC       acc     spec      sens  prec\n",
      "0     2  0.59127  0.550336  0.87013  0.208333   0.6\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 155/155 [00:24<00:00,  6.39it/s]\n",
      "Fold 3:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0           26           1     1  0.603939\n",
      "1           31           1     1  0.667296\n",
      "2           43           1     1  0.656533\n",
      "3           45           0     1  0.615303\n",
      "4           54           1     1  0.659957\n",
      "5           56           1     1  0.627431\n",
      "6           63           1     1  0.642928\n",
      "7           68           1     1  0.602829\n",
      "8           70           1     1  0.566899\n",
      "9           71           1     1  0.616138\n",
      "10          74           1     1  0.631249\n",
      "11          88           0     1  0.605520\n",
      "12          89           1     1  0.656851\n",
      "13          99           0     1  0.620382\n",
      "14         106           1     0  0.365969\n",
      "15         109           1     0  0.476312\n",
      "16         113           0     0  0.387308\n",
      "17         139           1     0  0.488828\n",
      "18         147           0     1  0.545495\n",
      "19         149           0     0  0.490473\n",
      "20         150           0     0  0.481426\n",
      "21         151           0     0  0.456737\n",
      "22         155           1     0  0.499526\n",
      "23         170           0     1  0.580419\n",
      "24         171           1     1  0.563107\n",
      "25         176           0     1  0.589156\n",
      "26         191           0     1  0.533982\n",
      "27         196           1     1  0.547103\n",
      "28         203           1     1  0.554480\n",
      "29         209           0     1  0.538200\n",
      "30         216           0     1  0.567083\n",
      "31         221           0     1  0.542867\n",
      "32         228           0     0  0.415570\n",
      "33         231           0     1  0.560760\n",
      "34         235           1     1  0.576599\n",
      "35         238           0     1  0.508580\n",
      "36         240           1     1  0.576444\n",
      "37         242           0     1  0.538852\n",
      "38         253           1     1  0.515194\n",
      "39         263           1     0  0.482215\n",
      "40         267           0     1  0.515243\n",
      "41         288           0     1  0.585182\n",
      "42         290           0     1  0.532509\n",
      "43         297           0     1  0.559276\n",
      "44         301           0     1  0.516524\n",
      "45         309           0     0  0.498958\n",
      "46         313           1     1  0.533809\n",
      "47         316           0     1  0.540470\n",
      "48         320           0     1  0.548691\n",
      "49         324           0     1  0.531724\n",
      "50         328           1     1  0.511300\n",
      "51         343           0     1  0.531657\n",
      "52         348           0     1  0.577946\n",
      "53         376           0     1  0.574595\n",
      "54         387           0     1  0.553360\n",
      "55         395           0     1  0.546891\n",
      "56         404           1     1  0.559226\n",
      "57         409           1     1  0.536846\n",
      "58         431           1     1  0.547648\n",
      "59         440           1     1  0.532788\n",
      "60         444           0     1  0.592711\n",
      "61         455           0     1  0.536362\n",
      "62         457           1     1  0.620194\n",
      "63         478           1     1  0.612390\n",
      "64         496           0     1  0.600457\n",
      "65         498           0     1  0.613090\n",
      "66         501           1     1  0.623072\n",
      "67         504           1     1  0.612345\n",
      "68         512           0     1  0.658137\n",
      "69         513           1     1  0.635775\n",
      "70         529           1     1  0.615335\n",
      "71         539           1     1  0.632723\n",
      "72         543           1     1  0.570824\n",
      "73         545           0     1  0.645270\n",
      "74         555           0     1  0.633215\n",
      "75         559           1     1  0.585847\n",
      "76         574           0     1  0.594887\n",
      "77         578           0     1  0.557448\n",
      "78         581           0     1  0.577247\n",
      "79         587           0     1  0.612364\n",
      "80         606           1     1  0.601318\n",
      "81         611           1     1  0.652283\n",
      "82         620           0     1  0.626296\n",
      "83         625           1     1  0.575028\n",
      "84         626           1     1  0.623323\n",
      "85         631           1     1  0.602609\n",
      "86         640           1     1  0.635583\n",
      "87         675           1     1  0.644390\n",
      "88         677           1     1  0.588799\n",
      "89         680           1     1  0.601600\n",
      "90         685           0     1  0.610916\n",
      "91         686           0     1  0.649384\n",
      "92         693           1     1  0.636275\n",
      "93         707           1     1  0.582022\n",
      "94         708           1     1  0.650653\n",
      "95         715           1     1  0.632646\n",
      "96         718           1     1  0.587752\n",
      "97         723           0     1  0.638922\n",
      "98         728           0     1  0.645183\n",
      "99         730           0     1  0.667848\n",
      "100        737           1     1  0.634022\n",
      "101        746           1     1  0.631801\n",
      "102        760           1     1  0.592901\n",
      "103        767           0     1  0.625358\n",
      "104        768           1     1  0.603603\n",
      "105        775           1     1  0.605921\n",
      "106        780           0     0  0.392044\n",
      "107        788           0     0  0.441760\n",
      "108        793           1     0  0.472938\n",
      "109        805           0     0  0.435450\n",
      "110        806           0     0  0.485521\n",
      "111        809           0     0  0.454446\n",
      "112        819           1     0  0.410203\n",
      "113       1000           1     1  0.532212\n",
      "114       1001           1     1  0.519840\n",
      "115       1004           0     1  0.546041\n",
      "116       1007           1     1  0.570592\n",
      "117     100094           0     0  0.239777\n",
      "118     100117           0     0  0.421596\n",
      "119     100121           0     0  0.363522\n",
      "120     100133           0     0  0.398040\n",
      "121     100134           0     1  0.883194\n",
      "122     100135           1     0  0.423321\n",
      "123     100146           0     0  0.352302\n",
      "124     100150           0     0  0.462925\n",
      "125     100183           1     0  0.485536\n",
      "126     100267           0     0  0.427823\n",
      "127     100267           0     0  0.427823\n",
      "128     100312           0     0  0.392558\n",
      "129     100350           1     0  0.398185\n",
      "130     100351           1     0  0.440769\n",
      "131     100352           0     0  0.404875\n",
      "132     100362           0     0  0.453521\n",
      "133     100376           0     0  0.432062\n",
      "134     100378           1     0  0.384222\n",
      "135     100379           0     0  0.433273\n",
      "136     100380           1     0  0.392789\n",
      "137     100384           0     0  0.457543\n",
      "138     100385           0     0  0.307496\n",
      "139     100392           1     1  0.525487\n",
      "140     100402           0     1  0.513232\n",
      "141     100405           0     0  0.417943\n",
      "142     100416           0     0  0.418203\n",
      "143     100430           1     0  0.486077\n",
      "144     100431           0     0  0.419369\n",
      "145     100436           0     0  0.386961\n",
      "146     100439           1     1  0.533090\n",
      "147     100442           1     0  0.442733\n",
      "148     100445           1     1  0.505710\n",
      "149     100460           0     0  0.416406\n",
      "150     100461           1     0  0.404722\n",
      "151     100462           0     0  0.484708\n",
      "152     100469           1     0  0.469645\n",
      "153     100475           0     0  0.442883\n",
      "154     100481           1     0  0.417349\n",
      "Prediction AUC: 0.6353\n",
      "Prediction Accuracy: 0.5806\n",
      "Prediction Specificity: 0.4125\n",
      "Prediction Sensitivity: 0.7600\n",
      "Prediction Precision: 0.5481\n",
      "the score of the fold number 3 and the type T1wCE: 0.6353333333333333\n",
      "  model       AUC       acc    spec  sens      prec\n",
      "0     3  0.635333  0.580645  0.4125  0.76  0.548077\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 156/156 [00:23<00:00,  6.65it/s]\n",
      "Fold 4:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            9           0     1  0.589923\n",
      "1           14           1     1  0.571315\n",
      "2           18           0     1  0.550912\n",
      "3           35           1     1  0.573952\n",
      "4           44           0     1  0.555019\n",
      "5           52           1     1  0.542741\n",
      "6           53           0     1  0.569942\n",
      "7           60           1     1  0.538978\n",
      "8           81           0     1  0.561922\n",
      "9           95           0     1  0.566171\n",
      "10          98           1     1  0.590725\n",
      "11         100           1     0  0.494581\n",
      "12         104           0     0  0.467806\n",
      "13         120           1     0  0.491364\n",
      "14         128           1     0  0.442246\n",
      "15         134           1     0  0.346730\n",
      "16         143           1     1  0.507588\n",
      "17         158           0     1  0.546274\n",
      "18         160           1     1  0.548981\n",
      "19         177           1     1  0.527734\n",
      "20         184           0     1  0.540645\n",
      "21         193           0     0  0.442888\n",
      "22         195           0     1  0.535461\n",
      "23         197           1     1  0.534323\n",
      "24         204           1     1  0.564521\n",
      "25         210           1     1  0.550051\n",
      "26         212           1     1  0.539014\n",
      "27         217           0     1  0.593812\n",
      "28         219           0     1  0.554500\n",
      "29         220           1     1  0.539765\n",
      "30         233           1     1  0.547775\n",
      "31         246           1     1  0.548506\n",
      "32         250           1     1  0.559854\n",
      "33         254           1     1  0.539273\n",
      "34         273           1     1  0.541827\n",
      "35         274           0     1  0.560546\n",
      "36         280           0     1  0.550812\n",
      "37         296           1     1  0.552784\n",
      "38         305           1     1  0.576391\n",
      "39         314           0     1  0.537915\n",
      "40         318           0     1  0.534830\n",
      "41         322           1     1  0.555832\n",
      "42         325           0     1  0.595857\n",
      "43         329           1     1  0.555417\n",
      "44         341           0     1  0.561329\n",
      "45         353           0     1  0.536858\n",
      "46         360           1     1  0.555363\n",
      "47         364           1     1  0.559848\n",
      "48         378           0     1  0.556892\n",
      "49         380           0     1  0.565643\n",
      "50         382           0     1  0.552745\n",
      "51         391           0     1  0.500183\n",
      "52         399           0     1  0.536529\n",
      "53         405           0     1  0.519786\n",
      "54         407           0     1  0.553636\n",
      "55         408           1     1  0.541610\n",
      "56         410           0     1  0.547153\n",
      "57         413           1     1  0.563947\n",
      "58         421           0     1  0.535140\n",
      "59         429           1     0  0.498957\n",
      "60         443           1     1  0.565736\n",
      "61         445           0     1  0.580951\n",
      "62         464           0     1  0.529159\n",
      "63         477           0     1  0.568513\n",
      "64         485           1     1  0.568524\n",
      "65         488           1     1  0.556107\n",
      "66         500           1     1  0.583008\n",
      "67         502           1     1  0.552783\n",
      "68         506           1     1  0.559471\n",
      "69         507           0     1  0.572215\n",
      "70         514           0     1  0.570638\n",
      "71         517           1     1  0.553552\n",
      "72         518           0     1  0.571346\n",
      "73         523           1     1  0.562396\n",
      "74         550           1     1  0.575038\n",
      "75         554           1     1  0.591118\n",
      "76         561           1     1  0.547985\n",
      "77         563           0     1  0.549027\n",
      "78         564           1     1  0.550708\n",
      "79         568           0     1  0.566418\n",
      "80         570           1     1  0.548766\n",
      "81         579           1     1  0.560672\n",
      "82         584           1     1  0.508474\n",
      "83         591           0     1  0.569092\n",
      "84         605           0     1  0.565184\n",
      "85         607           1     1  0.560475\n",
      "86         612           1     1  0.562937\n",
      "87         615           1     1  0.573030\n",
      "88         619           0     1  0.556808\n",
      "89         622           1     1  0.567469\n",
      "90         628           1     1  0.571884\n",
      "91         630           0     1  0.563754\n",
      "92         636           0     1  0.590091\n",
      "93         638           1     1  0.581986\n",
      "94         639           1     1  0.562261\n",
      "95         659           1     1  0.580656\n",
      "96         663           0     1  0.561781\n",
      "97         668           0     1  0.575171\n",
      "98         682           0     1  0.562143\n",
      "99         697           1     1  0.569353\n",
      "100        725           1     1  0.570480\n",
      "101        727           0     1  0.556374\n",
      "102        733           0     1  0.548822\n",
      "103        739           1     1  0.568511\n",
      "104        740           1     1  0.578054\n",
      "105        774           0     1  0.561522\n",
      "106        778           0     1  0.574970\n",
      "107        792           0     0  0.496467\n",
      "108        797           0     0  0.486770\n",
      "109        807           1     1  0.505769\n",
      "110        808           1     0  0.485783\n",
      "111        810           0     1  0.527111\n",
      "112        814           0     1  0.505182\n",
      "113        820           0     1  0.545158\n",
      "114        828           1     1  0.561442\n",
      "115        830           0     1  0.527577\n",
      "116        836           0     0  0.486747\n",
      "117     100034           0     0  0.483358\n",
      "118     100088           0     0  0.466374\n",
      "119     100093           0     0  0.445819\n",
      "120     100115           1     0  0.472718\n",
      "121     100122           0     0  0.432421\n",
      "122     100122           0     0  0.432421\n",
      "123     100124           1     0  0.430633\n",
      "124     100136           0     0  0.425496\n",
      "125     100140           0     0  0.419630\n",
      "126     100144           0     0  0.431380\n",
      "127     100149           0     0  0.491398\n",
      "128     100287           1     0  0.367339\n",
      "129     100290           0     0  0.449957\n",
      "130     100310           1     0  0.438811\n",
      "131     100314           0     0  0.475721\n",
      "132     100344           1     0  0.433363\n",
      "133     100358           1     0  0.405677\n",
      "134     100368           0     0  0.428574\n",
      "135     100371           1     0  0.424377\n",
      "136     100404           1     0  0.447368\n",
      "137     100406           1     0  0.425419\n",
      "138     100407           1     0  0.487651\n",
      "139     100412           1     0  0.440189\n",
      "140     100415           0     0  0.452163\n",
      "141     100421           0     0  0.409462\n",
      "142     100423           1     0  0.454970\n",
      "143     100425           1     0  0.429407\n",
      "144     100432           0     0  0.411769\n",
      "145     100438           1     0  0.445628\n",
      "146     100452           0     0  0.372890\n",
      "147     100454           0     0  0.428954\n",
      "148     100459           0     0  0.397227\n",
      "149     100474           1     0  0.396658\n",
      "150     100479           0     0  0.474084\n",
      "151     100480           0     0  0.456428\n",
      "152     100487           0     0  0.282686\n",
      "153     100488           1     0  0.381752\n",
      "154     100492           0     0  0.469734\n",
      "155     100500           0     0  0.359712\n",
      "Prediction AUC: 0.5473\n",
      "Prediction Accuracy: 0.5321\n",
      "Prediction Specificity: 0.3544\n",
      "Prediction Sensitivity: 0.7143\n",
      "Prediction Precision: 0.5189\n",
      "the score of the fold number 4 and the type T1wCE: 0.5472628637185598\n",
      "  model       AUC       acc     spec      sens      prec\n",
      "0     4  0.547263  0.532051  0.35443  0.714286  0.518868\n",
      "Prediction AUC: 0.5863\n",
      "Prediction Accuracy: 0.5688\n",
      "Prediction Specificity: 0.4822\n",
      "Prediction Sensitivity: 0.6596\n",
      "Prediction Precision: 0.5487\n",
      "Prediction AUC: 0.5863\n",
      "Prediction Accuracy: 0.5688\n",
      "Prediction Specificity: 0.4822\n",
      "Prediction Sensitivity: 0.6596\n",
      "Prediction Precision: 0.5487\n",
      "the final socre of the type T1wCE\n",
      "0.5863079166216654\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0   all  0.586308  0.568831  0.482234  0.659574  0.548673\n",
      "\n",
      "\n",
      "\n",
      "the final score is\n",
      "0.5863079166216654\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/validation.py --models_folder tunisiaai_AB --csv_file all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kfm-admin/GBM-MGMT-Detection/notebooks/../rsnaresnet10/working/validation.py:60: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data1.append(data2, ignore_index=True)\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 157/157 [00:26<00:00,  5.87it/s]\n",
      "Fold 0:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            3           0     1  0.595111\n",
      "1           12           1     1  0.589631\n",
      "2           21           0     1  0.571918\n",
      "3           22           0     1  0.582914\n",
      "4           25           1     1  0.567604\n",
      "5           28           1     1  0.586445\n",
      "6           32           0     1  0.572056\n",
      "7           33           1     1  0.555618\n",
      "8           46           1     1  0.590620\n",
      "9           48           1     1  0.575131\n",
      "10          49           0     1  0.578886\n",
      "11          58           1     1  0.585758\n",
      "12          72           0     1  0.542734\n",
      "13          84           0     1  0.586522\n",
      "14          87           1     1  0.589111\n",
      "15          90           0     1  0.588565\n",
      "16          94           1     1  0.557477\n",
      "17          96           1     1  0.587485\n",
      "18          97           0     1  0.587068\n",
      "19         124           0     1  0.579681\n",
      "20         133           0     1  0.532485\n",
      "21         137           0     1  0.591936\n",
      "22         140           1     1  0.622009\n",
      "23         148           0     1  0.573386\n",
      "24         156           1     1  0.576671\n",
      "25         162           0     1  0.523969\n",
      "26         172           0     1  0.535761\n",
      "27         178           1     1  0.540152\n",
      "28         183           0     1  0.541514\n",
      "29         211           0     1  0.527184\n",
      "30         214           0     1  0.555843\n",
      "31         243           0     1  0.611865\n",
      "32         249           0     1  0.528671\n",
      "33         260           1     1  0.529551\n",
      "34         266           0     1  0.562844\n",
      "35         270           1     1  0.632029\n",
      "36         283           0     1  0.523372\n",
      "37         285           1     1  0.534725\n",
      "38         299           1     1  0.526123\n",
      "39         304           1     1  0.527234\n",
      "40         306           1     1  0.530173\n",
      "41         310           0     1  0.545837\n",
      "42         312           0     1  0.570058\n",
      "43         317           1     1  0.579914\n",
      "44         332           1     1  0.544416\n",
      "45         339           0     1  0.535519\n",
      "46         340           1     1  0.527102\n",
      "47         344           1     1  0.545796\n",
      "48         347           0     1  0.526307\n",
      "49         351           0     1  0.576692\n",
      "50         359           1     1  0.585946\n",
      "51         377           0     1  0.613714\n",
      "52         379           0     1  0.541372\n",
      "53         390           0     1  0.533946\n",
      "54         401           0     1  0.532085\n",
      "55         414           0     1  0.527781\n",
      "56         416           1     1  0.521928\n",
      "57         419           0     1  0.508388\n",
      "58         425           1     1  0.520335\n",
      "59         432           0     1  0.551032\n",
      "60         442           1     1  0.574115\n",
      "61         446           0     1  0.546506\n",
      "62         454           0     1  0.575501\n",
      "63         456           1     1  0.565585\n",
      "64         466           1     1  0.601674\n",
      "65         472           1     1  0.560484\n",
      "66         493           1     1  0.583731\n",
      "67         494           1     1  0.594761\n",
      "68         499           1     1  0.554937\n",
      "69         505           1     1  0.577660\n",
      "70         511           1     1  0.570723\n",
      "71         519           0     1  0.564245\n",
      "72         524           1     1  0.577707\n",
      "73         526           1     1  0.553626\n",
      "74         538           0     1  0.595806\n",
      "75         540           0     1  0.582760\n",
      "76         547           0     1  0.575431\n",
      "77         549           1     1  0.558810\n",
      "78         552           1     1  0.603775\n",
      "79         558           1     1  0.582648\n",
      "80         575           0     1  0.592274\n",
      "81         577           1     1  0.567731\n",
      "82         582           1     1  0.534877\n",
      "83         583           1     1  0.625627\n",
      "84         586           1     1  0.560164\n",
      "85         594           1     1  0.538077\n",
      "86         596           0     1  0.555084\n",
      "87         597           1     1  0.556325\n",
      "88         598           1     1  0.557624\n",
      "89         616           0     1  0.569393\n",
      "90         641           0     1  0.575123\n",
      "91         642           0     1  0.570153\n",
      "92         651           0     1  0.571783\n",
      "93         652           1     1  0.552731\n",
      "94         656           1     1  0.569590\n",
      "95         657           0     1  0.575502\n",
      "96         667           0     1  0.566193\n",
      "97         690           1     1  0.583641\n",
      "98         704           1     1  0.604504\n",
      "99         706           0     1  0.568035\n",
      "100        714           1     1  0.592474\n",
      "101        716           1     1  0.577206\n",
      "102        744           0     1  0.551177\n",
      "103        747           0     1  0.568223\n",
      "104        750           1     1  0.584829\n",
      "105        753           0     1  0.562312\n",
      "106        759           0     1  0.565264\n",
      "107        773           1     1  0.574287\n",
      "108        777           1     1  0.579579\n",
      "109        784           1     1  0.599052\n",
      "110        789           1     1  0.614987\n",
      "111        794           1     1  0.590411\n",
      "112        837           0     1  0.536855\n",
      "113        838           1     1  0.533781\n",
      "114        840           1     1  0.528118\n",
      "115       1005           1     1  0.603450\n",
      "116       1009           0     1  0.616574\n",
      "117     100091           0     0  0.436495\n",
      "118     100130           1     0  0.450453\n",
      "119     100132           0     0  0.452453\n",
      "120     100139           0     0  0.426304\n",
      "121     100143           0     0  0.437663\n",
      "122     100148           0     0  0.419045\n",
      "123     100148           0     0  0.419045\n",
      "124     100197           1     0  0.422024\n",
      "125     100260           1     0  0.465612\n",
      "126     100263           1     0  0.464603\n",
      "127     100301           0     0  0.448448\n",
      "128     100304           0     0  0.449342\n",
      "129     100312           0     0  0.434898\n",
      "130     100316           0     0  0.445187\n",
      "131     100354           0     0  0.448539\n",
      "132     100359           1     0  0.461034\n",
      "133     100360           1     0  0.436606\n",
      "134     100363           1     1  0.538660\n",
      "135     100367           0     0  0.448742\n",
      "136     100369           0     1  0.545787\n",
      "137     100375           0     0  0.492048\n",
      "138     100391           0     0  0.432954\n",
      "139     100395           1     0  0.442100\n",
      "140     100398           0     0  0.436718\n",
      "141     100401           1     0  0.489967\n",
      "142     100411           0     0  0.429468\n",
      "143     100413           0     0  0.430262\n",
      "144     100414           1     1  0.524289\n",
      "145     100419           0     0  0.440147\n",
      "146     100424           1     0  0.457293\n",
      "147     100446           1     0  0.436779\n",
      "148     100455           1     0  0.456170\n",
      "149     100457           0     0  0.467795\n",
      "150     100458           0     0  0.431768\n",
      "151     100463           0     0  0.480421\n",
      "152     100470           0     0  0.423317\n",
      "153     100486           0     0  0.482297\n",
      "154     100494           1     0  0.497494\n",
      "155     100496           1     1  0.558080\n",
      "156     100499           0     0  0.429689\n",
      "Prediction AUC: 0.6195\n",
      "Prediction Accuracy: 0.5669\n",
      "Prediction Specificity: 0.3000\n",
      "Prediction Sensitivity: 0.8442\n",
      "Prediction Precision: 0.5372\n",
      "the score of the fold number 0 and the type T1wCE: 0.6194805194805195\n",
      "  model       AUC       acc  spec      sens     prec\n",
      "0     0  0.619481  0.566879   0.3  0.844156  0.53719\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 153/153 [00:25<00:00,  6.12it/s]\n",
      "Fold 1:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            0           1     1  0.534692\n",
      "1            5           1     1  0.531503\n",
      "2            8           1     1  0.540111\n",
      "3           17           0     1  0.531323\n",
      "4           19           0     1  0.534001\n",
      "5           20           1     1  0.500863\n",
      "6           24           0     1  0.501035\n",
      "7           36           0     1  0.531318\n",
      "8           62           1     1  0.521540\n",
      "9           66           1     1  0.539712\n",
      "10          85           1     1  0.512140\n",
      "11         102           0     0  0.358362\n",
      "12         111           0     1  0.589062\n",
      "13         116           0     0  0.406193\n",
      "14         117           1     1  0.574539\n",
      "15         130           0     1  0.569729\n",
      "16         132           0     0  0.461013\n",
      "17         136           1     0  0.457343\n",
      "18         144           1     0  0.477279\n",
      "19         146           1     0  0.495329\n",
      "20         154           0     1  0.505659\n",
      "21         165           0     0  0.496719\n",
      "22         166           1     1  0.535800\n",
      "23         167           0     1  0.508236\n",
      "24         188           1     1  0.509111\n",
      "25         192           0     1  0.528596\n",
      "26         206           0     0  0.419517\n",
      "27         222           1     1  0.511972\n",
      "28         234           1     1  0.512553\n",
      "29         236           0     1  0.510240\n",
      "30         237           0     1  0.524588\n",
      "31         245           1     1  0.506054\n",
      "32         247           0     1  0.513222\n",
      "33         251           0     1  0.501462\n",
      "34         258           0     0  0.413892\n",
      "35         259           0     1  0.521608\n",
      "36         261           0     1  0.516210\n",
      "37         269           0     1  0.519753\n",
      "38         275           0     0  0.424101\n",
      "39         281           1     1  0.574993\n",
      "40         286           0     0  0.499141\n",
      "41         289           0     1  0.526055\n",
      "42         293           1     0  0.496244\n",
      "43         294           1     0  0.413991\n",
      "44         311           1     1  0.506791\n",
      "45         331           1     1  0.506254\n",
      "46         338           1     0  0.443931\n",
      "47         349           0     0  0.391785\n",
      "48         366           1     1  0.562377\n",
      "49         370           1     1  0.511738\n",
      "50         371           1     1  0.507869\n",
      "51         373           0     1  0.520870\n",
      "52         388           0     0  0.387320\n",
      "53         389           0     1  0.543528\n",
      "54         402           0     1  0.510138\n",
      "55         403           1     1  0.561110\n",
      "56         417           0     1  0.512070\n",
      "57         418           0     1  0.543579\n",
      "58         426           1     1  0.519869\n",
      "59         430           0     1  0.503857\n",
      "60         451           1     1  0.554928\n",
      "61         469           0     1  0.537901\n",
      "62         470           1     1  0.539220\n",
      "63         479           1     1  0.535237\n",
      "64         491           1     1  0.533342\n",
      "65         516           1     1  0.537460\n",
      "66         520           1     1  0.547234\n",
      "67         532           1     1  0.534376\n",
      "68         548           1     1  0.529284\n",
      "69         551           1     1  0.542153\n",
      "70         557           1     1  0.554033\n",
      "71         565           0     1  0.518448\n",
      "72         572           0     1  0.539938\n",
      "73         576           1     0  0.450520\n",
      "74         590           1     1  0.516789\n",
      "75         599           1     1  0.534185\n",
      "76         601           0     1  0.535809\n",
      "77         604           1     1  0.529337\n",
      "78         608           1     1  0.526858\n",
      "79         621           1     1  0.521422\n",
      "80         645           0     1  0.533892\n",
      "81         646           1     1  0.534149\n",
      "82         649           0     1  0.519965\n",
      "83         650           1     1  0.548684\n",
      "84         654           0     1  0.553261\n",
      "85         655           1     1  0.509195\n",
      "86         658           1     1  0.513272\n",
      "87         661           1     1  0.538702\n",
      "88         674           1     1  0.549719\n",
      "89         676           1     1  0.534797\n",
      "90         683           0     1  0.548056\n",
      "91         684           0     1  0.542489\n",
      "92         687           0     1  0.538846\n",
      "93         698           1     1  0.529804\n",
      "94         705           1     1  0.535444\n",
      "95         724           0     1  0.542075\n",
      "96         729           0     1  0.535310\n",
      "97         732           1     1  0.542669\n",
      "98         734           0     1  0.528213\n",
      "99         736           1     1  0.527410\n",
      "100        742           0     1  0.523966\n",
      "101        756           0     1  0.555675\n",
      "102        757           1     1  0.539074\n",
      "103        758           1     1  0.526816\n",
      "104        764           0     1  0.549227\n",
      "105        791           1     0  0.405627\n",
      "106        800           0     0  0.397444\n",
      "107        801           1     0  0.394985\n",
      "108        803           0     0  0.391688\n",
      "109        804           0     0  0.426754\n",
      "110        811           1     0  0.412542\n",
      "111        823           1     0  0.382322\n",
      "112        824           0     0  0.408005\n",
      "113        834           0     0  0.496411\n",
      "114        839           0     1  0.512794\n",
      "115        999           1     0  0.370137\n",
      "116       1008           1     1  0.517922\n",
      "117     100092           0     0  0.459444\n",
      "118     100093           0     0  0.428301\n",
      "119     100095           1     0  0.435172\n",
      "120     100120           0     0  0.435210\n",
      "121     100128           1     0  0.445570\n",
      "122     100129           0     0  0.476420\n",
      "123     100131           1     1  0.534402\n",
      "124     100138           1     0  0.457510\n",
      "125     100145           0     0  0.394719\n",
      "126     100147           0     0  0.419863\n",
      "127     100151           0     0  0.447218\n",
      "128     100197           1     1  0.511176\n",
      "129     100262           0     0  0.442447\n",
      "130     100269           1     0  0.392222\n",
      "131     100282           0     0  0.388801\n",
      "132     100294           0     0  0.396484\n",
      "133     100352           0     0  0.408739\n",
      "134     100355           0     0  0.461799\n",
      "135     100373           0     0  0.439408\n",
      "136     100381           1     0  0.471233\n",
      "137     100387           0     0  0.403621\n",
      "138     100388           1     1  0.515773\n",
      "139     100390           0     0  0.424402\n",
      "140     100393           0     0  0.475824\n",
      "141     100399           1     0  0.401006\n",
      "142     100408           0     0  0.381109\n",
      "143     100409           1     0  0.473430\n",
      "144     100410           0     0  0.422812\n",
      "145     100420           1     1  0.509062\n",
      "146     100443           1     1  0.512016\n",
      "147     100444           1     1  0.535194\n",
      "148     100448           0     0  0.343343\n",
      "149     100449           0     0  0.439382\n",
      "150     100476           0     0  0.383965\n",
      "151     100482           0     0  0.313840\n",
      "152     100490           0     0  0.470686\n",
      "Prediction AUC: 0.6318\n",
      "Prediction Accuracy: 0.6144\n",
      "Prediction Specificity: 0.4872\n",
      "Prediction Sensitivity: 0.7467\n",
      "Prediction Precision: 0.5833\n",
      "the score of the fold number 1 and the type T1wCE: 0.6317948717948717\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0     1  0.631795  0.614379  0.487179  0.746667  0.583333\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 149/149 [00:23<00:00,  6.23it/s]\n",
      "Fold 2:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            2           1     1  0.515666\n",
      "1            6           1     1  0.536052\n",
      "2           11           1     0  0.413185\n",
      "3           30           0     0  0.457440\n",
      "4           59           1     0  0.470049\n",
      "5           61           0     1  0.537358\n",
      "6           64           0     1  0.555510\n",
      "7           77           1     0  0.488060\n",
      "8           78           1     1  0.506545\n",
      "9          105           1     0  0.333872\n",
      "10         107           1     0  0.313567\n",
      "11         108           0     0  0.356019\n",
      "12         110           0     0  0.337215\n",
      "13         112           0     0  0.271278\n",
      "14         121           0     0  0.155555\n",
      "15         122           0     0  0.363689\n",
      "16         123           0     0  0.238796\n",
      "17         138           1     0  0.427949\n",
      "18         142           0     0  0.300434\n",
      "19         157           0     0  0.489431\n",
      "20         159           1     0  0.428595\n",
      "21         169           0     0  0.277661\n",
      "22         185           1     0  0.477392\n",
      "23         186           1     0  0.418156\n",
      "24         187           1     0  0.429645\n",
      "25         194           0     0  0.441401\n",
      "26         199           1     0  0.430873\n",
      "27         201           0     0  0.431146\n",
      "28         218           0     0  0.429159\n",
      "29         227           0     0  0.472465\n",
      "30         230           1     0  0.376827\n",
      "31         239           0     0  0.416751\n",
      "32         241           0     0  0.420801\n",
      "33         262           0     0  0.292502\n",
      "34         271           1     0  0.461790\n",
      "35         282           1     0  0.439410\n",
      "36         284           1     0  0.399995\n",
      "37         291           1     0  0.233393\n",
      "38         298           0     0  0.419659\n",
      "39         300           0     0  0.421306\n",
      "40         303           1     0  0.429227\n",
      "41         308           0     0  0.418906\n",
      "42         321           1     0  0.445585\n",
      "43         327           0     0  0.411361\n",
      "44         334           1     0  0.432334\n",
      "45         336           0     0  0.394642\n",
      "46         346           0     0  0.468301\n",
      "47         350           1     0  0.247855\n",
      "48         352           1     0  0.417924\n",
      "49         356           0     0  0.406054\n",
      "50         367           1     0  0.203590\n",
      "51         369           1     0  0.411483\n",
      "52         383           1     0  0.390748\n",
      "53         386           1     0  0.410423\n",
      "54         392           0     0  0.429167\n",
      "55         397           0     0  0.428935\n",
      "56         400           1     0  0.464891\n",
      "57         406           1     0  0.434901\n",
      "58         412           0     0  0.423539\n",
      "59         423           0     0  0.434889\n",
      "60         433           0     0  0.457237\n",
      "61         436           1     0  0.266820\n",
      "62         441           0     0  0.265657\n",
      "63         449           1     0  0.455140\n",
      "64         452           0     0  0.453557\n",
      "65         459           0     0  0.354922\n",
      "66         468           1     1  0.536689\n",
      "67         480           1     1  0.521762\n",
      "68         481           0     1  0.504068\n",
      "69         483           1     1  0.542891\n",
      "70         495           0     0  0.482438\n",
      "71         510           0     1  0.508559\n",
      "72         525           1     1  0.503625\n",
      "73         528           1     1  0.508748\n",
      "74         530           0     1  0.511342\n",
      "75         533           0     0  0.443168\n",
      "76         537           1     0  0.424480\n",
      "77         542           1     1  0.501054\n",
      "78         544           1     1  0.532810\n",
      "79         556           1     0  0.477153\n",
      "80         567           0     0  0.435298\n",
      "81         569           0     0  0.416391\n",
      "82         571           0     0  0.291240\n",
      "83         588           0     0  0.416845\n",
      "84         589           0     0  0.390430\n",
      "85         593           1     0  0.457237\n",
      "86         602           1     1  0.500729\n",
      "87         610           1     0  0.496126\n",
      "88         613           1     1  0.512905\n",
      "89         618           1     0  0.461774\n",
      "90         623           0     1  0.531135\n",
      "91         624           0     1  0.536461\n",
      "92         679           1     0  0.314352\n",
      "93         688           0     1  0.512354\n",
      "94         691           1     0  0.455775\n",
      "95         692           1     0  0.458165\n",
      "96         694           1     0  0.461541\n",
      "97         703           0     1  0.522706\n",
      "98         709           0     1  0.532670\n",
      "99         731           1     1  0.516072\n",
      "100        735           0     0  0.477864\n",
      "101        751           0     0  0.456524\n",
      "102        765           1     1  0.532437\n",
      "103        772           1     1  0.526996\n",
      "104        781           1     0  0.378500\n",
      "105        782           1     0  0.320171\n",
      "106        787           1     0  0.320217\n",
      "107        795           1     0  0.293696\n",
      "108        796           0     0  0.419426\n",
      "109        799           0     0  0.297900\n",
      "110        802           0     0  0.374455\n",
      "111        816           1     0  0.279503\n",
      "112        818           0     0  0.299877\n",
      "113        998           1     0  0.086350\n",
      "114       1002           1     0  0.302610\n",
      "115       1003           1     0  0.286957\n",
      "116       1010           0     0  0.468754\n",
      "117     100022           0     0  0.110017\n",
      "118     100098           1     0  0.284781\n",
      "119     100128           1     0  0.151570\n",
      "120     100134           0     0  0.001335\n",
      "121     100140           0     0  0.075630\n",
      "122     100141           0     0  0.105347\n",
      "123     100150           0     0  0.102016\n",
      "124     100240           1     0  0.124822\n",
      "125     100244           0     0  0.021041\n",
      "126     100246           0     0  0.084950\n",
      "127     100264           1     0  0.129469\n",
      "128     100302           1     0  0.185514\n",
      "129     100336           0     0  0.081778\n",
      "130     100344           1     0  0.082952\n",
      "131     100356           1     0  0.135585\n",
      "132     100370           0     0  0.087371\n",
      "133     100403           1     0  0.105983\n",
      "134     100418           1     0  0.051923\n",
      "135     100426           0     0  0.064670\n",
      "136     100428           0     0  0.121640\n",
      "137     100434           0     0  0.177903\n",
      "138     100435           0     0  0.097706\n",
      "139     100447           0     0  0.093990\n",
      "140     100453           1     0  0.115833\n",
      "141     100456           0     0  0.060648\n",
      "142     100471           0     0  0.138774\n",
      "143     100473           0     0  0.108822\n",
      "144     100483           0     0  0.184788\n",
      "145     100484           0     0  0.179329\n",
      "146     100489           0     0  0.068158\n",
      "147     100495           0     0  0.222031\n",
      "148     100498           1     0  0.185343\n",
      "Prediction AUC: 0.5913\n",
      "Prediction Accuracy: 0.5503\n",
      "Prediction Specificity: 0.8701\n",
      "Prediction Sensitivity: 0.2083\n",
      "Prediction Precision: 0.6000\n",
      "the score of the fold number 2 and the type T1wCE: 0.5912698412698414\n",
      "  model      AUC       acc     spec      sens  prec\n",
      "0     2  0.59127  0.550336  0.87013  0.208333   0.6\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 155/155 [00:26<00:00,  5.79it/s]\n",
      "Fold 3:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0           26           1     1  0.603939\n",
      "1           31           1     1  0.667296\n",
      "2           43           1     1  0.656533\n",
      "3           45           0     1  0.615303\n",
      "4           54           1     1  0.659957\n",
      "5           56           1     1  0.627431\n",
      "6           63           1     1  0.642928\n",
      "7           68           1     1  0.602829\n",
      "8           70           1     1  0.566899\n",
      "9           71           1     1  0.616138\n",
      "10          74           1     1  0.631249\n",
      "11          88           0     1  0.605520\n",
      "12          89           1     1  0.656851\n",
      "13          99           0     1  0.620382\n",
      "14         106           1     0  0.365969\n",
      "15         109           1     0  0.476312\n",
      "16         113           0     0  0.387308\n",
      "17         139           1     0  0.488828\n",
      "18         147           0     1  0.545495\n",
      "19         149           0     0  0.490473\n",
      "20         150           0     0  0.481426\n",
      "21         151           0     0  0.456737\n",
      "22         155           1     0  0.499526\n",
      "23         170           0     1  0.580419\n",
      "24         171           1     1  0.563107\n",
      "25         176           0     1  0.589156\n",
      "26         191           0     1  0.533982\n",
      "27         196           1     1  0.547103\n",
      "28         203           1     1  0.554480\n",
      "29         209           0     1  0.538200\n",
      "30         216           0     1  0.567083\n",
      "31         221           0     1  0.542867\n",
      "32         228           0     0  0.415570\n",
      "33         231           0     1  0.560760\n",
      "34         235           1     1  0.576599\n",
      "35         238           0     1  0.508580\n",
      "36         240           1     1  0.576444\n",
      "37         242           0     1  0.538852\n",
      "38         253           1     1  0.515194\n",
      "39         263           1     0  0.482215\n",
      "40         267           0     1  0.515243\n",
      "41         288           0     1  0.585182\n",
      "42         290           0     1  0.532509\n",
      "43         297           0     1  0.559276\n",
      "44         301           0     1  0.516524\n",
      "45         309           0     0  0.498958\n",
      "46         313           1     1  0.533809\n",
      "47         316           0     1  0.540470\n",
      "48         320           0     1  0.548691\n",
      "49         324           0     1  0.531724\n",
      "50         328           1     1  0.511300\n",
      "51         343           0     1  0.531657\n",
      "52         348           0     1  0.577946\n",
      "53         376           0     1  0.574595\n",
      "54         387           0     1  0.553360\n",
      "55         395           0     1  0.546891\n",
      "56         404           1     1  0.559226\n",
      "57         409           1     1  0.536846\n",
      "58         431           1     1  0.547648\n",
      "59         440           1     1  0.532788\n",
      "60         444           0     1  0.592711\n",
      "61         455           0     1  0.536362\n",
      "62         457           1     1  0.620194\n",
      "63         478           1     1  0.612390\n",
      "64         496           0     1  0.600457\n",
      "65         498           0     1  0.613090\n",
      "66         501           1     1  0.623072\n",
      "67         504           1     1  0.612345\n",
      "68         512           0     1  0.658137\n",
      "69         513           1     1  0.635775\n",
      "70         529           1     1  0.615335\n",
      "71         539           1     1  0.632723\n",
      "72         543           1     1  0.570824\n",
      "73         545           0     1  0.645270\n",
      "74         555           0     1  0.633215\n",
      "75         559           1     1  0.585847\n",
      "76         574           0     1  0.594887\n",
      "77         578           0     1  0.557448\n",
      "78         581           0     1  0.577247\n",
      "79         587           0     1  0.612364\n",
      "80         606           1     1  0.601318\n",
      "81         611           1     1  0.652283\n",
      "82         620           0     1  0.626296\n",
      "83         625           1     1  0.575028\n",
      "84         626           1     1  0.623323\n",
      "85         631           1     1  0.602609\n",
      "86         640           1     1  0.635583\n",
      "87         675           1     1  0.644390\n",
      "88         677           1     1  0.588799\n",
      "89         680           1     1  0.601600\n",
      "90         685           0     1  0.610916\n",
      "91         686           0     1  0.649384\n",
      "92         693           1     1  0.636275\n",
      "93         707           1     1  0.582022\n",
      "94         708           1     1  0.650653\n",
      "95         715           1     1  0.632646\n",
      "96         718           1     1  0.587752\n",
      "97         723           0     1  0.638922\n",
      "98         728           0     1  0.645183\n",
      "99         730           0     1  0.667848\n",
      "100        737           1     1  0.634022\n",
      "101        746           1     1  0.631801\n",
      "102        760           1     1  0.592901\n",
      "103        767           0     1  0.625358\n",
      "104        768           1     1  0.603603\n",
      "105        775           1     1  0.605921\n",
      "106        780           0     0  0.392044\n",
      "107        788           0     0  0.441760\n",
      "108        793           1     0  0.472938\n",
      "109        805           0     0  0.435450\n",
      "110        806           0     0  0.485521\n",
      "111        809           0     0  0.454446\n",
      "112        819           1     0  0.410203\n",
      "113       1000           1     1  0.532212\n",
      "114       1001           1     1  0.519840\n",
      "115       1004           0     1  0.546041\n",
      "116       1007           1     1  0.570592\n",
      "117     100094           0     0  0.239777\n",
      "118     100117           0     0  0.421596\n",
      "119     100121           0     0  0.363522\n",
      "120     100133           0     0  0.398040\n",
      "121     100134           0     1  0.883194\n",
      "122     100135           1     0  0.423321\n",
      "123     100146           0     0  0.352302\n",
      "124     100150           0     0  0.462925\n",
      "125     100183           1     0  0.485536\n",
      "126     100267           0     0  0.427823\n",
      "127     100267           0     0  0.427823\n",
      "128     100312           0     0  0.392558\n",
      "129     100350           1     0  0.398185\n",
      "130     100351           1     0  0.440769\n",
      "131     100352           0     0  0.404875\n",
      "132     100362           0     0  0.453521\n",
      "133     100376           0     0  0.432062\n",
      "134     100378           1     0  0.384222\n",
      "135     100379           0     0  0.433273\n",
      "136     100380           1     0  0.392789\n",
      "137     100384           0     0  0.457543\n",
      "138     100385           0     0  0.307496\n",
      "139     100392           1     1  0.525487\n",
      "140     100402           0     1  0.513232\n",
      "141     100405           0     0  0.417943\n",
      "142     100416           0     0  0.418203\n",
      "143     100430           1     0  0.486077\n",
      "144     100431           0     0  0.419369\n",
      "145     100436           0     0  0.386961\n",
      "146     100439           1     1  0.533090\n",
      "147     100442           1     0  0.442733\n",
      "148     100445           1     1  0.505710\n",
      "149     100460           0     0  0.416406\n",
      "150     100461           1     0  0.404722\n",
      "151     100462           0     0  0.484708\n",
      "152     100469           1     0  0.469645\n",
      "153     100475           0     0  0.442883\n",
      "154     100481           1     0  0.417349\n",
      "Prediction AUC: 0.6353\n",
      "Prediction Accuracy: 0.5806\n",
      "Prediction Specificity: 0.4125\n",
      "Prediction Sensitivity: 0.7600\n",
      "Prediction Precision: 0.5481\n",
      "the score of the fold number 3 and the type T1wCE: 0.6353333333333333\n",
      "  model       AUC       acc    spec  sens      prec\n",
      "0     3  0.635333  0.580645  0.4125  0.76  0.548077\n",
      "Caulculating the best scans for every case...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 156/156 [00:26<00:00,  5.90it/s]\n",
      "Fold 4:\n",
      "     BraTS21ID  MGMT_value  pred      prob\n",
      "0            9           0     1  0.589923\n",
      "1           14           1     1  0.571315\n",
      "2           18           0     1  0.550912\n",
      "3           35           1     1  0.573952\n",
      "4           44           0     1  0.555019\n",
      "5           52           1     1  0.542741\n",
      "6           53           0     1  0.569942\n",
      "7           60           1     1  0.538978\n",
      "8           81           0     1  0.561922\n",
      "9           95           0     1  0.566171\n",
      "10          98           1     1  0.590725\n",
      "11         100           1     0  0.494581\n",
      "12         104           0     0  0.467806\n",
      "13         120           1     0  0.491364\n",
      "14         128           1     0  0.442246\n",
      "15         134           1     0  0.346730\n",
      "16         143           1     1  0.507588\n",
      "17         158           0     1  0.546274\n",
      "18         160           1     1  0.548981\n",
      "19         177           1     1  0.527734\n",
      "20         184           0     1  0.540645\n",
      "21         193           0     0  0.442888\n",
      "22         195           0     1  0.535461\n",
      "23         197           1     1  0.534323\n",
      "24         204           1     1  0.564521\n",
      "25         210           1     1  0.550051\n",
      "26         212           1     1  0.539014\n",
      "27         217           0     1  0.593812\n",
      "28         219           0     1  0.554500\n",
      "29         220           1     1  0.539765\n",
      "30         233           1     1  0.547775\n",
      "31         246           1     1  0.548506\n",
      "32         250           1     1  0.559854\n",
      "33         254           1     1  0.539273\n",
      "34         273           1     1  0.541827\n",
      "35         274           0     1  0.560546\n",
      "36         280           0     1  0.550812\n",
      "37         296           1     1  0.552784\n",
      "38         305           1     1  0.576391\n",
      "39         314           0     1  0.537915\n",
      "40         318           0     1  0.534830\n",
      "41         322           1     1  0.555832\n",
      "42         325           0     1  0.595857\n",
      "43         329           1     1  0.555417\n",
      "44         341           0     1  0.561329\n",
      "45         353           0     1  0.536858\n",
      "46         360           1     1  0.555363\n",
      "47         364           1     1  0.559848\n",
      "48         378           0     1  0.556892\n",
      "49         380           0     1  0.565643\n",
      "50         382           0     1  0.552745\n",
      "51         391           0     1  0.500183\n",
      "52         399           0     1  0.536529\n",
      "53         405           0     1  0.519786\n",
      "54         407           0     1  0.553636\n",
      "55         408           1     1  0.541610\n",
      "56         410           0     1  0.547153\n",
      "57         413           1     1  0.563947\n",
      "58         421           0     1  0.535140\n",
      "59         429           1     0  0.498957\n",
      "60         443           1     1  0.565736\n",
      "61         445           0     1  0.580951\n",
      "62         464           0     1  0.529159\n",
      "63         477           0     1  0.568513\n",
      "64         485           1     1  0.568524\n",
      "65         488           1     1  0.556107\n",
      "66         500           1     1  0.583008\n",
      "67         502           1     1  0.552783\n",
      "68         506           1     1  0.559471\n",
      "69         507           0     1  0.572215\n",
      "70         514           0     1  0.570638\n",
      "71         517           1     1  0.553552\n",
      "72         518           0     1  0.571346\n",
      "73         523           1     1  0.562396\n",
      "74         550           1     1  0.575038\n",
      "75         554           1     1  0.591118\n",
      "76         561           1     1  0.547985\n",
      "77         563           0     1  0.549027\n",
      "78         564           1     1  0.550708\n",
      "79         568           0     1  0.566418\n",
      "80         570           1     1  0.548766\n",
      "81         579           1     1  0.560672\n",
      "82         584           1     1  0.508474\n",
      "83         591           0     1  0.569092\n",
      "84         605           0     1  0.565184\n",
      "85         607           1     1  0.560475\n",
      "86         612           1     1  0.562937\n",
      "87         615           1     1  0.573030\n",
      "88         619           0     1  0.556808\n",
      "89         622           1     1  0.567469\n",
      "90         628           1     1  0.571884\n",
      "91         630           0     1  0.563754\n",
      "92         636           0     1  0.590091\n",
      "93         638           1     1  0.581986\n",
      "94         639           1     1  0.562261\n",
      "95         659           1     1  0.580656\n",
      "96         663           0     1  0.561781\n",
      "97         668           0     1  0.575171\n",
      "98         682           0     1  0.562143\n",
      "99         697           1     1  0.569353\n",
      "100        725           1     1  0.570480\n",
      "101        727           0     1  0.556374\n",
      "102        733           0     1  0.548822\n",
      "103        739           1     1  0.568511\n",
      "104        740           1     1  0.578054\n",
      "105        774           0     1  0.561522\n",
      "106        778           0     1  0.574970\n",
      "107        792           0     0  0.496467\n",
      "108        797           0     0  0.486770\n",
      "109        807           1     1  0.505769\n",
      "110        808           1     0  0.485783\n",
      "111        810           0     1  0.527111\n",
      "112        814           0     1  0.505182\n",
      "113        820           0     1  0.545158\n",
      "114        828           1     1  0.561442\n",
      "115        830           0     1  0.527577\n",
      "116        836           0     0  0.486747\n",
      "117     100034           0     0  0.483358\n",
      "118     100088           0     0  0.466374\n",
      "119     100093           0     0  0.445819\n",
      "120     100115           1     0  0.472718\n",
      "121     100122           0     0  0.432421\n",
      "122     100122           0     0  0.432421\n",
      "123     100124           1     0  0.430633\n",
      "124     100136           0     0  0.425496\n",
      "125     100140           0     0  0.419630\n",
      "126     100144           0     0  0.431380\n",
      "127     100149           0     0  0.491398\n",
      "128     100287           1     0  0.367339\n",
      "129     100290           0     0  0.449957\n",
      "130     100310           1     0  0.438811\n",
      "131     100314           0     0  0.475721\n",
      "132     100344           1     0  0.433363\n",
      "133     100358           1     0  0.405677\n",
      "134     100368           0     0  0.428574\n",
      "135     100371           1     0  0.424377\n",
      "136     100404           1     0  0.447368\n",
      "137     100406           1     0  0.425419\n",
      "138     100407           1     0  0.487651\n",
      "139     100412           1     0  0.440189\n",
      "140     100415           0     0  0.452163\n",
      "141     100421           0     0  0.409462\n",
      "142     100423           1     0  0.454970\n",
      "143     100425           1     0  0.429407\n",
      "144     100432           0     0  0.411769\n",
      "145     100438           1     0  0.445628\n",
      "146     100452           0     0  0.372890\n",
      "147     100454           0     0  0.428954\n",
      "148     100459           0     0  0.397227\n",
      "149     100474           1     0  0.396658\n",
      "150     100479           0     0  0.474084\n",
      "151     100480           0     0  0.456428\n",
      "152     100487           0     0  0.282686\n",
      "153     100488           1     0  0.381752\n",
      "154     100492           0     0  0.469734\n",
      "155     100500           0     0  0.359712\n",
      "Prediction AUC: 0.5473\n",
      "Prediction Accuracy: 0.5321\n",
      "Prediction Specificity: 0.3544\n",
      "Prediction Sensitivity: 0.7143\n",
      "Prediction Precision: 0.5189\n",
      "the score of the fold number 4 and the type T1wCE: 0.5472628637185598\n",
      "  model       AUC       acc     spec      sens      prec\n",
      "0     4  0.547263  0.532051  0.35443  0.714286  0.518868\n",
      "Prediction AUC: 0.5863\n",
      "Prediction Accuracy: 0.5688\n",
      "Prediction Specificity: 0.4822\n",
      "Prediction Sensitivity: 0.6596\n",
      "Prediction Precision: 0.5487\n",
      "Prediction AUC: 0.5863\n",
      "Prediction Accuracy: 0.5688\n",
      "Prediction Specificity: 0.4822\n",
      "Prediction Sensitivity: 0.6596\n",
      "Prediction Precision: 0.5487\n",
      "the final socre of the type T1wCE\n",
      "0.5863079166216654\n",
      "  model       AUC       acc      spec      sens      prec\n",
      "0   all  0.586308  0.568831  0.482234  0.659574  0.548673\n",
      "\n",
      "\n",
      "\n",
      "the final score is\n",
      "0.5863079166216654\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/validation.py --models_folder tunisiaai_AB --csv_file all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../rsnaresnet10/working/create_splits.py --in_csv_file train_labels.csv --out_csv_file train_f.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../rsnaresnet10/working/create_splits.py --in_csv_file upenn_train_labels.csv --out_csv_file train_n.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_T1wCE\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 413/413 [00:47<00:00,  8.61it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 114/114 [00:13<00:00,  8.30it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|███████████| 103/103 [00:56<00:00,  1.84it/s, batch_loss=0.696, loss=0.706]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|█████████████| 29/29 [00:07<00:00,  4.11it/s, batch_loss=0.517, loss=0.716]\n",
      "EPOCH 0/100: Validation average loss: 0.7164127189537575 + AUC SCORE = 0.4073959938366718 + AUC SCORE THRESH 0.0 = 0.5\n",
      "Saving the model...\n",
      "100%|███████████| 103/103 [00:54<00:00,  1.87it/s, batch_loss=0.686, loss=0.691]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|██████████████| 29/29 [00:06<00:00,  4.18it/s, batch_loss=0.513, loss=0.72]\n",
      "EPOCH 1/100: Validation average loss: 0.7198914310027813 + AUC SCORE = 0.3821263482280431 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|████████████| 103/103 [00:55<00:00,  1.87it/s, batch_loss=0.675, loss=0.68]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|█████████████| 29/29 [00:07<00:00,  4.08it/s, batch_loss=0.507, loss=0.723]\n",
      "EPOCH 2/100: Validation average loss: 0.7234024323266128 + AUC SCORE = 0.4049306625577812 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|█████████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.67, loss=0.67]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|█████████████| 29/29 [00:07<00:00,  3.94it/s, batch_loss=0.557, loss=0.727]\n",
      "EPOCH 3/100: Validation average loss: 0.727480294375584 + AUC SCORE = 0.4234206471494607 + AUC SCORE THRESH 0.4693877551020408 = 0.5186440677966102\n",
      "Saving the model...\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.87it/s, batch_loss=0.671, loss=0.658]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|█████████████| 29/29 [00:07<00:00,  4.04it/s, batch_loss=0.697, loss=0.747]\n",
      "EPOCH 4/100: Validation average loss: 0.747475149302647 + AUC SCORE = 0.43297380585516176 + AUC SCORE THRESH 0.3469387755102041 = 0.5072419106317412\n",
      "Saving the model...\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.644, loss=0.645]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|█████████████| 29/29 [00:07<00:00,  4.08it/s, batch_loss=0.924, loss=0.785]\n",
      "EPOCH 5/100: Validation average loss: 0.78463112999653 + AUC SCORE = 0.43235747303543914 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.601, loss=0.628]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|██████████████| 29/29 [00:07<00:00,  4.02it/s, batch_loss=1.13, loss=0.859]\n",
      "EPOCH 6/100: Validation average loss: 0.8585939592328565 + AUC SCORE = 0.4144838212634823 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.571, loss=0.602]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|██████████████| 29/29 [00:07<00:00,  4.09it/s, batch_loss=1.28, loss=0.929]\n",
      "EPOCH 7/100: Validation average loss: 0.9289548767024073 + AUC SCORE = 0.40061633281972264 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.539, loss=0.567]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|███████████████| 29/29 [00:06<00:00,  4.16it/s, batch_loss=1.53, loss=1.03]\n",
      "EPOCH 8/100: Validation average loss: 1.0333317805980813 + AUC SCORE = 0.41078582434514643 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.478, loss=0.529]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.07it/s, batch_loss=1.78, loss=1.13]\n",
      "EPOCH 9/100: Validation average loss: 1.1270692240575264 + AUC SCORE = 0.42742681047765796 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.404, loss=0.483]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 29/29 [00:07<00:00,  4.02it/s, batch_loss=0.974, loss=0.958]\n",
      "EPOCH 10/100: Validation average loss: 0.958069981172167 + AUC SCORE = 0.36425269645608627 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.347, loss=0.443]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.08it/s, batch_loss=1.13, loss=1.02]\n",
      "EPOCH 11/100: Validation average loss: 1.0183016483126015 + AUC SCORE = 0.3864406779661017 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.287, loss=0.408]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:06<00:00,  4.19it/s, batch_loss=1.14, loss=1.04]\n",
      "EPOCH 12/100: Validation average loss: 1.0358164885948444 + AUC SCORE = 0.40308166409861323 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.242, loss=0.375]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.13it/s, batch_loss=1.08, loss=1.03]\n",
      "EPOCH 13/100: Validation average loss: 1.0295716724519073 + AUC SCORE = 0.4151001540832049 + AUC SCORE THRESH 0.04081632653061224 = 0.5084745762711864\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.207, loss=0.344]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.04it/s, batch_loss=1.08, loss=1.04]\n",
      "EPOCH 14/100: Validation average loss: 1.0411062312537227 + AUC SCORE = 0.4184899845916795 + AUC SCORE THRESH 0.04081632653061224 = 0.5084745762711864\n",
      "100%|████████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.18, loss=0.316]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.05it/s, batch_loss=1.08, loss=1.05]\n",
      "EPOCH 15/100: Validation average loss: 1.0517143571171268 + AUC SCORE = 0.4206471494607088 + AUC SCORE THRESH 0.8775510204081632 = 0.5006163328197226\n",
      "100%|████████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.16, loss=0.291]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.05it/s, batch_loss=1.15, loss=1.09]\n",
      "EPOCH 16/100: Validation average loss: 1.0869325301770507 + AUC SCORE = 0.4283513097072419 + AUC SCORE THRESH 0.04081632653061224 = 0.5078582434514638\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.145, loss=0.269]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.00it/s, batch_loss=1.27, loss=1.14]\n",
      "EPOCH 17/100: Validation average loss: 1.1441490773496956 + AUC SCORE = 0.43420647149460706 + AUC SCORE THRESH 0.02040816326530612 = 0.5084745762711864\n",
      "Saving the model...\n",
      "100%|████████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.136, loss=0.25]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 29/29 [00:07<00:00,  4.00it/s, batch_loss=1.4, loss=1.22]\n",
      "EPOCH 18/100: Validation average loss: 1.221558310348412 + AUC SCORE = 0.4446841294298921 + AUC SCORE THRESH 0.02040816326530612 = 0.5169491525423728\n",
      "Saving the model...\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.122, loss=0.234]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:06<00:00,  4.22it/s, batch_loss=1.44, loss=1.28]\n",
      "EPOCH 19/100: Validation average loss: 1.2784315357948173 + AUC SCORE = 0.44745762711864406 + AUC SCORE THRESH 0.02040816326530612 = 0.5078582434514638\n",
      "Saving the model...\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.106, loss=0.219]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.12it/s, batch_loss=1.38, loss=1.28]\n",
      "EPOCH 20/100: Validation average loss: 1.2792422249913216 + AUC SCORE = 0.437904468412943 + AUC SCORE THRESH 0.9387755102040816 = 0.5006163328197226\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0925, loss=0.215]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|██████████████| 29/29 [00:07<00:00,  4.08it/s, batch_loss=0.976, loss=1.15]\n",
      "EPOCH 21/100: Validation average loss: 1.145640733940848 + AUC SCORE = 0.4206471494607088 + AUC SCORE THRESH 0.9591836734693877 = 0.5006163328197226\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.114, loss=0.231]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████| 29/29 [00:07<00:00,  4.07it/s, batch_loss=1.02, loss=1.1]\n",
      "EPOCH 22/100: Validation average loss: 1.0981528322244514 + AUC SCORE = 0.4446841294298922 + AUC SCORE THRESH 0.3061224489795918 = 0.510477657935285\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.0901, loss=0.213]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 29/29 [00:07<00:00,  4.14it/s, batch_loss=2.4, loss=1.85]\n",
      "EPOCH 23/100: Validation average loss: 1.8514853403421825 + AUC SCORE = 0.5060092449922958 + AUC SCORE THRESH 0.02040816326530612 = 0.5548536209553159\n",
      "Saving the model...\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0905, loss=0.204]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.00it/s, batch_loss=2.17, loss=1.83]\n",
      "EPOCH 24/100: Validation average loss: 1.8273495189074813 + AUC SCORE = 0.4979969183359014 + AUC SCORE THRESH 0.02040816326530612 = 0.5457627118644068\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.115, loss=0.188]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.05it/s, batch_loss=1.85, loss=1.58]\n",
      "EPOCH 25/100: Validation average loss: 1.5753335839715497 + AUC SCORE = 0.4591679506933744 + AUC SCORE THRESH 0.9387755102040816 = 0.509090909090909\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.103, loss=0.177]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.00it/s, batch_loss=1.75, loss=1.45]\n",
      "EPOCH 26/100: Validation average loss: 1.454237543303391 + AUC SCORE = 0.45608628659476114 + AUC SCORE THRESH 0.02040816326530612 = 0.5308166409861326\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0862, loss=0.17]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.03it/s, batch_loss=1.91, loss=1.53]\n",
      "EPOCH 27/100: Validation average loss: 1.5301748257258843 + AUC SCORE = 0.46070878274268107 + AUC SCORE THRESH 0.8979591836734693 = 0.509090909090909\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0898, loss=0.163]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.07it/s, batch_loss=1.92, loss=1.55]\n",
      "EPOCH 28/100: Validation average loss: 1.5526603068514118 + AUC SCORE = 0.4533127889060093 + AUC SCORE THRESH 0.02040816326530612 = 0.5186440677966102\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0903, loss=0.155]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  3.98it/s, batch_loss=1.81, loss=1.48]\n",
      "EPOCH 29/100: Validation average loss: 1.4791235548668895 + AUC SCORE = 0.44468412942989216 + AUC SCORE THRESH 0.8775510204081632 = 0.509090909090909\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0729, loss=0.148]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.14it/s, batch_loss=1.86, loss=1.43]\n",
      "EPOCH 30/100: Validation average loss: 1.4309714067855785 + AUC SCORE = 0.4440677966101695 + AUC SCORE THRESH 0.02040816326530612 = 0.5126348228043144\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.059, loss=0.141]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.00it/s, batch_loss=1.96, loss=1.45]\n",
      "EPOCH 31/100: Validation average loss: 1.451528213918209 + AUC SCORE = 0.44838212634822805 + AUC SCORE THRESH 0.02040816326530612 = 0.5041602465331279\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0494, loss=0.136]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.10it/s, batch_loss=2.04, loss=1.51]\n",
      "EPOCH 32/100: Validation average loss: 1.514545297828214 + AUC SCORE = 0.4446841294298922 + AUC SCORE THRESH 0.5918367346938775 = 0.5097072419106318\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0449, loss=0.131]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.07it/s, batch_loss=2.12, loss=1.58]\n",
      "EPOCH 33/100: Validation average loss: 1.5822477844254723 + AUC SCORE = 0.4453004622496148 + AUC SCORE THRESH 0.02040816326530612 = 0.5277349768875194\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0411, loss=0.126]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.02it/s, batch_loss=2.44, loss=1.76]\n",
      "EPOCH 34/100: Validation average loss: 1.7571313053626438 + AUC SCORE = 0.4591679506933744 + AUC SCORE THRESH 0.02040816326530612 = 0.5143297380585516\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0361, loss=0.123]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.00it/s, batch_loss=2.66, loss=1.92]\n",
      "EPOCH 35/100: Validation average loss: 1.9198890437275684 + AUC SCORE = 0.46656394453004624 + AUC SCORE THRESH 0.4693877551020408 = 0.5006163328197226\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0322, loss=0.121]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.00it/s, batch_loss=2.46, loss=1.95]\n",
      "EPOCH 36/100: Validation average loss: 1.9454803606495261 + AUC SCORE = 0.44961479198767335 + AUC SCORE THRESH 0.44897959183673464 = 0.5006163328197226\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.032, loss=0.118]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.08it/s, batch_loss=2.86, loss=2.07]\n",
      "EPOCH 37/100: Validation average loss: 2.0721111088220416 + AUC SCORE = 0.46933744221879814 + AUC SCORE THRESH 0.3469387755102041 = 0.5006163328197226\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0612, loss=0.125]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.05it/s, batch_loss=2.63, loss=1.95]\n",
      "EPOCH 38/100: Validation average loss: 1.9524126330326343 + AUC SCORE = 0.4582434514637905 + AUC SCORE THRESH 0.3061224489795918 = 0.5103235747303543\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0292, loss=0.121]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.11it/s, batch_loss=2.02, loss=1.81]\n",
      "EPOCH 39/100: Validation average loss: 1.8128311572403744 + AUC SCORE = 0.42342064714946065 + AUC SCORE THRESH 0.3877551020408163 = 0.5024653312788906\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.0521, loss=0.132]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.08it/s, batch_loss=1.26, loss=1.09]\n",
      "EPOCH 40/100: Validation average loss: 1.0949769826798603 + AUC SCORE = 0.3895223420647149 + AUC SCORE THRESH 0.5918367346938775 = 0.5328197226502311\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0284, loss=0.113]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.06it/s, batch_loss=1.44, loss=1.25]\n",
      "EPOCH 41/100: Validation average loss: 1.2465769725626912 + AUC SCORE = 0.39537750385208015 + AUC SCORE THRESH 0.4081632653061224 = 0.5158705701078583\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0331, loss=0.118]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.03it/s, batch_loss=1.34, loss=1.05]\n",
      "EPOCH 42/100: Validation average loss: 1.0534143689377555 + AUC SCORE = 0.3879815100154083 + AUC SCORE THRESH 0.7551020408163265 = 0.5231124807395994\n",
      "100%|███████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.113, loss=0.109]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.01it/s, batch_loss=1.75, loss=1.16]\n",
      "EPOCH 43/100: Validation average loss: 1.1616666984969173 + AUC SCORE = 0.4184899845916795 + AUC SCORE THRESH 0.7551020408163265 = 0.5097072419106318\n",
      "100%|████████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0437, loss=0.1]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.01it/s, batch_loss=1.92, loss=1.17]\n",
      "EPOCH 44/100: Validation average loss: 1.1687129895234931 + AUC SCORE = 0.4261941448382126 + AUC SCORE THRESH 0.061224489795918366 = 0.5380585516178736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0581, loss=0.104]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.09it/s, batch_loss=2.06, loss=1.45]\n",
      "EPOCH 45/100: Validation average loss: 1.448846079152206 + AUC SCORE = 0.43728813559322033 + AUC SCORE THRESH 0.02040816326530612 = 0.5035439137134052\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0253, loss=0.0864]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.11it/s, batch_loss=1.88, loss=1.27]\n",
      "EPOCH 46/100: Validation average loss: 1.2706241962210885 + AUC SCORE = 0.4228043143297381 + AUC SCORE THRESH 0.36734693877551017 = 0.5134052388289677\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.026, loss=0.0912]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  3.95it/s, batch_loss=2.06, loss=1.46]\n",
      "EPOCH 47/100: Validation average loss: 1.4636258444395558 + AUC SCORE = 0.4098613251155624 + AUC SCORE THRESH 0.6938775510204082 = 0.5006163328197226\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0321, loss=0.0909]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.01it/s, batch_loss=1.51, loss=1.13]\n",
      "EPOCH 48/100: Validation average loss: 1.1258060038089752 + AUC SCORE = 0.39291217257318956 + AUC SCORE THRESH 0.6530612244897959 = 0.5134052388289677\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0257, loss=0.0772]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 29/29 [00:07<00:00,  4.08it/s, batch_loss=1.97, loss=1.3]\n",
      "EPOCH 49/100: Validation average loss: 1.3042492833116959 + AUC SCORE = 0.3963020030816642 + AUC SCORE THRESH 0.6938775510204082 = 0.5012326656394452\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0273, loss=0.0731]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 29/29 [00:07<00:00,  4.02it/s, batch_loss=1.7, loss=1.11]\n",
      "EPOCH 50/100: Validation average loss: 1.1071425326939286 + AUC SCORE = 0.39784283513097074 + AUC SCORE THRESH 0.5918367346938775 = 0.5134052388289677\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.029, loss=0.0701]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.13it/s, batch_loss=2.22, loss=1.43]\n",
      "EPOCH 51/100: Validation average loss: 1.428575240846338 + AUC SCORE = 0.41294298921417566 + AUC SCORE THRESH 0.02040816326530612 = 0.5126348228043144\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.86it/s, batch_loss=0.0208, loss=0.0665]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.01it/s, batch_loss=1.91, loss=1.23]\n",
      "EPOCH 52/100: Validation average loss: 1.2269835297403664 + AUC SCORE = 0.39999999999999997 + AUC SCORE THRESH 0.8775510204081632 = 0.5097072419106318\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0205, loss=0.0618]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  3.98it/s, batch_loss=2.02, loss=1.42]\n",
      "EPOCH 53/100: Validation average loss: 1.4183536803927914 + AUC SCORE = 0.40585516178736514 + AUC SCORE THRESH 0.6530612244897959 = 0.5012326656394452\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0191, loss=0.0578]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.11it/s, batch_loss=1.96, loss=1.25]\n",
      "EPOCH 54/100: Validation average loss: 1.2459946839973843 + AUC SCORE = 0.3904468412942989 + AUC SCORE THRESH 0.6326530612244897 = 0.5030816640986132\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0203, loss=0.0543]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|██████████████████| 29/29 [00:07<00:00,  4.05it/s, batch_loss=2, loss=1.29]\n",
      "EPOCH 55/100: Validation average loss: 1.2932505453455037 + AUC SCORE = 0.39876733436055467 + AUC SCORE THRESH 0.7346938775510203 = 0.5012326656394452\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0179, loss=0.0515]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.02it/s, batch_loss=2.09, loss=1.34]\n",
      "EPOCH 56/100: Validation average loss: 1.339900137535457 + AUC SCORE = 0.39938366718027735 + AUC SCORE THRESH 0.7346938775510203 = 0.5012326656394452\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.84it/s, batch_loss=0.0182, loss=0.0489]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  3.94it/s, batch_loss=1.97, loss=1.32]\n",
      "EPOCH 57/100: Validation average loss: 1.3170674458659928 + AUC SCORE = 0.398151001540832 + AUC SCORE THRESH 0.7551020408163265 = 0.5012326656394452\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0199, loss=0.0476]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  3.95it/s, batch_loss=2.14, loss=1.48]\n",
      "EPOCH 58/100: Validation average loss: 1.4800284283942189 + AUC SCORE = 0.4033898305084746 + AUC SCORE THRESH 0.9183673469387754 = 0.5006163328197226\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.84it/s, batch_loss=0.0204, loss=0.0486]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  3.94it/s, batch_loss=1.74, loss=1.12]\n",
      "EPOCH 59/100: Validation average loss: 1.1233283723222798 + AUC SCORE = 0.3956856702619414 + AUC SCORE THRESH 0.9591836734693877 = 0.5097072419106318\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0306, loss=0.048]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.04it/s, batch_loss=2.29, loss=1.58]\n",
      "EPOCH 60/100: Validation average loss: 1.5849190313240578 + AUC SCORE = 0.41664098613251155 + AUC SCORE THRESH 0.8979591836734693 = 0.5006163328197226\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0201, loss=0.0469]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.11it/s, batch_loss=1.94, loss=1.16]\n",
      "EPOCH 61/100: Validation average loss: 1.1600531447550346 + AUC SCORE = 0.39815100154083205 + AUC SCORE THRESH 0.8571428571428571 = 0.5103235747303543\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0324, loss=0.0447]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████████| 29/29 [00:06<00:00,  4.22it/s, batch_loss=2.3, loss=1.6]\n",
      "EPOCH 62/100: Validation average loss: 1.6032443008032338 + AUC SCORE = 0.4184899845916795 + AUC SCORE THRESH 0.836734693877551 = 0.5006163328197226\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0178, loss=0.0408]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  3.99it/s, batch_loss=2.14, loss=1.25]\n",
      "EPOCH 63/100: Validation average loss: 1.2463163748897355 + AUC SCORE = 0.3929121725731895 + AUC SCORE THRESH 0.7755102040816326 = 0.5018489984591679\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0188, loss=0.035]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.01it/s, batch_loss=2.53, loss=1.68]\n",
      "EPOCH 64/100: Validation average loss: 1.6817760691046715 + AUC SCORE = 0.4141756548536209 + AUC SCORE THRESH 0.9183673469387754 = 0.5006163328197226\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0136, loss=0.0334]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.05it/s, batch_loss=2.17, loss=1.34]\n",
      "EPOCH 65/100: Validation average loss: 1.34240926904925 + AUC SCORE = 0.39599383667180277 + AUC SCORE THRESH 0.3061224489795918 = 0.5080123266563944\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0164, loss=0.0309]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:06<00:00,  4.16it/s, batch_loss=2.44, loss=1.63]\n",
      "EPOCH 66/100: Validation average loss: 1.6259790006382713 + AUC SCORE = 0.41047765793528507 + AUC SCORE THRESH 0.8571428571428571 = 0.5006163328197226\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0121, loss=0.0289]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 29/29 [00:07<00:00,  4.06it/s, batch_loss=2.2, loss=1.37]\n",
      "EPOCH 67/100: Validation average loss: 1.3671363129697998 + AUC SCORE = 0.39784283513097074 + AUC SCORE THRESH 0.836734693877551 = 0.5012326656394452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0129, loss=0.0273]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.02it/s, batch_loss=2.36, loss=1.51]\n",
      "EPOCH 68/100: Validation average loss: 1.510220160515144 + AUC SCORE = 0.40739599383667185 + AUC SCORE THRESH 0.7551020408163265 = 0.5012326656394452\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0132, loss=0.0259]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.02it/s, batch_loss=2.36, loss=1.51]\n",
      "EPOCH 69/100: Validation average loss: 1.5054346813723958 + AUC SCORE = 0.40431432973805853 + AUC SCORE THRESH 0.7755102040816326 = 0.5012326656394452\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0112, loss=0.0244]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.00it/s, batch_loss=2.33, loss=1.44]\n",
      "EPOCH 70/100: Validation average loss: 1.4445842242446438 + AUC SCORE = 0.40092449922958395 + AUC SCORE THRESH 0.8163265306122448 = 0.5012326656394452\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0124, loss=0.0232]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.06it/s, batch_loss=2.41, loss=1.55]\n",
      "EPOCH 71/100: Validation average loss: 1.5450006147910809 + AUC SCORE = 0.4067796610169492 + AUC SCORE THRESH 0.7755102040816326 = 0.5012326656394452\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0102, loss=0.022]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.07it/s, batch_loss=2.33, loss=1.44]\n",
      "EPOCH 72/100: Validation average loss: 1.4423152444691494 + AUC SCORE = 0.40061633281972264 + AUC SCORE THRESH 0.836734693877551 = 0.5012326656394452\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0117, loss=0.021]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.01it/s, batch_loss=2.45, loss=1.57]\n",
      "EPOCH 73/100: Validation average loss: 1.5746389049394378 + AUC SCORE = 0.40616332819722645 + AUC SCORE THRESH 0.9591836734693877 = 0.5006163328197226\n",
      "100%|████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00931, loss=0.0199]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.03it/s, batch_loss=2.34, loss=1.44]\n",
      "EPOCH 74/100: Validation average loss: 1.4407514724238166 + AUC SCORE = 0.4012326656394453 + AUC SCORE THRESH 0.836734693877551 = 0.5012326656394452\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.84it/s, batch_loss=0.0109, loss=0.0191]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 29/29 [00:07<00:00,  4.12it/s, batch_loss=2.49, loss=1.6]\n",
      "EPOCH 75/100: Validation average loss: 1.5961509390637791 + AUC SCORE = 0.4086286594761171 + AUC SCORE THRESH 0.9591836734693877 = 0.5006163328197226\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00854, loss=0.018]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:06<00:00,  4.22it/s, batch_loss=2.37, loss=1.46]\n",
      "EPOCH 76/100: Validation average loss: 1.4563510782759765 + AUC SCORE = 0.40092449922958395 + AUC SCORE THRESH 0.836734693877551 = 0.5012326656394452\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0101, loss=0.0173]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 29/29 [00:07<00:00,  3.92it/s, batch_loss=2.5, loss=1.61]\n",
      "EPOCH 77/100: Validation average loss: 1.6073585884838268 + AUC SCORE = 0.4089368258859784 + AUC SCORE THRESH 0.9591836734693877 = 0.5006163328197226\n",
      "100%|████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00781, loss=0.0163]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.01it/s, batch_loss=2.38, loss=1.46]\n",
      "EPOCH 78/100: Validation average loss: 1.4582300648607056 + AUC SCORE = 0.4027734976887519 + AUC SCORE THRESH 0.8571428571428571 = 0.5012326656394452\n",
      "100%|████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00947, loss=0.0157]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:06<00:00,  4.18it/s, batch_loss=2.54, loss=1.63]\n",
      "EPOCH 79/100: Validation average loss: 1.6341767067025448 + AUC SCORE = 0.410477657935285 + AUC SCORE THRESH 0.9795918367346939 = 0.5006163328197226\n",
      "100%|████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00716, loss=0.0147]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 29/29 [00:07<00:00,  4.05it/s, batch_loss=2.4, loss=1.47]\n",
      "EPOCH 80/100: Validation average loss: 1.4677860870443542 + AUC SCORE = 0.40431432973805853 + AUC SCORE THRESH 0.8571428571428571 = 0.5012326656394452\n",
      "100%|████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00925, loss=0.0142]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  3.99it/s, batch_loss=2.58, loss=1.67]\n",
      "EPOCH 81/100: Validation average loss: 1.6702893520223683 + AUC SCORE = 0.41540832049306625 + AUC SCORE THRESH 0.9795918367346939 = 0.5006163328197226\n",
      "100%|████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00665, loss=0.0134]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 29/29 [00:07<00:00,  4.07it/s, batch_loss=2.4, loss=1.46]\n",
      "EPOCH 82/100: Validation average loss: 1.4646819978952408 + AUC SCORE = 0.4064714946070878 + AUC SCORE THRESH 0.32653061224489793 = 0.5073959938366718\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00962, loss=0.013]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.05it/s, batch_loss=2.63, loss=1.72]\n",
      "EPOCH 83/100: Validation average loss: 1.7242855956328327 + AUC SCORE = 0.41602465331278893 + AUC SCORE THRESH 0.9795918367346939 = 0.5006163328197226\n",
      "100%|████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00649, loss=0.0122]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 29/29 [00:07<00:00,  4.00it/s, batch_loss=2.4, loss=1.45]\n",
      "EPOCH 84/100: Validation average loss: 1.446720988072198 + AUC SCORE = 0.40308166409861323 + AUC SCORE THRESH 0.3469387755102041 = 0.5158705701078583\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0118, loss=0.0121]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.11it/s, batch_loss=2.69, loss=1.81]\n",
      "EPOCH 85/100: Validation average loss: 1.8133393043074115 + AUC SCORE = 0.41848998459167946 + AUC SCORE THRESH 0.9795918367346939 = 0.5006163328197226\n",
      "100%|████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00775, loss=0.0114]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 29/29 [00:07<00:00,  4.13it/s, batch_loss=2.34, loss=1.4]\n",
      "EPOCH 86/100: Validation average loss: 1.3962167116074726 + AUC SCORE = 0.40832049306625573 + AUC SCORE THRESH 0.32653061224489793 = 0.5249614791987673\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0205, loss=0.0118]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.11it/s, batch_loss=2.62, loss=1.86]\n",
      "EPOCH 87/100: Validation average loss: 1.8591402503180092 + AUC SCORE = 0.4200308166409862 + AUC SCORE THRESH 0.9591836734693877 = 0.5006163328197226\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0232, loss=0.0134]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.02it/s, batch_loss=2.26, loss=1.36]\n",
      "EPOCH 88/100: Validation average loss: 1.358714613935043 + AUC SCORE = 0.40770416024653316 + AUC SCORE THRESH 0.4693877551020408 = 0.5249614791987673\n",
      "100%|██████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0931, loss=0.204]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 29/29 [00:07<00:00,  4.05it/s, batch_loss=1.6, loss=1.11]\n",
      "EPOCH 89/100: Validation average loss: 1.1059842602959995 + AUC SCORE = 0.37904468412942993 + AUC SCORE THRESH 0.02040816326530612 = 0.5078582434514638\n",
      "100%|████████| 103/103 [00:55<00:00,  1.84it/s, batch_loss=0.00706, loss=0.0649]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.07it/s, batch_loss=3.19, loss=1.77]\n",
      "EPOCH 90/100: Validation average loss: 1.7748181119818112 + AUC SCORE = 0.4388289676425269 + AUC SCORE THRESH 0.24489795918367346 = 0.5049306625577812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 103/103 [00:55<00:00,  1.84it/s, batch_loss=0.0143, loss=0.0281]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.08it/s, batch_loss=2.71, loss=1.43]\n",
      "EPOCH 91/100: Validation average loss: 1.434608327417538 + AUC SCORE = 0.39506933744221884 + AUC SCORE THRESH 0.6326530612244897 = 0.5043143297380585\n",
      "100%|████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00821, loss=0.0167]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  3.96it/s, batch_loss=3.36, loss=1.94]\n",
      "EPOCH 92/100: Validation average loss: 1.9363023109477142 + AUC SCORE = 0.4046224961479199 + AUC SCORE THRESH 0.9795918367346939 = 0.5006163328197226\n",
      "100%|█████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00684, loss=0.013]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:06<00:00,  4.16it/s, batch_loss=2.64, loss=1.43]\n",
      "EPOCH 93/100: Validation average loss: 1.4252057013840511 + AUC SCORE = 0.38120184899845916 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00546, loss=0.0114]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.03it/s, batch_loss=2.65, loss=1.43]\n",
      "EPOCH 94/100: Validation average loss: 1.4341034370249715 + AUC SCORE = 0.3855161787365177 + AUC SCORE THRESH 0.0 = 0.5\n",
      "100%|████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00526, loss=0.0106]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.03it/s, batch_loss=2.59, loss=1.41]\n",
      "EPOCH 95/100: Validation average loss: 1.4133914267194683 + AUC SCORE = 0.3870570107858244 + AUC SCORE THRESH 0.5510204081632653 = 0.5055469953775038\n",
      "100%|████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00506, loss=0.0101]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.08it/s, batch_loss=2.54, loss=1.39]\n",
      "EPOCH 96/100: Validation average loss: 1.394863478582481 + AUC SCORE = 0.3889060092449923 + AUC SCORE THRESH 0.5510204081632653 = 0.5061633281972265\n",
      "100%|███████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00486, loss=0.00968]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.10it/s, batch_loss=2.49, loss=1.38]\n",
      "EPOCH 97/100: Validation average loss: 1.3805144352131877 + AUC SCORE = 0.39044684129429896 + AUC SCORE THRESH 0.5714285714285714 = 0.514637904468413\n",
      "100%|███████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.00467, loss=0.00929]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.02it/s, batch_loss=2.47, loss=1.37]\n",
      "EPOCH 98/100: Validation average loss: 1.3715215027332306 + AUC SCORE = 0.3907550077041602 + AUC SCORE THRESH 0.5918367346938775 = 0.514637904468413\n",
      "100%|████████| 103/103 [00:55<00:00,  1.85it/s, batch_loss=0.0045, loss=0.00893]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 29/29 [00:07<00:00,  4.11it/s, batch_loss=2.44, loss=1.36]\n",
      "EPOCH 99/100: Validation average loss: 1.3648499281241977 + AUC SCORE = 0.3913713405238829 + AUC SCORE THRESH 0.5918367346938775 = 0.514637904468413\n",
      "0.5060092449922958\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/custom_train.py --csv_file train.csv --type T1wCE --model_name resnet10_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_T1wCE\n",
      "Caulculating the best scans for every case...\n",
      "100%|█████████████████████████████████████████| 283/283 [00:36<00:00,  7.73it/s]\n",
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 77/77 [00:09<00:00,  7.99it/s]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|█████████████| 70/70 [00:39<00:00,  1.79it/s, batch_loss=0.395, loss=0.651]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|█████████████| 20/20 [00:04<00:00,  4.05it/s, batch_loss=0.411, loss=0.617]\n",
      "EPOCH 0/100: Validation average loss: 0.6168386295437813 + AUC SCORE = 0.5915384615384616 + AUC SCORE THRESH 0.673469387755102 = 0.5684615384615385\n",
      "Saving the model...\n",
      "100%|█████████████| 70/70 [00:37<00:00,  1.87it/s, batch_loss=0.405, loss=0.638]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.98it/s, batch_loss=0.388, loss=0.623]\n",
      "EPOCH 1/100: Validation average loss: 0.6230489850044251 + AUC SCORE = 0.5638461538461539 + AUC SCORE THRESH 0.6530612244897959 = 0.575\n",
      "100%|█████████████| 70/70 [00:37<00:00,  1.85it/s, batch_loss=0.408, loss=0.627]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.92it/s, batch_loss=0.389, loss=0.631]\n",
      "EPOCH 2/100: Validation average loss: 0.6312400057911873 + AUC SCORE = 0.5484615384615383 + AUC SCORE THRESH 0.6530612244897959 = 0.5676923076923077\n",
      "100%|█████████████| 70/70 [00:37<00:00,  1.85it/s, batch_loss=0.415, loss=0.616]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|█████████████| 20/20 [00:04<00:00,  4.11it/s, batch_loss=0.414, loss=0.646]\n",
      "EPOCH 3/100: Validation average loss: 0.6455674067139625 + AUC SCORE = 0.5207692307692308 + AUC SCORE THRESH 0.6938775510204082 = 0.5561538461538461\n",
      "100%|██████████████| 70/70 [00:37<00:00,  1.85it/s, batch_loss=0.42, loss=0.603]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|█████████████| 20/20 [00:04<00:00,  4.00it/s, batch_loss=0.498, loss=0.684]\n",
      "EPOCH 4/100: Validation average loss: 0.6836688786745071 + AUC SCORE = 0.5023076923076923 + AUC SCORE THRESH 0.673469387755102 = 0.5465384615384615\n",
      "100%|██████████████| 70/70 [00:37<00:00,  1.85it/s, batch_loss=0.42, loss=0.591]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.91it/s, batch_loss=0.866, loss=0.838]\n",
      "EPOCH 5/100: Validation average loss: 0.8381471425294876 + AUC SCORE = 0.4915384615384616 + AUC SCORE THRESH 0.3469387755102041 = 0.5365384615384616\n",
      "100%|█████████████| 70/70 [00:37<00:00,  1.85it/s, batch_loss=0.413, loss=0.576]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|██████████████| 20/20 [00:05<00:00,  3.99it/s, batch_loss=1.05, loss=0.905]\n",
      "EPOCH 6/100: Validation average loss: 0.9045290172100067 + AUC SCORE = 0.5076923076923077 + AUC SCORE THRESH 0.2857142857142857 = 0.5653846153846154\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.409, loss=0.557]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  3.93it/s, batch_loss=1.01, loss=0.87]\n",
      "EPOCH 7/100: Validation average loss: 0.8702648311853409 + AUC SCORE = 0.5269230769230769 + AUC SCORE THRESH 0.2857142857142857 = 0.5846153846153846\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.394, loss=0.537]\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  4.00it/s, batch_loss=0.878, loss=0.801]\n",
      "EPOCH 8/100: Validation average loss: 0.8007502913475036 + AUC SCORE = 0.543076923076923 + AUC SCORE THRESH 0.36734693877551017 = 0.595\n",
      "100%|██████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.39, loss=0.517]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  3.88it/s, batch_loss=1.42, loss=1.06]\n",
      "EPOCH 9/100: Validation average loss: 1.061313261091709 + AUC SCORE = 0.5515384615384615 + AUC SCORE THRESH 0.16326530612244897 = 0.5934615384615385\n",
      "100%|█████████████| 70/70 [00:37<00:00,  1.84it/s, batch_loss=0.409, loss=0.481]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:04<00:00,  4.10it/s, batch_loss=1.19, loss=1.08]\n",
      "EPOCH 10/100: Validation average loss: 1.0826608002185822 + AUC SCORE = 0.5423076923076923 + AUC SCORE THRESH 0.24489795918367346 = 0.5676923076923077\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.397, loss=0.456]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  4.00it/s, batch_loss=1.65, loss=1.33]\n",
      "EPOCH 11/100: Validation average loss: 1.3315190136432649 + AUC SCORE = 0.5538461538461539 + AUC SCORE THRESH 0.14285714285714285 = 0.5861538461538461\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.389, loss=0.435]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:04<00:00,  4.02it/s, batch_loss=1.79, loss=1.42]\n",
      "EPOCH 12/100: Validation average loss: 1.4182365268468857 + AUC SCORE = 0.5623076923076923 + AUC SCORE THRESH 0.12244897959183673 = 0.6053846153846154\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.381, loss=0.415]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:04<00:00,  4.07it/s, batch_loss=1.84, loss=1.45]\n",
      "EPOCH 13/100: Validation average loss: 1.4521914944052696 + AUC SCORE = 0.5661538461538462 + AUC SCORE THRESH 0.12244897959183673 = 0.5957692307692308\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.374, loss=0.395]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 20/20 [00:05<00:00,  3.97it/s, batch_loss=1.8, loss=1.44]\n",
      "EPOCH 14/100: Validation average loss: 1.4382255762815475 + AUC SCORE = 0.573076923076923 + AUC SCORE THRESH 0.12244897959183673 = 0.6053846153846154\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.367, loss=0.375]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 20/20 [00:05<00:00,  3.95it/s, batch_loss=1.72, loss=1.4]\n",
      "EPOCH 15/100: Validation average loss: 1.401352021098137 + AUC SCORE = 0.5753846153846154 + AUC SCORE THRESH 0.12244897959183673 = 0.595\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.358, loss=0.356]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:04<00:00,  4.07it/s, batch_loss=1.73, loss=1.39]\n",
      "EPOCH 16/100: Validation average loss: 1.3880717754364014 + AUC SCORE = 0.5784615384615386 + AUC SCORE THRESH 0.2857142857142857 = 0.5946153846153845\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.348, loss=0.338]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:04<00:00,  4.22it/s, batch_loss=1.71, loss=1.35]\n",
      "EPOCH 17/100: Validation average loss: 1.348061177134514 + AUC SCORE = 0.5807692307692309 + AUC SCORE THRESH 0.16326530612244897 = 0.5869230769230769\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.339, loss=0.321]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  3.95it/s, batch_loss=1.76, loss=1.34]\n",
      "EPOCH 18/100: Validation average loss: 1.340910303592682 + AUC SCORE = 0.5800000000000001 + AUC SCORE THRESH 0.16326530612244897 = 0.5965384615384616\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.331, loss=0.306]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  3.94it/s, batch_loss=1.87, loss=1.37]\n",
      "EPOCH 19/100: Validation average loss: 1.3667510434985162 + AUC SCORE = 0.5776923076923077 + AUC SCORE THRESH 0.16326530612244897 = 0.5965384615384616\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.323, loss=0.293]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  3.88it/s, batch_loss=2.01, loss=1.41]\n",
      "EPOCH 20/100: Validation average loss: 1.4098449885845183 + AUC SCORE = 0.576923076923077 + AUC SCORE THRESH 0.16326530612244897 = 0.6069230769230769\n",
      "100%|███████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.32, loss=0.28]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  3.93it/s, batch_loss=2.18, loss=1.48]\n",
      "EPOCH 21/100: Validation average loss: 1.4750233590602875 + AUC SCORE = 0.5700000000000001 + AUC SCORE THRESH 0.14285714285714285 = 0.6157692307692308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.311, loss=0.27]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 20/20 [00:05<00:00,  3.93it/s, batch_loss=2.4, loss=1.56]\n",
      "EPOCH 22/100: Validation average loss: 1.5563359260559082 + AUC SCORE = 0.5738461538461539 + AUC SCORE THRESH 0.12244897959183673 = 0.6157692307692308\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.324, loss=0.259]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  3.89it/s, batch_loss=2.54, loss=1.63]\n",
      "EPOCH 23/100: Validation average loss: 1.630346167087555 + AUC SCORE = 0.5661538461538462 + AUC SCORE THRESH 0.1020408163265306 = 0.5757692307692308\n",
      "100%|███████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.3, loss=0.252]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  3.95it/s, batch_loss=2.83, loss=1.76]\n",
      "EPOCH 24/100: Validation average loss: 1.7571789294481277 + AUC SCORE = 0.5769230769230769 + AUC SCORE THRESH 0.16326530612244897 = 0.5915384615384616\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.458, loss=0.245]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  3.98it/s, batch_loss=2.43, loss=1.69]\n",
      "EPOCH 25/100: Validation average loss: 1.6866945266723632 + AUC SCORE = 0.5507692307692308 + AUC SCORE THRESH 0.12244897959183673 = 0.5684615384615385\n",
      "100%|███████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.3, loss=0.248]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 20/20 [00:05<00:00,  3.92it/s, batch_loss=2.91, loss=1.8]\n",
      "EPOCH 26/100: Validation average loss: 1.8031065821647645 + AUC SCORE = 0.5815384615384616 + AUC SCORE THRESH 0.08163265306122448 = 0.6165384615384616\n",
      "100%|██████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.455, loss=0.24]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:04<00:00,  4.03it/s, batch_loss=2.61, loss=1.95]\n",
      "EPOCH 27/100: Validation average loss: 1.9508771508932115 + AUC SCORE = 0.5384615384615385 + AUC SCORE THRESH 0.12244897959183673 = 0.5611538461538461\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.258, loss=0.226]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  3.94it/s, batch_loss=2.37, loss=1.69]\n",
      "EPOCH 28/100: Validation average loss: 1.6937730759382248 + AUC SCORE = 0.5715384615384616 + AUC SCORE THRESH 0.16326530612244897 = 0.5915384615384616\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.268, loss=0.213]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  3.95it/s, batch_loss=2.42, loss=1.84]\n",
      "EPOCH 29/100: Validation average loss: 1.8437646865844726 + AUC SCORE = 0.5492307692307692 + AUC SCORE THRESH 0.08163265306122448 = 0.5869230769230769\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.242, loss=0.205]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 20/20 [00:05<00:00,  3.95it/s, batch_loss=2.4, loss=1.81]\n",
      "EPOCH 30/100: Validation average loss: 1.8142082124948502 + AUC SCORE = 0.5538461538461539 + AUC SCORE THRESH 0.1020408163265306 = 0.5892307692307692\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.237, loss=0.198]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:04<00:00,  4.07it/s, batch_loss=2.12, loss=1.74]\n",
      "EPOCH 31/100: Validation average loss: 1.7395549356937408 + AUC SCORE = 0.5330769230769231 + AUC SCORE THRESH 0.12244897959183673 = 0.57\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.232, loss=0.193]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  3.85it/s, batch_loss=1.73, loss=1.52]\n",
      "EPOCH 32/100: Validation average loss: 1.5150044918060304 + AUC SCORE = 0.5392307692307693 + AUC SCORE THRESH 0.16326530612244897 = 0.5588461538461539\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.223, loss=0.188]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  3.97it/s, batch_loss=1.31, loss=1.35]\n",
      "EPOCH 33/100: Validation average loss: 1.3450738489627838 + AUC SCORE = 0.5123076923076924 + AUC SCORE THRESH 0.32653061224489793 = 0.5330769230769231\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.227, loss=0.186]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|██████████████| 20/20 [00:05<00:00,  3.79it/s, batch_loss=0.862, loss=1.06]\n",
      "EPOCH 34/100: Validation average loss: 1.0636481940746307 + AUC SCORE = 0.5015384615384616 + AUC SCORE THRESH 0.5714285714285714 = 0.5561538461538461\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.228, loss=0.183]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:04<00:00,  4.02it/s, batch_loss=0.397, loss=0.914]\n",
      "EPOCH 35/100: Validation average loss: 0.9144785344600678 + AUC SCORE = 0.4707692307692307 + AUC SCORE THRESH 0.22448979591836732 = 0.5326923076923077\n",
      "100%|██████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.26, loss=0.186]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:04<00:00,  4.03it/s, batch_loss=0.329, loss=0.821]\n",
      "EPOCH 36/100: Validation average loss: 0.8205014437437057 + AUC SCORE = 0.47307692307692306 + AUC SCORE THRESH 0.7959183673469387 = 0.5642307692307692\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.267, loss=0.187]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.94it/s, batch_loss=0.113, loss=0.888]\n",
      "EPOCH 37/100: Validation average loss: 0.8877698093652725 + AUC SCORE = 0.4484615384615384 + AUC SCORE THRESH 0.16326530612244897 = 0.5303846153846153\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.312, loss=0.188]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.85it/s, batch_loss=0.143, loss=0.796]\n",
      "EPOCH 38/100: Validation average loss: 0.7955231837928295 + AUC SCORE = 0.47615384615384615 + AUC SCORE THRESH 0.44897959183673464 = 0.5711538461538461\n",
      "100%|██████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.22, loss=0.181]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.87it/s, batch_loss=0.109, loss=0.862]\n",
      "EPOCH 39/100: Validation average loss: 0.8616125147789717 + AUC SCORE = 0.4392307692307692 + AUC SCORE THRESH 0.3469387755102041 = 0.5503846153846154\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.202, loss=0.173]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████| 20/20 [00:05<00:00,  3.81it/s, batch_loss=0.0546, loss=0.917]\n",
      "EPOCH 40/100: Validation average loss: 0.9166037634015083 + AUC SCORE = 0.44076923076923075 + AUC SCORE THRESH 0.5102040816326531 = 0.5711538461538461\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.194, loss=0.166]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████| 20/20 [00:05<00:00,  3.93it/s, batch_loss=0.0849, loss=0.894]\n",
      "EPOCH 41/100: Validation average loss: 0.8941037472337484 + AUC SCORE = 0.4276923076923077 + AUC SCORE THRESH 0.4693877551020408 = 0.551923076923077\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.195, loss=0.162]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████| 20/20 [00:05<00:00,  4.00it/s, batch_loss=0.0709, loss=0.896]\n",
      "EPOCH 42/100: Validation average loss: 0.8957297820597887 + AUC SCORE = 0.43153846153846154 + AUC SCORE THRESH 0.42857142857142855 = 0.5615384615384615\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.191, loss=0.157]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████| 20/20 [00:05<00:00,  3.99it/s, batch_loss=0.0539, loss=0.948]\n",
      "EPOCH 43/100: Validation average loss: 0.948247161693871 + AUC SCORE = 0.413076923076923 + AUC SCORE THRESH 0.4897959183673469 = 0.5615384615384615\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.213, loss=0.157]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████| 20/20 [00:05<00:00,  3.86it/s, batch_loss=0.0638, loss=0.899]\n",
      "EPOCH 44/100: Validation average loss: 0.8988425724208355 + AUC SCORE = 0.43384615384615377 + AUC SCORE THRESH 0.4081632653061224 = 0.5615384615384615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.193, loss=0.152]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████| 20/20 [00:05<00:00,  3.63it/s, batch_loss=0.0472, loss=0.978]\n",
      "EPOCH 45/100: Validation average loss: 0.9779336847364902 + AUC SCORE = 0.41384615384615386 + AUC SCORE THRESH 0.36734693877551017 = 0.5503846153846154\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.279, loss=0.158]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████| 20/20 [00:05<00:00,  3.94it/s, batch_loss=0.0616, loss=0.873]\n",
      "EPOCH 46/100: Validation average loss: 0.8732766542583704 + AUC SCORE = 0.4692307692307692 + AUC SCORE THRESH 0.4081632653061224 = 0.551923076923077\n",
      "100%|██████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.177, loss=0.15]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.97it/s, batch_loss=0.0463, loss=0.98]\n",
      "EPOCH 47/100: Validation average loss: 0.9801586830988527 + AUC SCORE = 0.42692307692307696 + AUC SCORE THRESH 0.5510204081632653 = 0.5415384615384616\n",
      "100%|██████████████| 70/70 [00:38<00:00,  1.82it/s, batch_loss=0.234, loss=0.15]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.91it/s, batch_loss=0.0451, loss=0.91]\n",
      "EPOCH 48/100: Validation average loss: 0.9103322958573699 + AUC SCORE = 0.46769230769230763 + AUC SCORE THRESH 0.8163265306122448 = 0.5349999999999999\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.178, loss=0.139]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.95it/s, batch_loss=0.0377, loss=1.01]\n",
      "EPOCH 49/100: Validation average loss: 1.013985908217728 + AUC SCORE = 0.4307692307692308 + AUC SCORE THRESH 0.7551020408163265 = 0.5423076923076923\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.165, loss=0.139]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.78it/s, batch_loss=0.034, loss=0.995]\n",
      "EPOCH 50/100: Validation average loss: 0.9947552561759949 + AUC SCORE = 0.44999999999999996 + AUC SCORE THRESH 0.836734693877551 = 0.5542307692307692\n",
      "100%|██████████████| 70/70 [00:38<00:00,  1.82it/s, batch_loss=0.182, loss=0.13]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.95it/s, batch_loss=0.0346, loss=1.04]\n",
      "EPOCH 51/100: Validation average loss: 1.0437257319688797 + AUC SCORE = 0.42846153846153845 + AUC SCORE THRESH 0.6326530612244897 = 0.5415384615384616\n",
      "100%|██████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.157, loss=0.13]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████| 20/20 [00:05<00:00,  3.94it/s, batch_loss=0.0464, loss=0.965]\n",
      "EPOCH 52/100: Validation average loss: 0.9654412727802992 + AUC SCORE = 0.4461538461538461 + AUC SCORE THRESH 0.6326530612244897 = 0.5423076923076923\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.172, loss=0.122]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.85it/s, batch_loss=0.0337, loss=1.05]\n",
      "EPOCH 53/100: Validation average loss: 1.0465175814926624 + AUC SCORE = 0.423076923076923 + AUC SCORE THRESH 0.5918367346938775 = 0.5511538461538462\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.145, loss=0.122]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████████| 20/20 [00:05<00:00,  3.92it/s, batch_loss=0.0377, loss=1]\n",
      "EPOCH 54/100: Validation average loss: 1.0012241069227457 + AUC SCORE = 0.4361538461538461 + AUC SCORE THRESH 0.6326530612244897 = 0.5423076923076923\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.84it/s, batch_loss=0.167, loss=0.116]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.84it/s, batch_loss=0.0308, loss=1.07]\n",
      "EPOCH 55/100: Validation average loss: 1.0665853695943952 + AUC SCORE = 0.4161538461538462 + AUC SCORE THRESH 0.5102040816326531 = 0.5503846153846154\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.82it/s, batch_loss=0.146, loss=0.116]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████| 20/20 [00:05<00:00,  3.95it/s, batch_loss=0.0501, loss=0.963]\n",
      "EPOCH 56/100: Validation average loss: 0.9626434557139874 + AUC SCORE = 0.4376923076923077 + AUC SCORE THRESH 0.5714285714285714 = 0.5423076923076923\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.82it/s, batch_loss=0.164, loss=0.111]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.90it/s, batch_loss=0.0297, loss=1.08]\n",
      "EPOCH 57/100: Validation average loss: 1.075018984824419 + AUC SCORE = 0.4076923076923077 + AUC SCORE THRESH 0.4693877551020408 = 0.5503846153846154\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.137, loss=0.112]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████| 20/20 [00:04<00:00,  4.07it/s, batch_loss=0.0418, loss=0.991]\n",
      "EPOCH 58/100: Validation average loss: 0.9913905445486307 + AUC SCORE = 0.4361538461538461 + AUC SCORE THRESH 0.5918367346938775 = 0.5423076923076923\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.167, loss=0.107]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.91it/s, batch_loss=0.0319, loss=1.08]\n",
      "EPOCH 59/100: Validation average loss: 1.0842813346534967 + AUC SCORE = 0.4076923076923077 + AUC SCORE THRESH 0.6122448979591836 = 0.5415384615384616\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.159, loss=0.108]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████| 20/20 [00:05<00:00,  3.97it/s, batch_loss=0.0696, loss=0.929]\n",
      "EPOCH 60/100: Validation average loss: 0.9285508062690496 + AUC SCORE = 0.4523076923076923 + AUC SCORE THRESH 0.7755102040816326 = 0.5261538461538462\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.178, loss=0.104]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.88it/s, batch_loss=0.0321, loss=1.09]\n",
      "EPOCH 61/100: Validation average loss: 1.090233325213194 + AUC SCORE = 0.4138461538461538 + AUC SCORE THRESH 0.6530612244897959 = 0.5319230769230769\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.127, loss=0.104]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████| 20/20 [00:05<00:00,  3.95it/s, batch_loss=0.0504, loss=0.992]\n",
      "EPOCH 62/100: Validation average loss: 0.9923658095300197 + AUC SCORE = 0.44153846153846155 + AUC SCORE THRESH 0.5510204081632653 = 0.5319230769230769\n",
      "100%|████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.134, loss=0.0972]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:04<00:00,  4.01it/s, batch_loss=0.0633, loss=1.01]\n",
      "EPOCH 63/100: Validation average loss: 1.0055049363523723 + AUC SCORE = 0.4307692307692308 + AUC SCORE THRESH 0.7959183673469387 = 0.5246153846153847\n",
      "100%|████████████| 70/70 [00:38<00:00,  1.82it/s, batch_loss=0.142, loss=0.0959]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.94it/s, batch_loss=0.113, loss=0.896]\n",
      "EPOCH 64/100: Validation average loss: 0.8963178679347038 + AUC SCORE = 0.44461538461538463 + AUC SCORE THRESH 0.6530612244897959 = 0.5438461538461539\n",
      "100%|█████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.145, loss=0.094]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.99it/s, batch_loss=0.0326, loss=1.11]\n",
      "EPOCH 65/100: Validation average loss: 1.10821206625551 + AUC SCORE = 0.4176923076923077 + AUC SCORE THRESH 0.44897959183673464 = 0.5303846153846153\n",
      "100%|████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.143, loss=0.0941]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:04<00:00,  4.00it/s, batch_loss=0.0318, loss=1.13]\n",
      "EPOCH 66/100: Validation average loss: 1.1329295901581644 + AUC SCORE = 0.42846153846153845 + AUC SCORE THRESH 0.3061224489795918 = 0.5303846153846153\n",
      "100%|████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.107, loss=0.0877]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|██████████████| 20/20 [00:05<00:00,  3.97it/s, batch_loss=0.165, loss=0.94]\n",
      "EPOCH 67/100: Validation average loss: 0.9397876717150211 + AUC SCORE = 0.43153846153846154 + AUC SCORE THRESH 0.7346938775510203 = 0.5261538461538462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.245, loss=0.0936]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.93it/s, batch_loss=0.409, loss=0.833]\n",
      "EPOCH 68/100: Validation average loss: 0.833241181075573 + AUC SCORE = 0.46692307692307694 + AUC SCORE THRESH 0.4081632653061224 = 0.5542307692307692\n",
      "100%|████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.241, loss=0.0981]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.99it/s, batch_loss=0.0203, loss=1.23]\n",
      "EPOCH 69/100: Validation average loss: 1.2344615891575814 + AUC SCORE = 0.403076923076923 + AUC SCORE THRESH 0.36734693877551017 = 0.5303846153846153\n",
      "100%|████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.121, loss=0.0961]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|██████████████| 20/20 [00:05<00:00,  4.00it/s, batch_loss=0.092, loss=1.01]\n",
      "EPOCH 70/100: Validation average loss: 1.0111267019063235 + AUC SCORE = 0.4307692307692307 + AUC SCORE THRESH 0.7959183673469387 = 0.5261538461538462\n",
      "100%|████████████| 70/70 [00:38<00:00,  1.82it/s, batch_loss=0.179, loss=0.0896]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:04<00:00,  4.00it/s, batch_loss=0.469, loss=0.828]\n",
      "EPOCH 71/100: Validation average loss: 0.8283093303442002 + AUC SCORE = 0.44999999999999996 + AUC SCORE THRESH 0.2857142857142857 = 0.5423076923076923\n",
      "100%|████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.137, loss=0.0838]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:04<00:00,  4.05it/s, batch_loss=0.123, loss=0.898]\n",
      "EPOCH 72/100: Validation average loss: 0.8975408777594567 + AUC SCORE = 0.4376923076923076 + AUC SCORE THRESH 0.7755102040816326 = 0.5357692307692308\n",
      "100%|████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.113, loss=0.0766]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.87it/s, batch_loss=0.074, loss=0.995]\n",
      "EPOCH 73/100: Validation average loss: 0.9949824865907431 + AUC SCORE = 0.42999999999999994 + AUC SCORE THRESH 0.8571428571428571 = 0.5269230769230769\n",
      "100%|████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.123, loss=0.0729]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.94it/s, batch_loss=0.0672, loss=1.02]\n",
      "EPOCH 74/100: Validation average loss: 1.0222501687705516 + AUC SCORE = 0.4261538461538461 + AUC SCORE THRESH 0.8571428571428571 = 0.5269230769230769\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0883, loss=0.0686]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.86it/s, batch_loss=0.106, loss=0.953]\n",
      "EPOCH 75/100: Validation average loss: 0.9533428560942412 + AUC SCORE = 0.43 + AUC SCORE THRESH 0.26530612244897955 = 0.5311538461538461\n",
      "100%|████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.103, loss=0.0664]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.95it/s, batch_loss=0.161, loss=0.886]\n",
      "EPOCH 76/100: Validation average loss: 0.8864471308887005 + AUC SCORE = 0.44153846153846155 + AUC SCORE THRESH 0.4897959183673469 = 0.5230769230769231\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0881, loss=0.0647]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.83it/s, batch_loss=0.163, loss=0.894]\n",
      "EPOCH 77/100: Validation average loss: 0.8937445275485516 + AUC SCORE = 0.4423076923076923 + AUC SCORE THRESH 0.42857142857142855 = 0.5119230769230769\n",
      "100%|████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0814, loss=0.062]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.93it/s, batch_loss=0.153, loss=0.906]\n",
      "EPOCH 78/100: Validation average loss: 0.9063879817724227 + AUC SCORE = 0.43153846153846154 + AUC SCORE THRESH 0.7551020408163265 = 0.5165384615384616\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0887, loss=0.0599]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|████████████| 20/20 [00:05<00:00,  3.86it/s, batch_loss=0.0973, loss=0.982]\n",
      "EPOCH 79/100: Validation average loss: 0.9824392784386873 + AUC SCORE = 0.42000000000000004 + AUC SCORE THRESH 0.5510204081632653 = 0.5223076923076923\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0811, loss=0.0582]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|███████████████| 20/20 [00:05<00:00,  3.80it/s, batch_loss=0.216, loss=0.9]\n",
      "EPOCH 80/100: Validation average loss: 0.9004965782165527 + AUC SCORE = 0.43538461538461537 + AUC SCORE THRESH 0.4081632653061224 = 0.5230769230769231\n",
      "100%|████████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0751, loss=0.056]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|██████████████| 20/20 [00:05<00:00,  3.63it/s, batch_loss=0.156, loss=0.91]\n",
      "EPOCH 81/100: Validation average loss: 0.9099252477288247 + AUC SCORE = 0.42846153846153834 + AUC SCORE THRESH 0.5306122448979591 = 0.5230769230769231\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.82it/s, batch_loss=0.0722, loss=0.0544]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.93it/s, batch_loss=0.192, loss=0.893]\n",
      "EPOCH 82/100: Validation average loss: 0.8930638149380684 + AUC SCORE = 0.4346153846153846 + AUC SCORE THRESH 0.4081632653061224 = 0.5126923076923077\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0855, loss=0.0529]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.82it/s, batch_loss=0.112, loss=0.966]\n",
      "EPOCH 83/100: Validation average loss: 0.9655705325305461 + AUC SCORE = 0.4238461538461539 + AUC SCORE THRESH 0.3877551020408163 = 0.5215384615384616\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0739, loss=0.0522]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.79it/s, batch_loss=0.237, loss=0.909]\n",
      "EPOCH 84/100: Validation average loss: 0.9088605262339116 + AUC SCORE = 0.4346153846153846 + AUC SCORE THRESH 0.4081632653061224 = 0.5134615384615384\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0723, loss=0.0501]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.82it/s, batch_loss=0.158, loss=0.929]\n",
      "EPOCH 85/100: Validation average loss: 0.9289220631122589 + AUC SCORE = 0.4176923076923077 + AUC SCORE THRESH 0.2857142857142857 = 0.5311538461538461\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0798, loss=0.0496]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.85it/s, batch_loss=0.288, loss=0.883]\n",
      "EPOCH 86/100: Validation average loss: 0.8830250911414623 + AUC SCORE = 0.43846153846153846 + AUC SCORE THRESH 0.32653061224489793 = 0.5223076923076923\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0863, loss=0.0478]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:04<00:00,  4.05it/s, batch_loss=0.154, loss=0.918]\n",
      "EPOCH 87/100: Validation average loss: 0.9183679819107056 + AUC SCORE = 0.4207692307692308 + AUC SCORE THRESH 0.4897959183673469 = 0.5223076923076923\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0661, loss=0.0478]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|██████████████| 20/20 [00:05<00:00,  3.90it/s, batch_loss=0.206, loss=0.91]\n",
      "EPOCH 88/100: Validation average loss: 0.9102939397096634 + AUC SCORE = 0.4338461538461539 + AUC SCORE THRESH 0.2040816326530612 = 0.5311538461538461\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.82it/s, batch_loss=0.0672, loss=0.0443]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.97it/s, batch_loss=0.208, loss=0.902]\n",
      "EPOCH 89/100: Validation average loss: 0.9018497437238693 + AUC SCORE = 0.4292307692307693 + AUC SCORE THRESH 0.44897959183673464 = 0.5134615384615384\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.82it/s, batch_loss=0.0609, loss=0.0431]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.98it/s, batch_loss=0.186, loss=0.912]\n",
      "EPOCH 90/100: Validation average loss: 0.9116801381111145 + AUC SCORE = 0.42538461538461536 + AUC SCORE THRESH 0.3877551020408163 = 0.5223076923076923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0638, loss=0.0409]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.93it/s, batch_loss=0.159, loss=0.933]\n",
      "EPOCH 91/100: Validation average loss: 0.9325252287089825 + AUC SCORE = 0.42000000000000004 + AUC SCORE THRESH 0.4897959183673469 = 0.5134615384615384\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0528, loss=0.0397]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.89it/s, batch_loss=0.142, loss=0.953]\n",
      "EPOCH 92/100: Validation average loss: 0.9533443652093411 + AUC SCORE = 0.42846153846153845 + AUC SCORE THRESH 0.5714285714285714 = 0.5230769230769231\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0544, loss=0.0382]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.91it/s, batch_loss=0.267, loss=0.914]\n",
      "EPOCH 93/100: Validation average loss: 0.9136553540825844 + AUC SCORE = 0.42615384615384616 + AUC SCORE THRESH 0.32653061224489793 = 0.5126923076923077\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0501, loss=0.0372]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:04<00:00,  4.03it/s, batch_loss=0.149, loss=0.942]\n",
      "EPOCH 94/100: Validation average loss: 0.942014779895544 + AUC SCORE = 0.4176923076923077 + AUC SCORE THRESH 0.4693877551020408 = 0.5223076923076923\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0512, loss=0.0355]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.93it/s, batch_loss=0.212, loss=0.932]\n",
      "EPOCH 95/100: Validation average loss: 0.9319903440773487 + AUC SCORE = 0.4230769230769231 + AUC SCORE THRESH 0.36734693877551017 = 0.5126923076923077\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.82it/s, batch_loss=0.0482, loss=0.0339]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:04<00:00,  4.05it/s, batch_loss=0.168, loss=0.956]\n",
      "EPOCH 96/100: Validation average loss: 0.9559621982276439 + AUC SCORE = 0.4246153846153846 + AUC SCORE THRESH 0.18367346938775508 = 0.5207692307692308\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0509, loss=0.0328]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.97it/s, batch_loss=0.275, loss=0.924]\n",
      "EPOCH 97/100: Validation average loss: 0.9242663875222206 + AUC SCORE = 0.42615384615384616 + AUC SCORE THRESH 0.36734693877551017 = 0.5134615384615384\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0473, loss=0.0321]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.89it/s, batch_loss=0.167, loss=0.948]\n",
      "EPOCH 98/100: Validation average loss: 0.9481742449104786 + AUC SCORE = 0.4146153846153846 + AUC SCORE THRESH 0.4897959183673469 = 0.5230769230769231\n",
      "100%|███████████| 70/70 [00:38<00:00,  1.83it/s, batch_loss=0.0441, loss=0.0308]\n",
      "Adjusting learning rate of group 0 to 5.0000e-06.\n",
      "100%|█████████████| 20/20 [00:05<00:00,  3.94it/s, batch_loss=0.246, loss=0.936]\n",
      "EPOCH 99/100: Validation average loss: 0.9358913153409958 + AUC SCORE = 0.4292307692307693 + AUC SCORE THRESH 0.3877551020408163 = 0.5134615384615384\n",
      "0.5915384615384616\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/custom_train.py --csv_file train_f.csv --type T1wCE --model_name resnet10_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 58/58 [00:06<00:00,  9.02it/s]\n",
      "the final socre of the type T1wCE\n",
      "0.39761904761904765\n",
      "\n",
      "\n",
      "\n",
      "Prediction AUC: 0.3976\n",
      "Prediction Accuracy: 0.5000\n",
      "Prediction Specificity: 0.9333\n",
      "Prediction Sensitivity: 0.0357\n",
      "Prediction Precision: 0.3333\n",
      "        model       AUC  acc      spec      sens      prec\n",
      "0  resnet10_m  0.397619  0.5  0.933333  0.035714  0.333333\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/custom_test.py --csv_file train.csv --type T1wCE --model_name resnet10_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caulculating the best scans for every case...\n",
      "100%|███████████████████████████████████████████| 58/58 [00:06<00:00,  9.58it/s]\n",
      "the final socre of the type T1wCE\n",
      "0.525\n",
      "\n",
      "\n",
      "\n",
      "Prediction AUC: 0.5250\n",
      "Prediction Accuracy: 0.4828\n",
      "Prediction Specificity: 0.0000\n",
      "Prediction Sensitivity: 1.0000\n",
      "Prediction Precision: 0.4828\n",
      "        model    AUC       acc  spec  sens      prec\n",
      "0  resnet10_f  0.525  0.482759   0.0   1.0  0.482759\n"
     ]
    }
   ],
   "source": [
    "!python3 ../rsnaresnet10/working/custom_test.py --csv_file train.csv --type T1wCE --model_name resnet10_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros:\n",
      "00009 2\n",
      "00017 2\n",
      "00018 0\n",
      "00019 0\n",
      "00021 0\n",
      "00022 0\n",
      "00024 2\n",
      "00030 0\n",
      "00032 2\n",
      "00036 0\n",
      "00044 2\n",
      "00045 2\n",
      "00049 2\n",
      "00053 0\n",
      "00061 2\n",
      "00064 0\n",
      "00072 0\n",
      "00081 2\n",
      "00084 2\n",
      "00088 0\n",
      "00090 0\n",
      "00095 0\n",
      "00097 0\n",
      "00099 0\n",
      "00102 2\n",
      "00104 2\n",
      "00110 2\n",
      "00111 2\n",
      "00112 2\n",
      "00113 2\n",
      "00116 0\n",
      "00121 0\n",
      "00122 0\n",
      "00123 2\n",
      "00124 0\n",
      "00130 0\n",
      "00133 0\n",
      "00142 0\n",
      "00149 2\n",
      "00150 0\n",
      "00151 2\n",
      "00154 0\n",
      "00157 0\n",
      "00158 0\n",
      "00162 2\n",
      "00165 0\n",
      "00167 0\n",
      "00169 0\n",
      "00170 2\n",
      "00172 0\n",
      "00176 0\n",
      "00183 0\n",
      "00184 0\n",
      "00191 0\n",
      "00192 2\n",
      "00193 2\n",
      "00194 2\n",
      "00195 0\n",
      "00201 0\n",
      "00206 0\n",
      "00209 0\n",
      "00211 0\n",
      "00214 0\n",
      "00216 0\n",
      "00217 2\n",
      "00218 2\n",
      "00219 2\n",
      "00221 0\n",
      "00227 2\n",
      "00228 2\n",
      "00231 0\n",
      "00236 2\n",
      "00237 0\n",
      "00238 0\n",
      "00239 0\n",
      "00241 2\n",
      "00242 0\n",
      "00243 2\n",
      "00247 0\n",
      "00249 0\n",
      "00251 2\n",
      "00258 2\n",
      "00259 0\n",
      "00261 2\n",
      "00262 2\n",
      "00266 0\n",
      "00267 0\n",
      "00269 2\n",
      "00274 2\n",
      "00275 2\n",
      "00280 2\n",
      "00283 0\n",
      "00286 2\n",
      "00288 2\n",
      "00289 0\n",
      "00290 0\n",
      "00297 0\n",
      "00298 2\n",
      "00300 0\n",
      "00301 0\n",
      "00308 2\n",
      "00309 0\n",
      "00310 0\n",
      "00312 0\n",
      "00314 2\n",
      "00316 0\n",
      "00318 2\n",
      "00320 0\n",
      "00324 0\n",
      "00325 0\n",
      "00327 0\n",
      "00336 0\n",
      "00339 2\n",
      "00341 2\n",
      "00343 2\n",
      "00346 2\n",
      "00347 0\n",
      "00348 2\n",
      "00349 2\n",
      "00351 0\n",
      "00353 2\n",
      "00356 0\n",
      "00373 0\n",
      "00376 2\n",
      "00377 2\n",
      "00378 2\n",
      "00379 2\n",
      "00380 2\n",
      "00382 0\n",
      "00387 2\n",
      "00388 0\n",
      "00389 2\n",
      "00390 2\n",
      "00391 0\n",
      "00392 0\n",
      "00395 0\n",
      "00397 2\n",
      "00399 2\n",
      "00401 0\n",
      "00402 2\n",
      "00405 2\n",
      "00407 2\n",
      "00410 2\n",
      "00412 2\n",
      "00414 2\n",
      "00417 0\n",
      "00418 2\n",
      "00419 2\n",
      "00421 0\n",
      "00423 2\n",
      "00430 0\n",
      "00432 2\n",
      "00433 0\n",
      "00441 2\n",
      "00444 2\n",
      "00445 0\n",
      "00446 2\n",
      "00452 0\n",
      "00454 2\n",
      "00455 0\n",
      "00459 2\n",
      "00464 2\n",
      "00469 2\n",
      "00477 0\n",
      "00481 0\n",
      "00495 0\n",
      "00496 2\n",
      "00498 0\n",
      "00507 2\n",
      "00510 2\n",
      "00512 0\n",
      "00514 2\n",
      "00518 2\n",
      "00519 2\n",
      "00530 2\n",
      "00533 2\n",
      "00538 2\n",
      "00540 2\n",
      "00545 2\n",
      "00547 2\n",
      "00555 2\n",
      "00563 0\n",
      "00565 2\n",
      "00567 2\n",
      "00568 0\n",
      "00569 0\n",
      "00571 2\n",
      "00572 2\n",
      "00574 2\n",
      "00575 2\n",
      "00578 2\n",
      "00581 2\n",
      "00587 0\n",
      "00588 0\n",
      "00589 0\n",
      "00591 0\n",
      "00596 0\n",
      "00601 0\n",
      "00605 0\n",
      "00616 0\n",
      "00619 2\n",
      "00620 2\n",
      "00623 2\n",
      "00624 2\n",
      "00630 2\n",
      "00636 2\n",
      "00641 0\n",
      "00642 2\n",
      "00645 2\n",
      "00649 2\n",
      "00651 2\n",
      "00654 0\n",
      "00657 2\n",
      "00663 0\n",
      "00667 2\n",
      "00668 0\n",
      "00682 0\n",
      "00683 0\n",
      "00684 2\n",
      "00685 2\n",
      "00686 0\n",
      "00687 0\n",
      "00688 2\n",
      "00703 2\n",
      "00706 0\n",
      "00709 2\n",
      "00723 2\n",
      "00724 0\n",
      "00727 0\n",
      "00728 2\n",
      "00729 2\n",
      "00730 0\n",
      "00733 0\n",
      "00734 2\n",
      "00735 2\n",
      "00742 0\n",
      "00744 2\n",
      "00747 0\n",
      "00751 2\n",
      "00753 2\n",
      "00756 0\n",
      "00759 0\n",
      "00764 0\n",
      "00767 0\n",
      "00774 0\n",
      "00778 2\n",
      "00780 0\n",
      "00788 2\n",
      "00792 0\n",
      "00796 2\n",
      "00797 0\n",
      "00799 2\n",
      "00800 2\n",
      "00802 2\n",
      "00803 2\n",
      "00804 0\n",
      "00805 2\n",
      "00806 2\n",
      "00809 2\n",
      "00810 0\n",
      "00814 0\n",
      "00818 0\n",
      "00820 2\n",
      "00824 2\n",
      "00830 0\n",
      "00836 2\n",
      "00837 2\n",
      "00839 2\n",
      "01004 0\n",
      "01009 0\n",
      "Ones:\n",
      "00000 1\n",
      "00002 1\n",
      "00005 1\n",
      "00006 1\n",
      "00008 1\n",
      "00011 1\n",
      "00012 1\n",
      "00014 1\n",
      "00020 1\n",
      "00025 1\n",
      "00026 1\n",
      "00028 1\n",
      "00031 1\n",
      "00033 1\n",
      "00035 1\n",
      "00043 1\n",
      "00046 1\n",
      "00052 1\n",
      "00054 1\n",
      "00056 1\n",
      "00058 1\n",
      "00059 1\n",
      "00060 1\n",
      "00062 1\n",
      "00063 1\n",
      "00066 1\n",
      "00068 1\n",
      "00070 1\n",
      "00071 1\n",
      "00074 1\n",
      "00077 1\n",
      "00085 1\n",
      "00087 1\n",
      "00094 1\n",
      "00096 1\n",
      "00098 1\n",
      "00100 1\n",
      "00105 1\n",
      "00106 1\n",
      "00109 1\n",
      "00117 1\n",
      "00128 1\n",
      "00136 1\n",
      "00138 1\n",
      "00139 1\n",
      "00140 1\n",
      "00146 1\n",
      "00155 1\n",
      "00156 1\n",
      "00159 1\n",
      "00160 1\n",
      "00166 1\n",
      "00177 1\n",
      "00178 1\n",
      "00185 1\n",
      "00186 1\n",
      "00188 1\n",
      "00196 1\n",
      "00197 1\n",
      "00203 1\n",
      "00204 1\n",
      "00210 1\n",
      "00212 1\n",
      "00220 1\n",
      "00222 1\n",
      "00230 1\n",
      "00233 1\n",
      "00234 1\n",
      "00235 1\n",
      "00246 1\n",
      "00250 1\n",
      "00253 1\n",
      "00254 1\n",
      "00260 1\n",
      "00263 1\n",
      "00270 1\n",
      "00271 1\n",
      "00273 1\n",
      "00281 1\n",
      "00282 1\n",
      "00284 1\n",
      "00285 1\n",
      "00291 1\n",
      "00293 1\n",
      "00294 1\n",
      "00296 1\n",
      "00303 1\n",
      "00304 1\n",
      "00305 1\n",
      "00306 1\n",
      "00311 1\n",
      "00313 1\n",
      "00317 1\n",
      "00321 1\n",
      "00322 1\n",
      "00328 1\n",
      "00329 1\n",
      "00331 1\n",
      "00332 1\n",
      "00334 1\n",
      "00338 1\n",
      "00340 1\n",
      "00344 1\n",
      "00350 1\n",
      "00352 1\n",
      "00359 1\n",
      "00360 1\n",
      "00364 1\n",
      "00366 1\n",
      "00367 1\n",
      "00369 1\n",
      "00370 1\n",
      "00371 1\n",
      "00383 1\n",
      "00386 1\n",
      "00400 1\n",
      "00403 1\n",
      "00404 1\n",
      "00406 1\n",
      "00409 1\n",
      "00413 1\n",
      "00425 1\n",
      "00426 1\n",
      "00429 1\n",
      "00431 1\n",
      "00436 1\n",
      "00440 1\n",
      "00442 1\n",
      "00443 1\n",
      "00449 1\n",
      "00451 1\n",
      "00456 1\n",
      "00468 1\n",
      "00470 1\n",
      "00472 1\n",
      "00478 1\n",
      "00479 1\n",
      "00480 1\n",
      "00483 1\n",
      "00485 1\n",
      "00488 1\n",
      "00491 1\n",
      "00493 1\n",
      "00494 1\n",
      "00499 1\n",
      "00500 1\n",
      "00501 1\n",
      "00504 1\n",
      "00505 1\n",
      "00506 1\n",
      "00511 1\n",
      "00513 1\n",
      "00516 1\n",
      "00517 1\n",
      "00520 1\n",
      "00523 1\n",
      "00525 1\n",
      "00526 1\n",
      "00528 1\n",
      "00529 1\n",
      "00532 1\n",
      "00537 1\n",
      "00539 1\n",
      "00542 1\n",
      "00543 1\n",
      "00544 1\n",
      "00548 1\n",
      "00550 1\n",
      "00551 1\n",
      "00554 1\n",
      "00556 1\n",
      "00557 1\n",
      "00558 1\n",
      "00559 1\n",
      "00561 1\n",
      "00564 1\n",
      "00570 1\n",
      "00576 1\n",
      "00579 1\n",
      "00582 1\n",
      "00583 1\n",
      "00584 1\n",
      "00586 1\n",
      "00590 1\n",
      "00593 1\n",
      "00594 1\n",
      "00597 1\n",
      "00598 1\n",
      "00599 1\n",
      "00602 1\n",
      "00604 1\n",
      "00606 1\n",
      "00607 1\n",
      "00608 1\n",
      "00612 1\n",
      "00613 1\n",
      "00615 1\n",
      "00618 1\n",
      "00621 1\n",
      "00622 1\n",
      "00625 1\n",
      "00626 1\n",
      "00628 1\n",
      "00631 1\n",
      "00639 1\n",
      "00640 1\n",
      "00650 1\n",
      "00652 1\n",
      "00655 1\n",
      "00656 1\n",
      "00659 1\n",
      "00661 1\n",
      "00674 1\n",
      "00675 1\n",
      "00676 1\n",
      "00677 1\n",
      "00679 1\n",
      "00690 1\n",
      "00691 1\n",
      "00693 1\n",
      "00697 1\n",
      "00698 1\n",
      "00704 1\n",
      "00705 1\n",
      "00708 1\n",
      "00715 1\n",
      "00716 1\n",
      "00718 1\n",
      "00725 1\n",
      "00731 1\n",
      "00732 1\n",
      "00736 1\n",
      "00737 1\n",
      "00739 1\n",
      "00740 1\n",
      "00746 1\n",
      "00750 1\n",
      "00757 1\n",
      "00758 1\n",
      "00760 1\n",
      "00765 1\n",
      "00768 1\n",
      "00772 1\n",
      "00773 1\n",
      "00775 1\n",
      "00777 1\n",
      "00781 1\n",
      "00782 1\n",
      "00784 1\n",
      "00787 1\n",
      "00789 1\n",
      "00791 1\n",
      "00793 1\n",
      "00794 1\n",
      "00795 1\n",
      "00801 1\n",
      "00807 1\n",
      "00811 1\n",
      "00816 1\n",
      "00819 1\n",
      "00823 1\n",
      "00828 1\n",
      "00838 1\n",
      "00840 1\n",
      "00998 1\n",
      "00999 1\n",
      "01000 1\n",
      "01001 1\n",
      "01002 1\n",
      "01003 1\n",
      "01005 1\n",
      "01007 1\n",
      "01008 1\n",
      "129 273 141\n",
      "['00018', '00019', '00021', '00022', '00030', '00036', '00053', '00064', '00072', '00088', '00090', '00095', '00097', '00099', '00116', '00121', '00122', '00124', '00130', '00133', '00142', '00150', '00154', '00157', '00158', '00165', '00167', '00169', '00172', '00176', '00183', '00184', '00191', '00195', '00201', '00206', '00209', '00211', '00214', '00216', '00221', '00231', '00237', '00238', '00239', '00242', '00247', '00249', '00259', '00266', '00267', '00283', '00289', '00290', '00297', '00300', '00301', '00309', '00310', '00312', '00316', '00320', '00324', '00325', '00327', '00336', '00347', '00351', '00356', '00373', '00382', '00388', '00391', '00392', '00395', '00401', '00417', '00421', '00430', '00433', '00445', '00452', '00455', '00477', '00481', '00495', '00498', '00512', '00563', '00568', '00569', '00587', '00588', '00589', '00591', '00596', '00601', '00605', '00616', '00641', '00654', '00663', '00668', '00682', '00683', '00686', '00687', '00706', '00724', '00727', '00730', '00733', '00742', '00747', '00756', '00759', '00764', '00767', '00774', '00780', '00792', '00797', '00804', '00810', '00814', '00818', '00830', '01004', '01009']\n",
      "['00000', '00002', '00005', '00006', '00008', '00011', '00012', '00014', '00020', '00025', '00026', '00028', '00031', '00033', '00035', '00043', '00046', '00052', '00054', '00056', '00058', '00059', '00060', '00062', '00063', '00066', '00068', '00070', '00071', '00074', '00077', '00085', '00087', '00094', '00096', '00098', '00100', '00105', '00106', '00109', '00117', '00128', '00136', '00138', '00139', '00140', '00146', '00155', '00156', '00159', '00160', '00166', '00177', '00178', '00185', '00186', '00188', '00196', '00197', '00203', '00204', '00210', '00212', '00220', '00222', '00230', '00233', '00234', '00235', '00246', '00250', '00253', '00254', '00260', '00263', '00270', '00271', '00273', '00281', '00282', '00284', '00285', '00291', '00293', '00294', '00296', '00303', '00304', '00305', '00306', '00311', '00313', '00317', '00321', '00322', '00328', '00329', '00331', '00332', '00334', '00338', '00340', '00344', '00350', '00352', '00359', '00360', '00364', '00366', '00367', '00369', '00370', '00371', '00383', '00386', '00400', '00403', '00404', '00406', '00409', '00413', '00425', '00426', '00429', '00431', '00436', '00440', '00442', '00443', '00449', '00451', '00456', '00468', '00470', '00472', '00478', '00479', '00480', '00483', '00485', '00488', '00491', '00493', '00494', '00499', '00500', '00501', '00504', '00505', '00506', '00511', '00513', '00516', '00517', '00520', '00523', '00525', '00526', '00528', '00529', '00532', '00537', '00539', '00542', '00543', '00544', '00548', '00550', '00551', '00554', '00556', '00557', '00558', '00559', '00561', '00564', '00570', '00576', '00579', '00582', '00583', '00584', '00586', '00590', '00593', '00594', '00597', '00598', '00599', '00602', '00604', '00606', '00607', '00608', '00612', '00613', '00615', '00618', '00621', '00622', '00625', '00626', '00628', '00631', '00639', '00640', '00650', '00652', '00655', '00656', '00659', '00661', '00674', '00675', '00676', '00677', '00679', '00690', '00691', '00693', '00697', '00698', '00704', '00705', '00708', '00715', '00716', '00718', '00725', '00731', '00732', '00736', '00737', '00739', '00740', '00746', '00750', '00757', '00758', '00760', '00765', '00768', '00772', '00773', '00775', '00777', '00781', '00782', '00784', '00787', '00789', '00791', '00793', '00794', '00795', '00801', '00807', '00811', '00816', '00819', '00823', '00828', '00838', '00840', '00998', '00999', '01000', '01001', '01002', '01003', '01005', '01007', '01008']\n",
      "['00009', '00017', '00024', '00032', '00044', '00045', '00049', '00061', '00081', '00084', '00102', '00104', '00110', '00111', '00112', '00113', '00123', '00149', '00151', '00162', '00170', '00192', '00193', '00194', '00217', '00218', '00219', '00227', '00228', '00236', '00241', '00243', '00251', '00258', '00261', '00262', '00269', '00274', '00275', '00280', '00286', '00288', '00298', '00308', '00314', '00318', '00339', '00341', '00343', '00346', '00348', '00349', '00353', '00376', '00377', '00378', '00379', '00380', '00387', '00389', '00390', '00397', '00399', '00402', '00405', '00407', '00410', '00412', '00414', '00418', '00419', '00423', '00432', '00441', '00444', '00446', '00454', '00459', '00464', '00469', '00496', '00507', '00510', '00514', '00518', '00519', '00530', '00533', '00538', '00540', '00545', '00547', '00555', '00565', '00567', '00571', '00572', '00574', '00575', '00578', '00581', '00619', '00620', '00623', '00624', '00630', '00636', '00642', '00645', '00649', '00651', '00657', '00667', '00684', '00685', '00688', '00703', '00709', '00723', '00728', '00729', '00734', '00735', '00744', '00751', '00753', '00778', '00788', '00796', '00799', '00800', '00802', '00803', '00805', '00806', '00809', '00820', '00824', '00836', '00837', '00839']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "common_path = \"../../RSNA-BTC-Datasets/train_mat/T1wCE/\"\n",
    "tumor_path = \"../../RSNA-BTC-Datasets/ec_train_mat/T1wCE/\"\n",
    "print(\"Zeros:\")\n",
    "full_0 = os.listdir(common_path+\"0\")\n",
    "full_0.sort()\n",
    "full_1 = os.listdir(common_path+\"1\")\n",
    "full_1.sort()\n",
    "tumor_only_0 = os.listdir(tumor_path+\"0\") # t=1 and m=0\n",
    "tumor_only_1 = os.listdir(tumor_path+\"1\") #t=1 and m=1\n",
    "count_0 = 0 #f0\n",
    "count_1 = 0 #f1\n",
    "count_2 = 0 #h0\n",
    "f0_list = []\n",
    "f1_list = []\n",
    "h0_list = []\n",
    "\n",
    "for file in full_0:\n",
    "    if file in tumor_only_0:\n",
    "        print(file[:5],0) #tumor = 1 and meth = 0\n",
    "        count_0 += 1\n",
    "        f0_list.append(file[:5])\n",
    "    else:\n",
    "        print(file[:5],2) #tumor = 0 and meth = 0\n",
    "        count_2 += 1\n",
    "        h0_list.append(file[:5])\n",
    "print(\"Ones:\")\n",
    "for file in full_1:\n",
    "    if file in tumor_only_1:\n",
    "        print(file[:5],1) # tumor = 1 and meth = 1\n",
    "        count_1 += 1\n",
    "        f1_list.append(file[:5])\n",
    "    \n",
    "print(count_0, count_1, count_2)\n",
    "print(f0_list)\n",
    "print(f1_list)\n",
    "print(h0_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.dataset_utils import *\n",
    "from utils.classifier_utils import *\n",
    "\n",
    "import os\n",
    "\n",
    "import monai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                    format='%(asctime)s - %(message)s', datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#import config\n",
    "#from dataset import BrainRSNADataset\n",
    "\n",
    "dir_path = \"../../RSNA-BTC-Datasets/train_mat\"\n",
    "test_dir_path = \"../../RSNA-BTC-Datasets/test_mat\"\n",
    "tumor_only_dir_path = \"../../RSNA-BTC-Datasets/ec_train_mat\"\n",
    "tumor_only_test_dir_path = \"../../RSNA-BTC-Datasets/ec_test_mat\"\n",
    "no_tumor_dir_path = \"../../RSNA-BTC-Datasets/no_tumor_train_mat\"\n",
    "#ext_test_1_dir_path = \"../../RSNA-BTC-Datasets/brats18_mat\"\n",
    "#ext_test_0_dir_path = \"../../RSNA-BTC-Datasets/OpenNeuroDS000221_ss_mat\"\n",
    "new_dir_path = \"../../RSNA-BTC-Datasets/UPENN-GBM_mat\"\n",
    "\n",
    "def generate_datasets(types):\n",
    "    data_packs = {}\n",
    "    ext = \"mat\"\n",
    "    transform = None\n",
    "    dims = 3\n",
    "    sel_slices = None\n",
    "    for t in types:\n",
    "        print(\"Type: \"+t)\n",
    "        # Competition Train + Val + Test\n",
    "        m_dataset_0 = Dataset(dir_path, [t], list_classes=[\"0\"], transform=transform, ext=ext, dims=dims, sel_slices=sel_slices)\n",
    "\n",
    "        logging.info(\"m0 Train/Val datasets size: {}\".format(len(m_dataset_0)))\n",
    "\n",
    "        m_dataset_1 = Dataset(dir_path, [t], list_classes=[\"1\"], transform=transform, ext=ext, dims=dims, sel_slices=sel_slices)\n",
    "        \n",
    "        logging.info(\"m1 Train/Val datasets size: {}\".format(len(m_dataset_1)))\n",
    "\n",
    "        # External Train + Val + Test\n",
    "        #t_dataset_0 = Dataset(ext_test_0_dir_path, [t], list_classes=[\"0\"], transform=transform, ext=ext, dims=dims, sel_slices=sel_slices)\n",
    "\n",
    "        #logging.info(\"Train/Val datasets size: {}\".format(len(t_dataset_0)))\n",
    "\n",
    "        #t_dataset_1 = Dataset(ext_test_1_dir_path, [t], list_classes=[\"1\"], transform=transform, ext=ext, dims=dims, sel_slices=sel_slices)\n",
    "\n",
    "        #logging.info(\"Train/Val datasets size: {}\".format(len(t_dataset_1)))\n",
    "\n",
    "        # UPENN Train + Val + Test\n",
    "        n_dataset_0 = Dataset(new_dir_path, [t], list_classes=[\"0\"], transform=transform, ext=ext, dims=dims, sel_slices=sel_slices)\n",
    "\n",
    "        logging.info(\"n0 Train/Val datasets size: {}\".format(len(n_dataset_0)))\n",
    "\n",
    "        n_dataset_1 = Dataset(new_dir_path, [t], list_classes=[\"1\"], transform=transform, ext=ext, dims=dims, sel_slices=sel_slices)\n",
    "        \n",
    "        logging.info(\"n1 Train/Val datasets size: {}\".format(len(n_dataset_1)))\n",
    "        \n",
    "        if t == \"KLF\" or t == \"T1wCE\":\n",
    "            # Competition (Tumor Only) Train + Val + Test\n",
    "            f_dataset_0 = Dataset(tumor_only_dir_path, [t], list_classes=[\"0\"], transform=transform, ext=ext, dims=dims, sel_slices=sel_slices)\n",
    "\n",
    "            logging.info(\"f0 Train/Val datasets size: {}\".format(len(f_dataset_0)))\n",
    "\n",
    "            f_dataset_1 = Dataset(tumor_only_dir_path, [t], list_classes=[\"1\"], transform=transform, ext=ext, dims=dims, sel_slices=sel_slices)\n",
    "\n",
    "            logging.info(\"f1 Train/Val datasets size: {}\".format(len(f_dataset_1)))\n",
    "            \n",
    "            # Competition (No Tumor) Train + Val + Test\n",
    "            h_dataset_0 = Dataset(no_tumor_dir_path, [t], list_classes=[\"0\"], transform=transform, ext=ext, dims=dims, sel_slices=sel_slices)\n",
    "\n",
    "            logging.info(\"h0 Train/Val datasets size: {}\".format(len(h_dataset_0)))\n",
    "            \n",
    "            # Competition (Tumor Only) + UPENN Train + Val + Test\n",
    "            fn_dataset_0 = Dataset().concat_datasets(f_dataset_0, n_dataset_0)\n",
    "            \n",
    "            logging.info(\"fn0 Train/Val datasets size: {}\".format(len(fn_dataset_0)))\n",
    "            \n",
    "            fn_dataset_1 = Dataset().concat_datasets(f_dataset_1, n_dataset_1)\n",
    "            \n",
    "            logging.info(\"fn1 Train/Val datasets size: {}\".format(len(fn_dataset_1)))\n",
    "        \n",
    "        if t == \"KLF\" or t == \"T1wCE\":\n",
    "            data_packs[t] = {\n",
    "                \"m_dataset_0\": m_dataset_0,\n",
    "                \"m_dataset_1\": m_dataset_1,\n",
    "                \"f_dataset_0\": f_dataset_0,\n",
    "                \"f_dataset_1\": f_dataset_1,\n",
    "                \"h_dataset_0\": h_dataset_0,\n",
    "                #\"t_dataset_0\": t_dataset_0,\n",
    "                #\"t_dataset_1\": t_dataset_1,\n",
    "                \"n_dataset_0\": n_dataset_0,\n",
    "                \"n_dataset_1\": n_dataset_1,\n",
    "                \"fn_dataset_0\": fn_dataset_0,\n",
    "                \"fn_dataset_1\": fn_dataset_1\n",
    "            }\n",
    "        else:\n",
    "            data_packs[t] = {\n",
    "                \"m_dataset_0\": m_dataset_0,\n",
    "                \"m_dataset_1\": m_dataset_1,\n",
    "                #\"t_dataset_0\": t_dataset_0,\n",
    "                #\"t_dataset_1\": t_dataset_1,\n",
    "                \"n_dataset_0\": n_dataset_0,\n",
    "                \"n_dataset_1\": n_dataset_1\n",
    "            }\n",
    "    return data_packs\n",
    "\n",
    "def get_merged_dataset(dataset_0, dataset_1, k=1):\n",
    "    dataset_merged = Dataset().concat_datasets(dataset_0, dataset_1)\n",
    "    dataset_merged_no_tr = Dataset().concat_datasets(dataset_0, dataset_1, import_transform=False)\n",
    "\n",
    "    val_total_ratio = 0.2\n",
    "    is_k_fold = (k > 1)\n",
    "    splits = dataset_utils.get_splits(dataset_0, dataset_1, val_total_ratio, is_k_fold, 0.1, k)\n",
    "    print(splits)\n",
    "    return dataset_merged, dataset_merged_no_tr, splits\n",
    "\n",
    "def get_loaders(packs, batch_size):\n",
    "    loader_packs = {}\n",
    "    for t, pack in packs.items():\n",
    "        print(\"Type: \"+t)\n",
    "        m_dataset_merged, m_dataset_merged_no_tr, m_splits = get_merged_dataset(pack['m_dataset_0'], pack['m_dataset_1'])\n",
    "        n_dataset_merged, n_dataset_merged_no_tr, n_splits = get_merged_dataset(pack['n_dataset_0'], pack['n_dataset_1'])\n",
    "        #t_dataset_merged, t_dataset_merged_no_tr, t_splits = get_merged_dataset(pack['t_dataset_0'], pack['t_dataset_1'])\n",
    "        if t == \"KLF\" or t == \"T1wCE\":\n",
    "            f_dataset_merged, f_dataset_merged_no_tr, f_splits = get_merged_dataset(pack['f_dataset_0'], pack['f_dataset_1'])\n",
    "            h_dataset_merged, h_dataset_merged_no_tr, h_splits = get_merged_dataset(pack['h_dataset_0'], None)\n",
    "            fn_dataset_merged, fn_dataset_merged_no_tr, fn_splits = get_merged_dataset(pack['fn_dataset_0'], pack['fn_dataset_1'])\n",
    "        \n",
    "        m_dataloader = get_all_split_loaders(m_dataset_merged, m_dataset_merged_no_tr, m_splits, batch_size)\n",
    "        m_dataloaders = list(m_dataloader[0])\n",
    "        logging.info(\"(M) Train validation test splitted: {} {} {}\".format(len(m_splits[0][0]),len(m_splits[0][1]),len(m_splits[0][2])))\n",
    "\n",
    "        #t_dataloader = get_all_split_loaders(t_dataset_merged, t_dataset_merged_no_tr, t_splits, info[\"batch_size\"])\n",
    "        #t_dataloaders = list(t_dataloader[0])\n",
    "        #logging.info(\"(T) Train validation test splitted: {} {} {}\".format(len(t_splits[0][0]),len(t_splits[0][1]),len(t_splits[0][2])))\n",
    "\n",
    "        n_dataloader = get_all_split_loaders(n_dataset_merged, n_dataset_merged_no_tr, n_splits, batch_size)\n",
    "        n_dataloaders = list(n_dataloader[0])\n",
    "        logging.info(\"(N) Train validation test splitted: {} {} {}\".format(len(n_splits[0][0]),len(n_splits[0][1]),len(n_splits[0][2])))\n",
    "        \n",
    "        if t == \"KLF\" or t == \"T1wCE\":\n",
    "            f_dataloader = get_all_split_loaders(f_dataset_merged, f_dataset_merged_no_tr, f_splits, batch_size)\n",
    "            f_dataloaders = list(f_dataloader[0])\n",
    "            logging.info(\"(F) Train validation test splitted: {} {} {}\".format(len(f_splits[0][0]),len(f_splits[0][1]),len(f_splits[0][2])))\n",
    "            \n",
    "            h_dataloader = get_all_split_loaders(h_dataset_merged, h_dataset_merged_no_tr, h_splits, batch_size)\n",
    "            h_dataloaders = list(h_dataloader[0])\n",
    "            logging.info(\"(H) Train validation test splitted: {} {} {}\".format(len(h_splits[0][0]),len(h_splits[0][1]),len(h_splits[0][2])))\n",
    "\n",
    "            fn_dataloader = get_all_split_loaders(fn_dataset_merged, fn_dataset_merged_no_tr, fn_splits, batch_size)\n",
    "            fn_dataloaders = list(fn_dataloader[0])\n",
    "            logging.info(\"(FN) Train validation test splitted: {} {} {}\".format(len(fn_splits[0][0]),len(fn_splits[0][1]),len(fn_splits[0][2])))\n",
    "        \n",
    "        if t == \"KLF\" or t == \"T1wCE\":\n",
    "            loader_packs[t] = {\n",
    "                \"m_dataloaders\": m_dataloaders,\n",
    "                \"f_dataloaders\": f_dataloaders,\n",
    "                \"h_dataloaders\": h_dataloaders,\n",
    "                \"n_dataloaders\": n_dataloaders,\n",
    "                #\"t_dataloaders\": t_dataloaders,\n",
    "                \"fn_dataloaders\": fn_dataloaders\n",
    "            }\n",
    "        else:\n",
    "            loader_packs[t] = {\n",
    "                \"m_dataloaders\": m_dataloaders,\n",
    "                \"n_dataloaders\": n_dataloaders,\n",
    "                #\"t_dataloaders\": t_dataloaders\n",
    "            }\n",
    "        \n",
    "    return loader_packs\n",
    "\n",
    "import importlib\n",
    "from utils import dataset_utils\n",
    "importlib.reload(dataset_utils)\n",
    "\n",
    "def train_model(train_dl, validation_dl, fold_number, mri_type, model_name, epochs):\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = monai.networks.nets.resnet10(spatial_dims=3, n_input_channels=1, n_classes=1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.5, last_epoch=-1, verbose=True)\n",
    "\n",
    "    model.zero_grad()\n",
    "    model.to(device)\n",
    "    best_loss = 9999\n",
    "    best_auc = 0\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    train_writer = SummaryWriter()\n",
    "    for counter in range(epochs):\n",
    "\n",
    "        epoch_iterator_train = tqdm(train_dl)\n",
    "        tr_loss = 0.0\n",
    "        for step, batch in enumerate(epoch_iterator_train):\n",
    "            model.train()\n",
    "            (img_ids, imgs, labels) = batch\n",
    "            images, targets = imgs[0].to(device), labels.to(device)\n",
    "            #images, targets = batch[\"image\"].to(device), batch[\"target\"].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            targets = targets  # .view(-1, 1)\n",
    "            loss = criterion(outputs.squeeze(1), targets.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            epoch_iterator_train.set_postfix(\n",
    "                batch_loss=(loss.item()), loss=(tr_loss / (step + 1))\n",
    "            )\n",
    "\n",
    "        train_writer.add_scalar('loss', (tr_loss/(step+1)), counter+1)\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            preds = []\n",
    "            true_labels = []\n",
    "            case_ids = []\n",
    "            epoch_iterator_val = tqdm(validation_dl)\n",
    "            for step, batch in enumerate(epoch_iterator_val):\n",
    "                model.eval()\n",
    "                (img_ids, imgs, labels) = batch\n",
    "                images, targets = imgs[0].to(device), labels.to(device)\n",
    "                #images, targets = batch[\"image\"].to(device), batch[\"target\"].to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                targets = targets  # .view(-1, 1)\n",
    "                loss = criterion(outputs.squeeze(1), targets.float())\n",
    "                val_loss += loss.item()\n",
    "                epoch_iterator_val.set_postfix(\n",
    "                    batch_loss=(loss.item()), loss=(val_loss / (step + 1))\n",
    "                )\n",
    "                preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "                true_labels.append(targets.cpu().numpy())\n",
    "                #case_ids.append(batch[\"case_id\"])\n",
    "                if img_ids[0][0][0].isnumeric():\n",
    "                    img_ids_fixed = [img_id[:5] for img_id in img_ids[0]]\n",
    "                else:\n",
    "                    img_ids_fixed = [img_id[:-4] for img_id in img_ids[0]]\n",
    "                case_ids.append(img_ids_fixed)\n",
    "        preds = np.vstack(preds).T[0].tolist()\n",
    "        true_labels = np.hstack(true_labels).tolist()\n",
    "        case_ids = np.hstack(case_ids).tolist()\n",
    "        auc_score = roc_auc_score(true_labels, preds)\n",
    "        auc_score_adj_best = 0\n",
    "        for thresh in np.linspace(0, 1, 50):\n",
    "            auc_score_adj = roc_auc_score(true_labels, list(np.array(preds) > thresh))\n",
    "            if auc_score_adj > auc_score_adj_best:\n",
    "                best_thresh = thresh\n",
    "                auc_score_adj_best = auc_score_adj\n",
    "\n",
    "        print(\n",
    "            f\"EPOCH {counter+1}/{epochs}: Validation average loss: {val_loss/(step+1)} + AUC SCORE = {auc_score} + AUC SCORE THRESH {best_thresh} = {auc_score_adj_best}\"\n",
    "        )\n",
    "        train_writer.add_scalar('val_loss', val_loss/(step+1), counter+1)\n",
    "        train_writer.add_scalar('val_auc', auc_score, counter+1)\n",
    "        if auc_score > best_auc:\n",
    "            print(\"Saving the model...\")\n",
    "            date_time = datetime.now()\n",
    "            date_str = date_time.strftime(\"%b%d_%H-%M-%S\")\n",
    "    \n",
    "            #modelname = model.__class__.__name__\n",
    "        \n",
    "            model_fullname = f\"{model_name}_{date_str}\"\n",
    "\n",
    "            if not os.path.exists(\"../../RSNA-BTC-Datasets/rsnaresnet10_output_weights/\"):\n",
    "                os.mkdir(\"../../RSNA-BTC-Datasets/rsnaresnet10_output_weights/\")\n",
    "            if not os.path.exists(f\"../../RSNA-BTC-Datasets/rsnaresnet10_output_weights/{model_fullname}\"):\n",
    "                os.mkdir(f\"../../RSNA-BTC-Datasets/rsnaresnet10_output_weights/{model_fullname}\")\n",
    "            all_files = os.listdir(f\"../../RSNA-BTC-Datasets/rsnaresnet10_output_weights/{model_fullname}\")\n",
    "\n",
    "            for f in all_files:\n",
    "                if f\"{model_name}_{mri_type}_fold{fold_number}\" in f:\n",
    "                    os.remove(f\"../../RSNA-BTC-Datasets/rsnaresnet10_output_weights/{model_fullname}/{f}\")\n",
    "\n",
    "            best_auc = auc_score\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                f\"../../RSNA-BTC-Datasets/rsnaresnet10_output_weights/{model_fullname}/3d-{model_name}_{mri_type}_fold{fold_number}_{round(best_auc,3)}.pth\",\n",
    "            )\n",
    "\n",
    "    print(best_auc)\n",
    "\n",
    "def train_folded_models(fold, mri_type, model_name, batch_size, epochs, train_origins):\n",
    "    #data = pd.read_csv(\"train.csv\")\n",
    "    #train_df = data[data.fold != fold].reset_index(drop=False)\n",
    "    #val_df = data[data.fold == fold].reset_index(drop=False)\n",
    "    \n",
    "    print(f\"train_{mri_type}_{fold}\")\n",
    "    #train_dataset = BrainRSNADataset(data=train_df, mri_type=args.type, ds_type=f\"train_{args.type}_{args.fold}\")\n",
    "\n",
    "    #valid_dataset = BrainRSNADataset(data=val_df, mri_type=args.type, ds_type=f\"val_{args.type}_{args.fold}\")\n",
    "\n",
    "    ##packs = generate_datasets([mri_type])\n",
    "    \n",
    "    loader_packs = {}\n",
    "    for t, pack in packs.items():\n",
    "        print(\"Type: \"+t)\n",
    "        m_dataset_merged, m_dataset_merged_no_tr, m_splits = get_merged_dataset(pack['m_dataset_0'], pack['m_dataset_1'], fold)\n",
    "        n_dataset_merged, n_dataset_merged_no_tr, n_splits = get_merged_dataset(pack['n_dataset_0'], pack['n_dataset_1'], fold)\n",
    "        fn_dataset_merged, fn_dataset_merged_no_tr, fn_splits = get_merged_dataset(pack['fn_dataset_0'], pack['fn_dataset_1'], fold)\n",
    "        f_dataset_merged, f_dataset_merged_no_tr, f_splits = get_merged_dataset(pack['f_dataset_0'], pack['f_dataset_1'], fold)\n",
    "        print(\"SPLITS:\")\n",
    "        print(\"- folds:\")\n",
    "        print(len(m_splits))\n",
    "        print(\"- splits per fold:\")\n",
    "        print(len(m_splits[0]))\n",
    "        #print(len(n_splits))\n",
    "        for to in train_origins:\n",
    "            if to == \"m\":\n",
    "                i = 0\n",
    "                for split in m_splits:\n",
    "                    m_dataloader = get_all_split_loaders(m_dataset_merged, m_dataset_merged_no_tr, [split], batch_size)\n",
    "                    m_dataloaders = list(m_dataloader[0])\n",
    "                    print(f\"(M) Train validation splitted: {len(split[0])} {len(split[1])}\")\n",
    "                    print(m_dataloaders)\n",
    "                    train_dl = m_dataloaders[0]\n",
    "                    validation_dl = m_dataloaders[1]\n",
    "                    train_model(train_dl, validation_dl, i, mri_type, model_name+\"_m\", epochs)\n",
    "                    \"\"\"\n",
    "                    device = torch.device(\"cuda\")\n",
    "                    model = monai.networks.nets.resnet10(spatial_dims=3, n_input_channels=1, n_classes=1)\n",
    "                    model.to(device)\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "                    criterion = nn.BCEWithLogitsLoss()\n",
    "                    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.5, last_epoch=-1, verbose=True)\n",
    "                    trainer = Trainer(\n",
    "                        model, \n",
    "                        device, \n",
    "                        optimizer, \n",
    "                        criterion,\n",
    "                        scheduler,\n",
    "                        1\n",
    "                    )\n",
    "\n",
    "                    history = trainer.fit(\n",
    "                        device,\n",
    "                        epochs, \n",
    "                        train_dl,\n",
    "                        validation_dl,\n",
    "                        model_name+\"_m\", \n",
    "                        15\n",
    "                    )\n",
    "\n",
    "                    trainer.train_writer.flush()\n",
    "                    \"\"\"\n",
    "                    i += 1\n",
    "\n",
    "            elif to == \"n\":\n",
    "                i = 0\n",
    "                for split in n_splits:\n",
    "                    n_dataloader = get_all_split_loaders(n_dataset_merged, n_dataset_merged_no_tr, [split], batch_size)\n",
    "                    n_dataloaders = list(n_dataloader[0])\n",
    "                    print(f\"(N) Train validation splitted: {len(split[0])} {len(split[1])}\")\n",
    "                    train_dl = n_dataloaders[0]\n",
    "                    validation_dl = n_dataloaders[1]\n",
    "                    train_model(train_dl, validation_dl, i, mri_type, model_name+\"_n\", epochs)\n",
    "                    i += 1\n",
    "            elif to == \"fn\":\n",
    "                i = 0\n",
    "                for split in fn_splits:\n",
    "                    print(f\"Fold n.{i} started\")\n",
    "                    fn_dataloader = get_all_split_loaders(fn_dataset_merged, fn_dataset_merged_no_tr, [split], batch_size)\n",
    "                    fn_dataloaders = list(fn_dataloader[0])\n",
    "                    print(f\"(FN) Train validation splitted: {len(split[0])} {len(split[1])}\")\n",
    "                    train_dl = fn_dataloaders[0]\n",
    "                    validation_dl = fn_dataloaders[1]\n",
    "                    train_model(train_dl, validation_dl, i, mri_type, model_name+\"_fn\", epochs)\n",
    "                    i += 1\n",
    "            elif to == \"f\":\n",
    "                i = 0\n",
    "                for split in f_splits:\n",
    "                    f_dataloader = get_all_split_loaders(f_dataset_merged, f_dataset_merged_no_tr, [split], batch_size)\n",
    "                    f_dataloaders = list(f_dataloader[0])\n",
    "                    print(f\"(F) Train validation splitted: {len(split[0])} {len(split[1])}\")\n",
    "                    train_dl = f_dataloaders[0]\n",
    "                    validation_dl = f_dataloaders[1]\n",
    "                    train_model(train_dl, validation_dl, i, mri_type, model_name+\"_f\", epochs)\n",
    "                    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14356929"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(monai.networks.nets.resnet10(spatial_dims=3, n_input_channels=1, n_classes=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce RTX 3090\n",
      "1.12.1\n",
      "11.3\n",
      "tensor([-1.2654], device='cuda:0')\n",
      "2022-08-09 19:35:23 - Printing one: 1\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\"\"\"\n",
    "for file in os.listdir(no_tumor_dir_path+\"/KLF/0\"):\n",
    "    fixed_file = file[:-7]+\"T1wCE.mat\"\n",
    "    shutil.copy(dir_path+\"/T1wCE/0/\"+fixed_file, no_tumor_dir_path+\"/T1wCE/0/\"+fixed_file)\n",
    "    \n",
    "for file in os.listdir(tumor_only_dir_path+\"/KLF/0\"):\n",
    "    fixed_file = file[:-7]+\"T1wCE.mat\"\n",
    "    shutil.copy(dir_path+\"/T1wCE/0/\"+fixed_file, tumor_only_dir_path+\"/T1wCE/0/\"+fixed_file)\n",
    "    \n",
    "for file in os.listdir(tumor_only_dir_path+\"/KLF/1\"):\n",
    "    fixed_file = file[:-7]+\"T1wCE.mat\"\n",
    "    shutil.copy(dir_path+\"/T1wCE/1/\"+fixed_file, tumor_only_dir_path+\"/T1wCE/1/\"+fixed_file)\n",
    "\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "x = torch.randn(1).cuda()\n",
    "print(x)\n",
    "logging.info(\"Printing one: {}\".format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: T1wCE\n",
      "2022-08-09 19:35:24 - m0 Train/Val datasets size: 270\n",
      "2022-08-09 19:35:24 - m1 Train/Val datasets size: 303\n",
      "2022-08-09 19:35:24 - n0 Train/Val datasets size: 170\n",
      "2022-08-09 19:35:24 - n1 Train/Val datasets size: 121\n",
      "2022-08-09 19:35:24 - f0 Train/Val datasets size: 129\n",
      "2022-08-09 19:35:24 - f1 Train/Val datasets size: 273\n",
      "2022-08-09 19:35:24 - h0 Train/Val datasets size: 141\n",
      "Length of concatenated dataset: 340\n",
      "2022-08-09 19:35:24 - fn0 Train/Val datasets size: 340\n",
      "Length of concatenated dataset: 546\n",
      "2022-08-09 19:35:24 - fn1 Train/Val datasets size: 546\n",
      "Type: T1wCE\n",
      "Length of concatenated dataset: 606\n",
      "Length of concatenated dataset: 606\n",
      "Train Idx:\n",
      "[238, 137, 106, 284, 44, 139, 247, 288, 156, 297, 252, 54, 234, 18, 205, 254, 182, 56, 71, 144, 249, 209, 290, 219, 158, 176, 33, 83, 136, 210, 118, 60, 159, 282, 110, 21, 29, 150, 16, 75, 109, 179, 283, 4, 96, 229, 61, 67, 295, 266, 171, 281, 40, 189, 13, 107, 200, 3, 161, 125, 24, 30, 77, 279, 190, 19, 257, 235, 268, 80, 51, 2, 239, 104, 262, 86, 10, 224, 58, 41, 14, 155, 50, 215, 237, 123, 220, 62, 191, 230, 130, 213, 187, 43, 114, 138, 199, 222, 149, 112, 298, 98, 221, 93, 208, 162, 36, 178, 113, 0, 94, 294, 95, 299, 263, 256, 69, 49, 48, 85, 300, 141, 207, 23, 250, 148, 143, 78, 180, 100, 204, 131, 269, 301, 196, 6, 68, 203, 84, 170, 121, 140, 258, 276, 142, 259, 91, 82, 285, 11, 119, 102, 35, 57, 169, 231, 65, 1, 120, 267, 186, 42, 105, 132, 79, 17, 271, 38, 53, 260, 128, 28, 183, 163, 151, 244, 202, 31, 32, 127, 185, 280, 273, 147, 278, 177, 99, 197, 243, 115, 265, 72, 25, 165, 289, 174, 291, 39, 193, 88, 70, 87, 292, 242, 277, 211, 9, 195, 251, 192, 117, 47, 172, 516, 380, 549, 365, 554, 536, 489, 433, 486, 369, 338, 320, 465, 589, 448, 528, 499, 348, 353, 360, 556, 547, 531, 544, 335, 381, 449, 436, 399, 517, 401, 505, 471, 392, 543, 561, 550, 435, 349, 500, 410, 473, 303, 542, 374, 579, 405, 370, 411, 525, 387, 584, 557, 428, 539, 412, 329, 484, 602, 545, 490, 437, 372, 402, 527, 521, 342, 496, 493, 432, 551, 443, 434, 564, 416, 321, 379, 495, 552, 414, 479, 312, 492, 462, 599, 346, 394, 407, 441, 478, 415, 371, 339, 513, 368, 345, 429, 306, 420, 451, 596, 580, 574, 431, 485, 440, 600, 310, 389, 482, 422, 569, 418, 464, 332, 458, 563, 488, 404, 400, 323, 456, 357, 333, 583, 352, 403, 565, 594, 454, 417, 359, 447, 363, 497, 535, 515, 502, 601, 309, 562, 311, 520, 382, 576, 511, 577, 546, 568, 470, 341, 373, 424, 582, 347, 603, 597, 480, 361, 466, 427, 457, 383, 362, 595, 351, 356, 461, 354, 575, 522, 377, 307, 442, 510, 590, 559, 308, 444, 364, 438, 386, 530, 325, 573, 523, 506, 423, 384, 316, 587, 375, 450, 463, 390, 541, 605, 439, 498, 494, 419, 327, 397, 467, 409, 319, 366, 408, 512, 331, 504, 477, 396]\n",
      "Val Idx:\n",
      "[225, 152, 228, 201, 52, 245, 175, 168, 223, 217, 111, 135, 218, 12, 15, 66, 97, 90, 198, 103, 22, 212, 226, 264, 133, 216, 275, 270, 154, 55, 194, 255, 134, 8, 157, 241, 240, 81, 214, 167, 5, 59, 92, 274, 34, 145, 116, 188, 246, 7, 45, 129, 122, 63, 124, 227, 146, 302, 26, 108, 560, 472, 367, 518, 487, 343, 514, 459, 588, 426, 524, 355, 538, 566, 395, 474, 425, 468, 558, 324, 567, 326, 591, 328, 592, 571, 337, 322, 314, 391, 421, 586, 453, 455, 508, 570, 491, 406, 501, 388, 378, 534, 452, 598, 385, 578, 315, 336, 581, 555, 413, 507, 334, 445, 529, 330, 340, 317, 604, 481]\n",
      "Test Idx:\n",
      "[89, 74, 153, 64, 296, 287, 286, 236, 126, 73, 20, 46, 160, 232, 181, 27, 173, 261, 37, 101, 166, 233, 184, 164, 206, 248, 253, 293, 76, 272, 305, 585, 304, 475, 476, 533, 548, 553, 344, 318, 393, 483, 532, 509, 540, 313, 430, 350, 526, 572, 460, 446, 398, 469, 537, 593, 358, 376, 519, 503]\n",
      "[([238, 137, 106, 284, 44, 139, 247, 288, 156, 297, 252, 54, 234, 18, 205, 254, 182, 56, 71, 144, 249, 209, 290, 219, 158, 176, 33, 83, 136, 210, 118, 60, 159, 282, 110, 21, 29, 150, 16, 75, 109, 179, 283, 4, 96, 229, 61, 67, 295, 266, 171, 281, 40, 189, 13, 107, 200, 3, 161, 125, 24, 30, 77, 279, 190, 19, 257, 235, 268, 80, 51, 2, 239, 104, 262, 86, 10, 224, 58, 41, 14, 155, 50, 215, 237, 123, 220, 62, 191, 230, 130, 213, 187, 43, 114, 138, 199, 222, 149, 112, 298, 98, 221, 93, 208, 162, 36, 178, 113, 0, 94, 294, 95, 299, 263, 256, 69, 49, 48, 85, 300, 141, 207, 23, 250, 148, 143, 78, 180, 100, 204, 131, 269, 301, 196, 6, 68, 203, 84, 170, 121, 140, 258, 276, 142, 259, 91, 82, 285, 11, 119, 102, 35, 57, 169, 231, 65, 1, 120, 267, 186, 42, 105, 132, 79, 17, 271, 38, 53, 260, 128, 28, 183, 163, 151, 244, 202, 31, 32, 127, 185, 280, 273, 147, 278, 177, 99, 197, 243, 115, 265, 72, 25, 165, 289, 174, 291, 39, 193, 88, 70, 87, 292, 242, 277, 211, 9, 195, 251, 192, 117, 47, 172, 516, 380, 549, 365, 554, 536, 489, 433, 486, 369, 338, 320, 465, 589, 448, 528, 499, 348, 353, 360, 556, 547, 531, 544, 335, 381, 449, 436, 399, 517, 401, 505, 471, 392, 543, 561, 550, 435, 349, 500, 410, 473, 303, 542, 374, 579, 405, 370, 411, 525, 387, 584, 557, 428, 539, 412, 329, 484, 602, 545, 490, 437, 372, 402, 527, 521, 342, 496, 493, 432, 551, 443, 434, 564, 416, 321, 379, 495, 552, 414, 479, 312, 492, 462, 599, 346, 394, 407, 441, 478, 415, 371, 339, 513, 368, 345, 429, 306, 420, 451, 596, 580, 574, 431, 485, 440, 600, 310, 389, 482, 422, 569, 418, 464, 332, 458, 563, 488, 404, 400, 323, 456, 357, 333, 583, 352, 403, 565, 594, 454, 417, 359, 447, 363, 497, 535, 515, 502, 601, 309, 562, 311, 520, 382, 576, 511, 577, 546, 568, 470, 341, 373, 424, 582, 347, 603, 597, 480, 361, 466, 427, 457, 383, 362, 595, 351, 356, 461, 354, 575, 522, 377, 307, 442, 510, 590, 559, 308, 444, 364, 438, 386, 530, 325, 573, 523, 506, 423, 384, 316, 587, 375, 450, 463, 390, 541, 605, 439, 498, 494, 419, 327, 397, 467, 409, 319, 366, 408, 512, 331, 504, 477, 396], [225, 152, 228, 201, 52, 245, 175, 168, 223, 217, 111, 135, 218, 12, 15, 66, 97, 90, 198, 103, 22, 212, 226, 264, 133, 216, 275, 270, 154, 55, 194, 255, 134, 8, 157, 241, 240, 81, 214, 167, 5, 59, 92, 274, 34, 145, 116, 188, 246, 7, 45, 129, 122, 63, 124, 227, 146, 302, 26, 108, 560, 472, 367, 518, 487, 343, 514, 459, 588, 426, 524, 355, 538, 566, 395, 474, 425, 468, 558, 324, 567, 326, 591, 328, 592, 571, 337, 322, 314, 391, 421, 586, 453, 455, 508, 570, 491, 406, 501, 388, 378, 534, 452, 598, 385, 578, 315, 336, 581, 555, 413, 507, 334, 445, 529, 330, 340, 317, 604, 481], [89, 74, 153, 64, 296, 287, 286, 236, 126, 73, 20, 46, 160, 232, 181, 27, 173, 261, 37, 101, 166, 233, 184, 164, 206, 248, 253, 293, 76, 272, 305, 585, 304, 475, 476, 533, 548, 553, 344, 318, 393, 483, 532, 509, 540, 313, 430, 350, 526, 572, 460, 446, 398, 469, 537, 593, 358, 376, 519, 503])]\n",
      "Length of concatenated dataset: 546\n",
      "Length of concatenated dataset: 546\n",
      "Train Idx:\n",
      "[33, 190, 245, 129, 253, 21, 237, 146, 16, 204, 75, 173, 135, 209, 179, 4, 96, 188, 61, 67, 52, 66, 26, 110, 233, 124, 268, 40, 13, 107, 145, 166, 3, 159, 24, 30, 252, 213, 60, 56, 248, 157, 46, 19, 212, 153, 54, 227, 80, 51, 2, 196, 104, 150, 86, 10, 168, 58, 41, 14, 50, 194, 123, 62, 158, 187, 130, 246, 154, 43, 221, 138, 254, 181, 149, 112, 207, 98, 180, 93, 171, 162, 36, 113, 0, 94, 95, 223, 69, 49, 48, 85, 270, 141, 23, 249, 143, 78, 100, 131, 228, 271, 6, 68, 84, 170, 121, 140, 214, 240, 234, 239, 91, 241, 257, 11, 119, 102, 35, 57, 169, 65, 1, 120, 226, 186, 42, 105, 132, 244, 17, 38, 133, 53, 164, 217, 128, 34, 28, 183, 114, 203, 163, 151, 202, 31, 232, 127, 185, 250, 260, 32, 167, 142, 236, 147, 29, 177, 216, 99, 82, 269, 175, 79, 197, 243, 208, 115, 148, 266, 72, 77, 25, 165, 81, 261, 174, 263, 39, 193, 88, 70, 87, 242, 211, 9, 195, 251, 192, 117, 47, 172, 413, 531, 485, 274, 430, 285, 426, 404, 451, 455, 349, 453, 308, 375, 416, 480, 425, 382, 304, 504, 524, 513, 435, 283, 320, 488, 496, 479, 462, 338, 315, 276, 385, 406, 481, 309, 465, 460, 407, 282, 386, 448, 280, 391, 359, 444, 392, 475, 388, 541, 374, 370, 469, 327, 303, 439, 322, 373, 478, 499, 542, 329, 333, 279, 281, 352, 467, 540, 432, 507, 311, 438, 394, 483, 535, 502, 442, 397, 427, 353, 332, 500, 321, 326, 343, 484, 519, 328, 509, 431, 544, 307, 390, 534, 510, 495, 325, 347, 299, 446, 525, 440, 277, 412, 454, 538, 273, 278, 414, 334, 408, 523, 395, 295, 341, 293, 511, 287, 443, 305, 543, 501, 476, 337, 456, 498, 393, 354, 331, 286, 489, 527, 345, 420, 433, 360, 396, 520, 532, 457, 409, 324, 468, 464, 389, 297, 367, 437, 379, 289, 336, 487, 401, 459, 378, 482, 508, 301, 346, 492, 497, 515, 362, 474, 422, 447, 366, 314, 450, 356, 316, 365, 291, 472, 471, 418, 423, 312, 364, 384, 526, 317, 342, 275, 494, 419, 514, 302, 387, 298, 449, 441]\n",
      "Val Idx:\n",
      "[218, 259, 225, 111, 178, 161, 89, 220, 55, 198, 265, 267, 210, 126, 97, 15, 8, 152, 59, 238, 45, 116, 247, 160, 63, 7, 92, 235, 206, 255, 144, 118, 103, 191, 73, 137, 230, 83, 64, 258, 101, 200, 272, 106, 27, 224, 189, 182, 219, 37, 231, 136, 20, 76, 318, 284, 402, 330, 357, 411, 503, 410, 372, 344, 323, 436, 505, 351, 533, 400, 398, 491, 376, 355, 415, 516, 363, 545, 521, 434, 539, 428, 288, 466, 518, 371, 537, 290, 335, 403, 493, 340, 350, 522, 528, 405, 429, 473, 296, 445, 377, 517, 424, 348, 477, 319, 463, 529]\n",
      "Test Idx:\n",
      "[201, 229, 134, 176, 5, 22, 199, 184, 12, 122, 215, 74, 44, 222, 125, 156, 155, 108, 18, 139, 256, 90, 71, 109, 205, 262, 264, 486, 310, 294, 383, 292, 369, 421, 361, 300, 461, 458, 339, 490, 512, 506, 399, 452, 380, 470, 306, 368, 313, 530, 417, 536, 381, 358]\n",
      "[([33, 190, 245, 129, 253, 21, 237, 146, 16, 204, 75, 173, 135, 209, 179, 4, 96, 188, 61, 67, 52, 66, 26, 110, 233, 124, 268, 40, 13, 107, 145, 166, 3, 159, 24, 30, 252, 213, 60, 56, 248, 157, 46, 19, 212, 153, 54, 227, 80, 51, 2, 196, 104, 150, 86, 10, 168, 58, 41, 14, 50, 194, 123, 62, 158, 187, 130, 246, 154, 43, 221, 138, 254, 181, 149, 112, 207, 98, 180, 93, 171, 162, 36, 113, 0, 94, 95, 223, 69, 49, 48, 85, 270, 141, 23, 249, 143, 78, 100, 131, 228, 271, 6, 68, 84, 170, 121, 140, 214, 240, 234, 239, 91, 241, 257, 11, 119, 102, 35, 57, 169, 65, 1, 120, 226, 186, 42, 105, 132, 244, 17, 38, 133, 53, 164, 217, 128, 34, 28, 183, 114, 203, 163, 151, 202, 31, 232, 127, 185, 250, 260, 32, 167, 142, 236, 147, 29, 177, 216, 99, 82, 269, 175, 79, 197, 243, 208, 115, 148, 266, 72, 77, 25, 165, 81, 261, 174, 263, 39, 193, 88, 70, 87, 242, 211, 9, 195, 251, 192, 117, 47, 172, 413, 531, 485, 274, 430, 285, 426, 404, 451, 455, 349, 453, 308, 375, 416, 480, 425, 382, 304, 504, 524, 513, 435, 283, 320, 488, 496, 479, 462, 338, 315, 276, 385, 406, 481, 309, 465, 460, 407, 282, 386, 448, 280, 391, 359, 444, 392, 475, 388, 541, 374, 370, 469, 327, 303, 439, 322, 373, 478, 499, 542, 329, 333, 279, 281, 352, 467, 540, 432, 507, 311, 438, 394, 483, 535, 502, 442, 397, 427, 353, 332, 500, 321, 326, 343, 484, 519, 328, 509, 431, 544, 307, 390, 534, 510, 495, 325, 347, 299, 446, 525, 440, 277, 412, 454, 538, 273, 278, 414, 334, 408, 523, 395, 295, 341, 293, 511, 287, 443, 305, 543, 501, 476, 337, 456, 498, 393, 354, 331, 286, 489, 527, 345, 420, 433, 360, 396, 520, 532, 457, 409, 324, 468, 464, 389, 297, 367, 437, 379, 289, 336, 487, 401, 459, 378, 482, 508, 301, 346, 492, 497, 515, 362, 474, 422, 447, 366, 314, 450, 356, 316, 365, 291, 472, 471, 418, 423, 312, 364, 384, 526, 317, 342, 275, 494, 419, 514, 302, 387, 298, 449, 441], [218, 259, 225, 111, 178, 161, 89, 220, 55, 198, 265, 267, 210, 126, 97, 15, 8, 152, 59, 238, 45, 116, 247, 160, 63, 7, 92, 235, 206, 255, 144, 118, 103, 191, 73, 137, 230, 83, 64, 258, 101, 200, 272, 106, 27, 224, 189, 182, 219, 37, 231, 136, 20, 76, 318, 284, 402, 330, 357, 411, 503, 410, 372, 344, 323, 436, 505, 351, 533, 400, 398, 491, 376, 355, 415, 516, 363, 545, 521, 434, 539, 428, 288, 466, 518, 371, 537, 290, 335, 403, 493, 340, 350, 522, 528, 405, 429, 473, 296, 445, 377, 517, 424, 348, 477, 319, 463, 529], [201, 229, 134, 176, 5, 22, 199, 184, 12, 122, 215, 74, 44, 222, 125, 156, 155, 108, 18, 139, 256, 90, 71, 109, 205, 262, 264, 486, 310, 294, 383, 292, 369, 421, 361, 300, 461, 458, 339, 490, 512, 506, 399, 452, 380, 470, 306, 368, 313, 530, 417, 536, 381, 358])]\n",
      "Length of concatenated dataset: 546\n",
      "Length of concatenated dataset: 546\n",
      "Train Idx:\n",
      "[33, 190, 245, 129, 253, 21, 237, 146, 16, 204, 75, 173, 135, 209, 179, 4, 96, 188, 61, 67, 52, 66, 26, 110, 233, 124, 268, 40, 13, 107, 145, 166, 3, 159, 24, 30, 252, 213, 60, 56, 248, 157, 46, 19, 212, 153, 54, 227, 80, 51, 2, 196, 104, 150, 86, 10, 168, 58, 41, 14, 50, 194, 123, 62, 158, 187, 130, 246, 154, 43, 221, 138, 254, 181, 149, 112, 207, 98, 180, 93, 171, 162, 36, 113, 0, 94, 95, 223, 69, 49, 48, 85, 270, 141, 23, 249, 143, 78, 100, 131, 228, 271, 6, 68, 84, 170, 121, 140, 214, 240, 234, 239, 91, 241, 257, 11, 119, 102, 35, 57, 169, 65, 1, 120, 226, 186, 42, 105, 132, 244, 17, 38, 133, 53, 164, 217, 128, 34, 28, 183, 114, 203, 163, 151, 202, 31, 232, 127, 185, 250, 260, 32, 167, 142, 236, 147, 29, 177, 216, 99, 82, 269, 175, 79, 197, 243, 208, 115, 148, 266, 72, 77, 25, 165, 81, 261, 174, 263, 39, 193, 88, 70, 87, 242, 211, 9, 195, 251, 192, 117, 47, 172, 413, 531, 485, 274, 430, 285, 426, 404, 451, 455, 349, 453, 308, 375, 416, 480, 425, 382, 304, 504, 524, 513, 435, 283, 320, 488, 496, 479, 462, 338, 315, 276, 385, 406, 481, 309, 465, 460, 407, 282, 386, 448, 280, 391, 359, 444, 392, 475, 388, 541, 374, 370, 469, 327, 303, 439, 322, 373, 478, 499, 542, 329, 333, 279, 281, 352, 467, 540, 432, 507, 311, 438, 394, 483, 535, 502, 442, 397, 427, 353, 332, 500, 321, 326, 343, 484, 519, 328, 509, 431, 544, 307, 390, 534, 510, 495, 325, 347, 299, 446, 525, 440, 277, 412, 454, 538, 273, 278, 414, 334, 408, 523, 395, 295, 341, 293, 511, 287, 443, 305, 543, 501, 476, 337, 456, 498, 393, 354, 331, 286, 489, 527, 345, 420, 433, 360, 396, 520, 532, 457, 409, 324, 468, 464, 389, 297, 367, 437, 379, 289, 336, 487, 401, 459, 378, 482, 508, 301, 346, 492, 497, 515, 362, 474, 422, 447, 366, 314, 450, 356, 316, 365, 291, 472, 471, 418, 423, 312, 364, 384, 526, 317, 342, 275, 494, 419, 514, 302, 387, 298, 449, 441]\n",
      "Val Idx:\n",
      "[218, 259, 225, 111, 178, 161, 89, 220, 55, 198, 265, 267, 210, 126, 97, 15, 8, 152, 59, 238, 45, 116, 247, 160, 63, 7, 92, 235, 206, 255, 144, 118, 103, 191, 73, 137, 230, 83, 64, 258, 101, 200, 272, 106, 27, 224, 189, 182, 219, 37, 231, 136, 20, 76, 318, 284, 402, 330, 357, 411, 503, 410, 372, 344, 323, 436, 505, 351, 533, 400, 398, 491, 376, 355, 415, 516, 363, 545, 521, 434, 539, 428, 288, 466, 518, 371, 537, 290, 335, 403, 493, 340, 350, 522, 528, 405, 429, 473, 296, 445, 377, 517, 424, 348, 477, 319, 463, 529]\n",
      "Test Idx:\n",
      "[201, 229, 134, 176, 5, 22, 199, 184, 12, 122, 215, 74, 44, 222, 125, 156, 155, 108, 18, 139, 256, 90, 71, 109, 205, 262, 264, 486, 310, 294, 383, 292, 369, 421, 361, 300, 461, 458, 339, 490, 512, 506, 399, 452, 380, 470, 306, 368, 313, 530, 417, 536, 381, 358]\n",
      "[([33, 190, 245, 129, 253, 21, 237, 146, 16, 204, 75, 173, 135, 209, 179, 4, 96, 188, 61, 67, 52, 66, 26, 110, 233, 124, 268, 40, 13, 107, 145, 166, 3, 159, 24, 30, 252, 213, 60, 56, 248, 157, 46, 19, 212, 153, 54, 227, 80, 51, 2, 196, 104, 150, 86, 10, 168, 58, 41, 14, 50, 194, 123, 62, 158, 187, 130, 246, 154, 43, 221, 138, 254, 181, 149, 112, 207, 98, 180, 93, 171, 162, 36, 113, 0, 94, 95, 223, 69, 49, 48, 85, 270, 141, 23, 249, 143, 78, 100, 131, 228, 271, 6, 68, 84, 170, 121, 140, 214, 240, 234, 239, 91, 241, 257, 11, 119, 102, 35, 57, 169, 65, 1, 120, 226, 186, 42, 105, 132, 244, 17, 38, 133, 53, 164, 217, 128, 34, 28, 183, 114, 203, 163, 151, 202, 31, 232, 127, 185, 250, 260, 32, 167, 142, 236, 147, 29, 177, 216, 99, 82, 269, 175, 79, 197, 243, 208, 115, 148, 266, 72, 77, 25, 165, 81, 261, 174, 263, 39, 193, 88, 70, 87, 242, 211, 9, 195, 251, 192, 117, 47, 172, 413, 531, 485, 274, 430, 285, 426, 404, 451, 455, 349, 453, 308, 375, 416, 480, 425, 382, 304, 504, 524, 513, 435, 283, 320, 488, 496, 479, 462, 338, 315, 276, 385, 406, 481, 309, 465, 460, 407, 282, 386, 448, 280, 391, 359, 444, 392, 475, 388, 541, 374, 370, 469, 327, 303, 439, 322, 373, 478, 499, 542, 329, 333, 279, 281, 352, 467, 540, 432, 507, 311, 438, 394, 483, 535, 502, 442, 397, 427, 353, 332, 500, 321, 326, 343, 484, 519, 328, 509, 431, 544, 307, 390, 534, 510, 495, 325, 347, 299, 446, 525, 440, 277, 412, 454, 538, 273, 278, 414, 334, 408, 523, 395, 295, 341, 293, 511, 287, 443, 305, 543, 501, 476, 337, 456, 498, 393, 354, 331, 286, 489, 527, 345, 420, 433, 360, 396, 520, 532, 457, 409, 324, 468, 464, 389, 297, 367, 437, 379, 289, 336, 487, 401, 459, 378, 482, 508, 301, 346, 492, 497, 515, 362, 474, 422, 447, 366, 314, 450, 356, 316, 365, 291, 472, 471, 418, 423, 312, 364, 384, 526, 317, 342, 275, 494, 419, 514, 302, 387, 298, 449, 441], [218, 259, 225, 111, 178, 161, 89, 220, 55, 198, 265, 267, 210, 126, 97, 15, 8, 152, 59, 238, 45, 116, 247, 160, 63, 7, 92, 235, 206, 255, 144, 118, 103, 191, 73, 137, 230, 83, 64, 258, 101, 200, 272, 106, 27, 224, 189, 182, 219, 37, 231, 136, 20, 76, 318, 284, 402, 330, 357, 411, 503, 410, 372, 344, 323, 436, 505, 351, 533, 400, 398, 491, 376, 355, 415, 516, 363, 545, 521, 434, 539, 428, 288, 466, 518, 371, 537, 290, 335, 403, 493, 340, 350, 522, 528, 405, 429, 473, 296, 445, 377, 517, 424, 348, 477, 319, 463, 529], [201, 229, 134, 176, 5, 22, 199, 184, 12, 122, 215, 74, 44, 222, 125, 156, 155, 108, 18, 139, 256, 90, 71, 109, 205, 262, 264, 486, 310, 294, 383, 292, 369, 421, 361, 300, 461, 458, 339, 490, 512, 506, 399, 452, 380, 470, 306, 368, 313, 530, 417, 536, 381, 358])]\n",
      "Length of concatenated dataset: 141\n",
      "Length of concatenated dataset: 141\n",
      "Train Idx:\n",
      "[112, 78, 132, 68, 93, 85, 48, 135, 13, 92, 95, 73, 119, 15, 116, 40, 62, 128, 138, 3, 52, 63, 113, 6, 139, 12, 86, 104, 109, 127, 11, 94, 110, 41, 101, 1, 97, 130, 42, 4, 114, 17, 38, 5, 53, 134, 89, 0, 34, 28, 55, 75, 35, 23, 74, 31, 102, 57, 120, 65, 32, 129, 14, 106, 19, 29, 49, 126, 99, 82, 64, 140, 79, 69, 118, 80, 115, 20, 136, 72, 77, 25, 37, 81, 131, 46, 133, 39, 58, 88, 70, 87, 36, 21, 9, 103, 67, 117, 47]\n",
      "Val Idx:\n",
      "[45, 60, 7, 51, 66, 27, 71, 54, 123, 8, 76, 16, 125, 122, 124, 98, 105, 83, 33, 56, 91, 22, 137, 24, 2, 111, 26, 121]\n",
      "Test Idx:\n",
      "[18, 10, 96, 43, 100, 108, 50, 84, 61, 107, 90, 59, 44, 30]\n",
      "[([112, 78, 132, 68, 93, 85, 48, 135, 13, 92, 95, 73, 119, 15, 116, 40, 62, 128, 138, 3, 52, 63, 113, 6, 139, 12, 86, 104, 109, 127, 11, 94, 110, 41, 101, 1, 97, 130, 42, 4, 114, 17, 38, 5, 53, 134, 89, 0, 34, 28, 55, 75, 35, 23, 74, 31, 102, 57, 120, 65, 32, 129, 14, 106, 19, 29, 49, 126, 99, 82, 64, 140, 79, 69, 118, 80, 115, 20, 136, 72, 77, 25, 37, 81, 131, 46, 133, 39, 58, 88, 70, 87, 36, 21, 9, 103, 67, 117, 47], [45, 60, 7, 51, 66, 27, 71, 54, 123, 8, 76, 16, 125, 122, 124, 98, 105, 83, 33, 56, 91, 22, 137, 24, 2, 111, 26, 121], [18, 10, 96, 43, 100, 108, 50, 84, 61, 107, 90, 59, 44, 30])]\n",
      "Length of concatenated dataset: 1092\n",
      "Length of concatenated dataset: 1092\n",
      "Train Idx:\n",
      "[386, 463, 206, 38, 188, 361, 339, 272, 289, 437, 462, 124, 154, 526, 59, 252, 466, 491, 541, 235, 211, 420, 55, 171, 233, 205, 158, 385, 34, 367, 18, 506, 247, 529, 52, 74, 26, 173, 92, 167, 4, 435, 181, 246, 443, 5, 141, 301, 200, 135, 263, 122, 22, 68, 210, 20, 306, 336, 14, 170, 374, 281, 271, 220, 537, 64, 520, 250, 120, 81, 421, 500, 240, 308, 534, 528, 160, 536, 238, 497, 484, 392, 478, 465, 51, 195, 191, 116, 342, 395, 164, 106, 372, 63, 511, 284, 519, 445, 316, 93, 366, 332, 198, 145, 150, 39, 495, 439, 253, 344, 464, 2, 221, 514, 146, 241, 538, 114, 391, 176, 168, 515, 346, 513, 136, 424, 254, 476, 299, 457, 432, 255, 232, 133, 33, 88, 290, 44, 61, 199, 505, 525, 544, 340, 218, 302, 297, 73, 295, 35, 467, 29, 427, 446, 412, 309, 523, 217, 27, 469, 347, 156, 493, 409, 138, 212, 104, 414, 410, 416, 215, 521, 189, 214, 501, 204, 234, 259, 67, 24, 216, 300, 223, 129, 111, 166, 498, 470, 40, 274, 422, 79, 403, 468, 327, 13, 287, 326, 489, 251, 228, 415, 161, 83, 117, 25, 110, 149, 152, 16, 402, 331, 109, 417, 139, 237, 260, 352, 527, 317, 323, 477, 248, 389, 479, 19, 328, 296, 447, 269, 363, 226, 458, 3, 413, 125, 280, 286, 77, 184, 371, 461, 535, 275, 294, 517, 182, 32, 80, 307, 258, 11, 360, 440, 86, 266, 36, 193, 58, 41, 270, 411, 50, 209, 474, 473, 496, 123, 222, 419, 62, 449, 377, 130, 187, 23, 444, 43, 370, 394, 0, 383, 201, 405, 368, 522, 98, 387, 349, 304, 418, 292, 178, 369, 256, 94, 197, 95, 531, 163, 169, 69, 305, 48, 341, 373, 397, 207, 279, 509, 227, 148, 143, 334, 180, 356, 460, 131, 127, 47, 452, 262, 324, 203, 84, 426, 121, 539, 516, 530, 398, 384, 91, 82, 430, 267, 119, 358, 291, 57, 425, 487, 321, 257, 376, 31, 442, 42, 105, 388, 335, 273, 488, 53, 518, 128, 28, 183, 459, 510, 151, 244, 265, 288, 423, 147, 177, 99, 448, 431, 115, 72, 174, 87, 486, 314, 396, 472, 70, 277, 9, 359, 192, 1064, 1012, 998, 663, 1010, 712, 953, 551, 937, 791, 548, 866, 748, 628, 786, 790, 821, 572, 1052, 1049, 1007, 976, 685, 1070, 568, 693, 964, 982, 589, 799, 1041, 587, 794, 680, 635, 842, 737, 657, 851, 988, 898, 746, 569, 611, 1072, 1027, 930, 562, 672, 829, 640, 575, 800, 766, 967, 1063, 734, 854, 715, 844, 606, 760, 907, 947, 1087, 652, 694, 550, 793, 576, 843, 714, 816, 911, 1037, 879, 675, 1055, 987, 1019, 755, 642, 554, 653, 565, 647, 863, 874, 939, 1051, 658, 951, 968, 965, 709, 600, 940, 557, 660, 868, 1078, 833, 1089, 881, 553, 774, 1025, 776, 870, 736, 1033, 691, 855, 801, 1057, 1068, 960, 690, 607, 974, 798, 753, 761, 1043, 626, 952, 1003, 841, 1004, 700, 807, 999, 629, 838, 696, 639, 869, 991, 956, 641, 659, 605, 583, 689, 704, 586, 969, 941, 886, 759, 897, 782, 638, 932, 559, 958, 1077, 695, 972, 1036, 608, 625, 845, 739, 1001, 727, 1013, 989, 555, 996, 980, 1006, 853, 1085, 877, 950, 808, 552, 812, 803, 584, 697, 579, 1005, 817, 686, 1029, 740, 1061, 674, 1075, 1071, 957, 1031, 596, 708, 901, 1090, 914, 1032, 546, 856, 1038, 757, 718, 741, 977, 1008, 830, 669, 780, 585, 820, 1066, 702, 768, 822, 756, 890, 852, 763, 645, 835, 924, 713, 771, 650, 995, 556, 651, 614, 636, 1067, 919, 926, 1028, 779, 910, 918, 804, 811, 648, 850, 726, 889, 574, 588, 754, 566, 921, 1076, 630, 1044, 1073, 831, 1045, 617, 595, 725, 1069, 973, 662, 839, 622, 594, 1015, 809, 806, 692, 619, 992, 878, 610, 656, 598, 909, 1083, 955, 670, 633, 661, 1074, 834, 920, 848, 664, 558, 703, 883, 963, 673, 592, 781, 983, 681, 787, 899, 948, 707, 1056, 985, 723, 604, 710, 970, 871, 677, 698, 896, 993, 1023, 563, 934, 732, 743, 884, 752, 1080, 933, 891, 1021, 859, 788, 1079, 997, 716, 733, 593, 679, 847, 745, 618, 864, 624, 984, 620, 892, 942, 744, 935, 931, 929, 711, 665, 1040, 699, 946, 1016, 775, 975, 567, 827, 938, 796, 789, 783, 581, 1053, 1088, 875, 655, 925, 717, 784, 549, 836, 994, 867, 666, 701, 684, 671, 1002, 612, 720, 1054, 724, 767, 961, 646, 792, 765, 880]\n",
      "Val Idx:\n",
      "[85, 436, 96, 186, 134, 37, 450, 90, 502, 179, 140, 429, 66, 325, 285, 330, 196, 393, 137, 298, 407, 337, 503, 278, 230, 320, 175, 338, 162, 142, 451, 311, 261, 485, 113, 225, 242, 243, 15, 155, 380, 283, 17, 303, 475, 355, 353, 45, 532, 365, 456, 185, 406, 504, 512, 345, 438, 249, 433, 540, 224, 348, 108, 89, 401, 46, 102, 239, 21, 107, 315, 208, 103, 71, 75, 313, 78, 378, 357, 441, 379, 322, 310, 190, 236, 480, 219, 318, 381, 351, 157, 159, 276, 481, 492, 153, 268, 350, 1, 172, 12, 132, 390, 453, 434, 483, 245, 76, 229, 959, 1030, 887, 922, 876, 573, 917, 905, 865, 1039, 1062, 1058, 1020, 893, 1014, 772, 560, 903, 582, 637, 885, 1017, 826, 916, 1048, 615, 810, 928, 966, 1060, 616, 778, 872, 923, 764, 900, 936, 979, 1050, 735, 1091, 861, 832, 1035, 577, 873, 912, 578, 777, 986, 795, 621, 762, 785, 590, 602, 944, 742, 705, 631, 906, 580, 888, 846, 1082, 682, 773, 815, 1011, 1047, 915, 1046, 849, 1022, 688, 913, 721, 981, 954, 599, 564, 547, 758, 609, 769, 840, 818, 978, 728, 882, 654, 908, 747, 837, 927, 722, 824, 814, 862, 603, 632, 797, 1084, 949, 945, 613, 683, 601, 667]\n",
      "Test Idx:\n",
      "[312, 118, 400, 543, 202, 165, 508, 454, 264, 524, 455, 329, 375, 10, 482, 112, 282, 343, 293, 542, 65, 404, 126, 490, 545, 231, 213, 408, 194, 7, 507, 499, 494, 533, 471, 101, 97, 382, 54, 30, 364, 49, 100, 428, 362, 319, 399, 56, 144, 60, 6, 8, 354, 333, 823, 1024, 749, 813, 649, 1081, 1065, 1042, 561, 802, 904, 719, 770, 1000, 1026, 962, 805, 751, 943, 597, 729, 706, 902, 857, 591, 1018, 731, 571, 894, 738, 643, 1009, 750, 860, 895, 668, 1059, 644, 627, 687, 730, 623, 828, 990, 676, 858, 570, 678, 819, 971, 825, 634, 1086, 1034]\n",
      "[([386, 463, 206, 38, 188, 361, 339, 272, 289, 437, 462, 124, 154, 526, 59, 252, 466, 491, 541, 235, 211, 420, 55, 171, 233, 205, 158, 385, 34, 367, 18, 506, 247, 529, 52, 74, 26, 173, 92, 167, 4, 435, 181, 246, 443, 5, 141, 301, 200, 135, 263, 122, 22, 68, 210, 20, 306, 336, 14, 170, 374, 281, 271, 220, 537, 64, 520, 250, 120, 81, 421, 500, 240, 308, 534, 528, 160, 536, 238, 497, 484, 392, 478, 465, 51, 195, 191, 116, 342, 395, 164, 106, 372, 63, 511, 284, 519, 445, 316, 93, 366, 332, 198, 145, 150, 39, 495, 439, 253, 344, 464, 2, 221, 514, 146, 241, 538, 114, 391, 176, 168, 515, 346, 513, 136, 424, 254, 476, 299, 457, 432, 255, 232, 133, 33, 88, 290, 44, 61, 199, 505, 525, 544, 340, 218, 302, 297, 73, 295, 35, 467, 29, 427, 446, 412, 309, 523, 217, 27, 469, 347, 156, 493, 409, 138, 212, 104, 414, 410, 416, 215, 521, 189, 214, 501, 204, 234, 259, 67, 24, 216, 300, 223, 129, 111, 166, 498, 470, 40, 274, 422, 79, 403, 468, 327, 13, 287, 326, 489, 251, 228, 415, 161, 83, 117, 25, 110, 149, 152, 16, 402, 331, 109, 417, 139, 237, 260, 352, 527, 317, 323, 477, 248, 389, 479, 19, 328, 296, 447, 269, 363, 226, 458, 3, 413, 125, 280, 286, 77, 184, 371, 461, 535, 275, 294, 517, 182, 32, 80, 307, 258, 11, 360, 440, 86, 266, 36, 193, 58, 41, 270, 411, 50, 209, 474, 473, 496, 123, 222, 419, 62, 449, 377, 130, 187, 23, 444, 43, 370, 394, 0, 383, 201, 405, 368, 522, 98, 387, 349, 304, 418, 292, 178, 369, 256, 94, 197, 95, 531, 163, 169, 69, 305, 48, 341, 373, 397, 207, 279, 509, 227, 148, 143, 334, 180, 356, 460, 131, 127, 47, 452, 262, 324, 203, 84, 426, 121, 539, 516, 530, 398, 384, 91, 82, 430, 267, 119, 358, 291, 57, 425, 487, 321, 257, 376, 31, 442, 42, 105, 388, 335, 273, 488, 53, 518, 128, 28, 183, 459, 510, 151, 244, 265, 288, 423, 147, 177, 99, 448, 431, 115, 72, 174, 87, 486, 314, 396, 472, 70, 277, 9, 359, 192, 1064, 1012, 998, 663, 1010, 712, 953, 551, 937, 791, 548, 866, 748, 628, 786, 790, 821, 572, 1052, 1049, 1007, 976, 685, 1070, 568, 693, 964, 982, 589, 799, 1041, 587, 794, 680, 635, 842, 737, 657, 851, 988, 898, 746, 569, 611, 1072, 1027, 930, 562, 672, 829, 640, 575, 800, 766, 967, 1063, 734, 854, 715, 844, 606, 760, 907, 947, 1087, 652, 694, 550, 793, 576, 843, 714, 816, 911, 1037, 879, 675, 1055, 987, 1019, 755, 642, 554, 653, 565, 647, 863, 874, 939, 1051, 658, 951, 968, 965, 709, 600, 940, 557, 660, 868, 1078, 833, 1089, 881, 553, 774, 1025, 776, 870, 736, 1033, 691, 855, 801, 1057, 1068, 960, 690, 607, 974, 798, 753, 761, 1043, 626, 952, 1003, 841, 1004, 700, 807, 999, 629, 838, 696, 639, 869, 991, 956, 641, 659, 605, 583, 689, 704, 586, 969, 941, 886, 759, 897, 782, 638, 932, 559, 958, 1077, 695, 972, 1036, 608, 625, 845, 739, 1001, 727, 1013, 989, 555, 996, 980, 1006, 853, 1085, 877, 950, 808, 552, 812, 803, 584, 697, 579, 1005, 817, 686, 1029, 740, 1061, 674, 1075, 1071, 957, 1031, 596, 708, 901, 1090, 914, 1032, 546, 856, 1038, 757, 718, 741, 977, 1008, 830, 669, 780, 585, 820, 1066, 702, 768, 822, 756, 890, 852, 763, 645, 835, 924, 713, 771, 650, 995, 556, 651, 614, 636, 1067, 919, 926, 1028, 779, 910, 918, 804, 811, 648, 850, 726, 889, 574, 588, 754, 566, 921, 1076, 630, 1044, 1073, 831, 1045, 617, 595, 725, 1069, 973, 662, 839, 622, 594, 1015, 809, 806, 692, 619, 992, 878, 610, 656, 598, 909, 1083, 955, 670, 633, 661, 1074, 834, 920, 848, 664, 558, 703, 883, 963, 673, 592, 781, 983, 681, 787, 899, 948, 707, 1056, 985, 723, 604, 710, 970, 871, 677, 698, 896, 993, 1023, 563, 934, 732, 743, 884, 752, 1080, 933, 891, 1021, 859, 788, 1079, 997, 716, 733, 593, 679, 847, 745, 618, 864, 624, 984, 620, 892, 942, 744, 935, 931, 929, 711, 665, 1040, 699, 946, 1016, 775, 975, 567, 827, 938, 796, 789, 783, 581, 1053, 1088, 875, 655, 925, 717, 784, 549, 836, 994, 867, 666, 701, 684, 671, 1002, 612, 720, 1054, 724, 767, 961, 646, 792, 765, 880], [85, 436, 96, 186, 134, 37, 450, 90, 502, 179, 140, 429, 66, 325, 285, 330, 196, 393, 137, 298, 407, 337, 503, 278, 230, 320, 175, 338, 162, 142, 451, 311, 261, 485, 113, 225, 242, 243, 15, 155, 380, 283, 17, 303, 475, 355, 353, 45, 532, 365, 456, 185, 406, 504, 512, 345, 438, 249, 433, 540, 224, 348, 108, 89, 401, 46, 102, 239, 21, 107, 315, 208, 103, 71, 75, 313, 78, 378, 357, 441, 379, 322, 310, 190, 236, 480, 219, 318, 381, 351, 157, 159, 276, 481, 492, 153, 268, 350, 1, 172, 12, 132, 390, 453, 434, 483, 245, 76, 229, 959, 1030, 887, 922, 876, 573, 917, 905, 865, 1039, 1062, 1058, 1020, 893, 1014, 772, 560, 903, 582, 637, 885, 1017, 826, 916, 1048, 615, 810, 928, 966, 1060, 616, 778, 872, 923, 764, 900, 936, 979, 1050, 735, 1091, 861, 832, 1035, 577, 873, 912, 578, 777, 986, 795, 621, 762, 785, 590, 602, 944, 742, 705, 631, 906, 580, 888, 846, 1082, 682, 773, 815, 1011, 1047, 915, 1046, 849, 1022, 688, 913, 721, 981, 954, 599, 564, 547, 758, 609, 769, 840, 818, 978, 728, 882, 654, 908, 747, 837, 927, 722, 824, 814, 862, 603, 632, 797, 1084, 949, 945, 613, 683, 601, 667], [312, 118, 400, 543, 202, 165, 508, 454, 264, 524, 455, 329, 375, 10, 482, 112, 282, 343, 293, 542, 65, 404, 126, 490, 545, 231, 213, 408, 194, 7, 507, 499, 494, 533, 471, 101, 97, 382, 54, 30, 364, 49, 100, 428, 362, 319, 399, 56, 144, 60, 6, 8, 354, 333, 823, 1024, 749, 813, 649, 1081, 1065, 1042, 561, 802, 904, 719, 770, 1000, 1026, 962, 805, 751, 943, 597, 729, 706, 902, 857, 591, 1018, 731, 571, 894, 738, 643, 1009, 750, 860, 895, 668, 1059, 644, 627, 687, 730, 623, 828, 990, 676, 858, 570, 678, 819, 971, 825, 634, 1086, 1034])]\n",
      "2022-08-09 19:35:24 - (M) Train validation test splitted: 426 120 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-09 19:35:24 - (N) Train validation test splitted: 384 108 54\n",
      "2022-08-09 19:35:24 - (F) Train validation test splitted: 384 108 54\n",
      "2022-08-09 19:35:24 - (H) Train validation test splitted: 99 28 14\n",
      "2022-08-09 19:35:24 - (FN) Train validation test splitted: 766 218 108\n"
     ]
    }
   ],
   "source": [
    "fold = 1 #5\n",
    "mri_type = \"T1wCE\"\n",
    "batch_size = 1\n",
    "epochs = 15\n",
    "train_origins = [\"f\"]#[\"m\",\"n\",\"fn\",\"f\"]\n",
    "packs = generate_datasets([mri_type])\n",
    "loader_packs = get_loaders(packs, batch_size)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "#print(loader_packs['KLF']['m_dataloaders'])\n",
    "\n",
    "m_dataloader = loader_packs[mri_type]['m_dataloaders']#m_dataloaders[0]\n",
    "# Competition Train\n",
    "m_train_loader = m_dataloader[0]\n",
    "# Competition Val\n",
    "m_val_loader = m_dataloader[1]\n",
    "# Competition Test\n",
    "m_test_loader = m_dataloader[2]\n",
    "\n",
    "#t_dataloader = loader_packs[selected_type]['t_dataloaders']#t_dataloaders[0]\n",
    "# External Train\n",
    "#t_train_loader = t_dataloader[0]\n",
    "# External Val\n",
    "#t_val_loader = t_dataloader[1]\n",
    "# External Test\n",
    "#t_test_loader = t_dataloader[2]\n",
    "\n",
    "n_dataloader = loader_packs[mri_type]['n_dataloaders']#n_dataloaders[0]\n",
    "# UPENN Train\n",
    "n_train_loader = n_dataloader[0]\n",
    "# UPENN Val\n",
    "n_val_loader = n_dataloader[1]\n",
    "# UPENN Test\n",
    "n_test_loader = n_dataloader[2]\n",
    "\n",
    "#if selected_type == \"KLF\":\n",
    "f_dataloader = loader_packs[mri_type]['f_dataloaders']#f_dataloaders[0]\n",
    "# Competition (Tumor Only) Train\n",
    "f_train_loader = f_dataloader[0]\n",
    "# Competition (Tumor Only) Val\n",
    "f_val_loader = f_dataloader[1]\n",
    "# Competition (Tumor Only) Test\n",
    "f_test_loader = f_dataloader[2]\n",
    "\n",
    "h_dataloader = loader_packs[mri_type]['h_dataloaders']#h_dataloaders[0]\n",
    "# Competition (No Tumor) Train\n",
    "h_train_loader = h_dataloader[0]\n",
    "# Competition (No Tumor) Val\n",
    "h_val_loader = h_dataloader[1]\n",
    "# Competition (No Tumor) Test\n",
    "h_test_loader = h_dataloader[2]\n",
    "    \n",
    "fn_dataloader = loader_packs[mri_type]['fn_dataloaders']#fn_dataloaders[0]\n",
    "# Competition (Tumor Only) + UPENN Train\n",
    "fn_train_loader = fn_dataloader[0]\n",
    "# Competition (Tumor Only) + UPENN Val\n",
    "fn_val_loader = fn_dataloader[1]\n",
    "# Competition (Tumor Only) + UPENN Test\n",
    "fn_test_loader = fn_dataloader[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_T1wCE_1\n",
      "Type: T1wCE\n",
      "Length of concatenated dataset: 606\n",
      "Length of concatenated dataset: 606\n",
      "Train Idx:\n",
      "[238, 137, 106, 284, 44, 139, 247, 288, 156, 297, 252, 54, 234, 18, 205, 254, 182, 56, 71, 144, 249, 209, 290, 219, 158, 176, 33, 83, 136, 210, 118, 60, 159, 282, 110, 21, 29, 150, 16, 75, 109, 179, 283, 4, 96, 229, 61, 67, 295, 266, 171, 281, 40, 189, 13, 107, 200, 3, 161, 125, 24, 30, 77, 279, 190, 19, 257, 235, 268, 80, 51, 2, 239, 104, 262, 86, 10, 224, 58, 41, 14, 155, 50, 215, 237, 123, 220, 62, 191, 230, 130, 213, 187, 43, 114, 138, 199, 222, 149, 112, 298, 98, 221, 93, 208, 162, 36, 178, 113, 0, 94, 294, 95, 299, 263, 256, 69, 49, 48, 85, 300, 141, 207, 23, 250, 148, 143, 78, 180, 100, 204, 131, 269, 301, 196, 6, 68, 203, 84, 170, 121, 140, 258, 276, 142, 259, 91, 82, 285, 11, 119, 102, 35, 57, 169, 231, 65, 1, 120, 267, 186, 42, 105, 132, 79, 17, 271, 38, 53, 260, 128, 28, 183, 163, 151, 244, 202, 31, 32, 127, 185, 280, 273, 147, 278, 177, 99, 197, 243, 115, 265, 72, 25, 165, 289, 174, 291, 39, 193, 88, 70, 87, 292, 242, 277, 211, 9, 195, 251, 192, 117, 47, 172, 516, 380, 549, 365, 554, 536, 489, 433, 486, 369, 338, 320, 465, 589, 448, 528, 499, 348, 353, 360, 556, 547, 531, 544, 335, 381, 449, 436, 399, 517, 401, 505, 471, 392, 543, 561, 550, 435, 349, 500, 410, 473, 303, 542, 374, 579, 405, 370, 411, 525, 387, 584, 557, 428, 539, 412, 329, 484, 602, 545, 490, 437, 372, 402, 527, 521, 342, 496, 493, 432, 551, 443, 434, 564, 416, 321, 379, 495, 552, 414, 479, 312, 492, 462, 599, 346, 394, 407, 441, 478, 415, 371, 339, 513, 368, 345, 429, 306, 420, 451, 596, 580, 574, 431, 485, 440, 600, 310, 389, 482, 422, 569, 418, 464, 332, 458, 563, 488, 404, 400, 323, 456, 357, 333, 583, 352, 403, 565, 594, 454, 417, 359, 447, 363, 497, 535, 515, 502, 601, 309, 562, 311, 520, 382, 576, 511, 577, 546, 568, 470, 341, 373, 424, 582, 347, 603, 597, 480, 361, 466, 427, 457, 383, 362, 595, 351, 356, 461, 354, 575, 522, 377, 307, 442, 510, 590, 559, 308, 444, 364, 438, 386, 530, 325, 573, 523, 506, 423, 384, 316, 587, 375, 450, 463, 390, 541, 605, 439, 498, 494, 419, 327, 397, 467, 409, 319, 366, 408, 512, 331, 504, 477, 396]\n",
      "Val Idx:\n",
      "[225, 152, 228, 201, 52, 245, 175, 168, 223, 217, 111, 135, 218, 12, 15, 66, 97, 90, 198, 103, 22, 212, 226, 264, 133, 216, 275, 270, 154, 55, 194, 255, 134, 8, 157, 241, 240, 81, 214, 167, 5, 59, 92, 274, 34, 145, 116, 188, 246, 7, 45, 129, 122, 63, 124, 227, 146, 302, 26, 108, 560, 472, 367, 518, 487, 343, 514, 459, 588, 426, 524, 355, 538, 566, 395, 474, 425, 468, 558, 324, 567, 326, 591, 328, 592, 571, 337, 322, 314, 391, 421, 586, 453, 455, 508, 570, 491, 406, 501, 388, 378, 534, 452, 598, 385, 578, 315, 336, 581, 555, 413, 507, 334, 445, 529, 330, 340, 317, 604, 481]\n",
      "Test Idx:\n",
      "[89, 74, 153, 64, 296, 287, 286, 236, 126, 73, 20, 46, 160, 232, 181, 27, 173, 261, 37, 101, 166, 233, 184, 164, 206, 248, 253, 293, 76, 272, 305, 585, 304, 475, 476, 533, 548, 553, 344, 318, 393, 483, 532, 509, 540, 313, 430, 350, 526, 572, 460, 446, 398, 469, 537, 593, 358, 376, 519, 503]\n",
      "[([238, 137, 106, 284, 44, 139, 247, 288, 156, 297, 252, 54, 234, 18, 205, 254, 182, 56, 71, 144, 249, 209, 290, 219, 158, 176, 33, 83, 136, 210, 118, 60, 159, 282, 110, 21, 29, 150, 16, 75, 109, 179, 283, 4, 96, 229, 61, 67, 295, 266, 171, 281, 40, 189, 13, 107, 200, 3, 161, 125, 24, 30, 77, 279, 190, 19, 257, 235, 268, 80, 51, 2, 239, 104, 262, 86, 10, 224, 58, 41, 14, 155, 50, 215, 237, 123, 220, 62, 191, 230, 130, 213, 187, 43, 114, 138, 199, 222, 149, 112, 298, 98, 221, 93, 208, 162, 36, 178, 113, 0, 94, 294, 95, 299, 263, 256, 69, 49, 48, 85, 300, 141, 207, 23, 250, 148, 143, 78, 180, 100, 204, 131, 269, 301, 196, 6, 68, 203, 84, 170, 121, 140, 258, 276, 142, 259, 91, 82, 285, 11, 119, 102, 35, 57, 169, 231, 65, 1, 120, 267, 186, 42, 105, 132, 79, 17, 271, 38, 53, 260, 128, 28, 183, 163, 151, 244, 202, 31, 32, 127, 185, 280, 273, 147, 278, 177, 99, 197, 243, 115, 265, 72, 25, 165, 289, 174, 291, 39, 193, 88, 70, 87, 292, 242, 277, 211, 9, 195, 251, 192, 117, 47, 172, 516, 380, 549, 365, 554, 536, 489, 433, 486, 369, 338, 320, 465, 589, 448, 528, 499, 348, 353, 360, 556, 547, 531, 544, 335, 381, 449, 436, 399, 517, 401, 505, 471, 392, 543, 561, 550, 435, 349, 500, 410, 473, 303, 542, 374, 579, 405, 370, 411, 525, 387, 584, 557, 428, 539, 412, 329, 484, 602, 545, 490, 437, 372, 402, 527, 521, 342, 496, 493, 432, 551, 443, 434, 564, 416, 321, 379, 495, 552, 414, 479, 312, 492, 462, 599, 346, 394, 407, 441, 478, 415, 371, 339, 513, 368, 345, 429, 306, 420, 451, 596, 580, 574, 431, 485, 440, 600, 310, 389, 482, 422, 569, 418, 464, 332, 458, 563, 488, 404, 400, 323, 456, 357, 333, 583, 352, 403, 565, 594, 454, 417, 359, 447, 363, 497, 535, 515, 502, 601, 309, 562, 311, 520, 382, 576, 511, 577, 546, 568, 470, 341, 373, 424, 582, 347, 603, 597, 480, 361, 466, 427, 457, 383, 362, 595, 351, 356, 461, 354, 575, 522, 377, 307, 442, 510, 590, 559, 308, 444, 364, 438, 386, 530, 325, 573, 523, 506, 423, 384, 316, 587, 375, 450, 463, 390, 541, 605, 439, 498, 494, 419, 327, 397, 467, 409, 319, 366, 408, 512, 331, 504, 477, 396], [225, 152, 228, 201, 52, 245, 175, 168, 223, 217, 111, 135, 218, 12, 15, 66, 97, 90, 198, 103, 22, 212, 226, 264, 133, 216, 275, 270, 154, 55, 194, 255, 134, 8, 157, 241, 240, 81, 214, 167, 5, 59, 92, 274, 34, 145, 116, 188, 246, 7, 45, 129, 122, 63, 124, 227, 146, 302, 26, 108, 560, 472, 367, 518, 487, 343, 514, 459, 588, 426, 524, 355, 538, 566, 395, 474, 425, 468, 558, 324, 567, 326, 591, 328, 592, 571, 337, 322, 314, 391, 421, 586, 453, 455, 508, 570, 491, 406, 501, 388, 378, 534, 452, 598, 385, 578, 315, 336, 581, 555, 413, 507, 334, 445, 529, 330, 340, 317, 604, 481], [89, 74, 153, 64, 296, 287, 286, 236, 126, 73, 20, 46, 160, 232, 181, 27, 173, 261, 37, 101, 166, 233, 184, 164, 206, 248, 253, 293, 76, 272, 305, 585, 304, 475, 476, 533, 548, 553, 344, 318, 393, 483, 532, 509, 540, 313, 430, 350, 526, 572, 460, 446, 398, 469, 537, 593, 358, 376, 519, 503])]\n",
      "Length of concatenated dataset: 546\n",
      "Length of concatenated dataset: 546\n",
      "Train Idx:\n",
      "[33, 190, 245, 129, 253, 21, 237, 146, 16, 204, 75, 173, 135, 209, 179, 4, 96, 188, 61, 67, 52, 66, 26, 110, 233, 124, 268, 40, 13, 107, 145, 166, 3, 159, 24, 30, 252, 213, 60, 56, 248, 157, 46, 19, 212, 153, 54, 227, 80, 51, 2, 196, 104, 150, 86, 10, 168, 58, 41, 14, 50, 194, 123, 62, 158, 187, 130, 246, 154, 43, 221, 138, 254, 181, 149, 112, 207, 98, 180, 93, 171, 162, 36, 113, 0, 94, 95, 223, 69, 49, 48, 85, 270, 141, 23, 249, 143, 78, 100, 131, 228, 271, 6, 68, 84, 170, 121, 140, 214, 240, 234, 239, 91, 241, 257, 11, 119, 102, 35, 57, 169, 65, 1, 120, 226, 186, 42, 105, 132, 244, 17, 38, 133, 53, 164, 217, 128, 34, 28, 183, 114, 203, 163, 151, 202, 31, 232, 127, 185, 250, 260, 32, 167, 142, 236, 147, 29, 177, 216, 99, 82, 269, 175, 79, 197, 243, 208, 115, 148, 266, 72, 77, 25, 165, 81, 261, 174, 263, 39, 193, 88, 70, 87, 242, 211, 9, 195, 251, 192, 117, 47, 172, 413, 531, 485, 274, 430, 285, 426, 404, 451, 455, 349, 453, 308, 375, 416, 480, 425, 382, 304, 504, 524, 513, 435, 283, 320, 488, 496, 479, 462, 338, 315, 276, 385, 406, 481, 309, 465, 460, 407, 282, 386, 448, 280, 391, 359, 444, 392, 475, 388, 541, 374, 370, 469, 327, 303, 439, 322, 373, 478, 499, 542, 329, 333, 279, 281, 352, 467, 540, 432, 507, 311, 438, 394, 483, 535, 502, 442, 397, 427, 353, 332, 500, 321, 326, 343, 484, 519, 328, 509, 431, 544, 307, 390, 534, 510, 495, 325, 347, 299, 446, 525, 440, 277, 412, 454, 538, 273, 278, 414, 334, 408, 523, 395, 295, 341, 293, 511, 287, 443, 305, 543, 501, 476, 337, 456, 498, 393, 354, 331, 286, 489, 527, 345, 420, 433, 360, 396, 520, 532, 457, 409, 324, 468, 464, 389, 297, 367, 437, 379, 289, 336, 487, 401, 459, 378, 482, 508, 301, 346, 492, 497, 515, 362, 474, 422, 447, 366, 314, 450, 356, 316, 365, 291, 472, 471, 418, 423, 312, 364, 384, 526, 317, 342, 275, 494, 419, 514, 302, 387, 298, 449, 441]\n",
      "Val Idx:\n",
      "[218, 259, 225, 111, 178, 161, 89, 220, 55, 198, 265, 267, 210, 126, 97, 15, 8, 152, 59, 238, 45, 116, 247, 160, 63, 7, 92, 235, 206, 255, 144, 118, 103, 191, 73, 137, 230, 83, 64, 258, 101, 200, 272, 106, 27, 224, 189, 182, 219, 37, 231, 136, 20, 76, 318, 284, 402, 330, 357, 411, 503, 410, 372, 344, 323, 436, 505, 351, 533, 400, 398, 491, 376, 355, 415, 516, 363, 545, 521, 434, 539, 428, 288, 466, 518, 371, 537, 290, 335, 403, 493, 340, 350, 522, 528, 405, 429, 473, 296, 445, 377, 517, 424, 348, 477, 319, 463, 529]\n",
      "Test Idx:\n",
      "[201, 229, 134, 176, 5, 22, 199, 184, 12, 122, 215, 74, 44, 222, 125, 156, 155, 108, 18, 139, 256, 90, 71, 109, 205, 262, 264, 486, 310, 294, 383, 292, 369, 421, 361, 300, 461, 458, 339, 490, 512, 506, 399, 452, 380, 470, 306, 368, 313, 530, 417, 536, 381, 358]\n",
      "[([33, 190, 245, 129, 253, 21, 237, 146, 16, 204, 75, 173, 135, 209, 179, 4, 96, 188, 61, 67, 52, 66, 26, 110, 233, 124, 268, 40, 13, 107, 145, 166, 3, 159, 24, 30, 252, 213, 60, 56, 248, 157, 46, 19, 212, 153, 54, 227, 80, 51, 2, 196, 104, 150, 86, 10, 168, 58, 41, 14, 50, 194, 123, 62, 158, 187, 130, 246, 154, 43, 221, 138, 254, 181, 149, 112, 207, 98, 180, 93, 171, 162, 36, 113, 0, 94, 95, 223, 69, 49, 48, 85, 270, 141, 23, 249, 143, 78, 100, 131, 228, 271, 6, 68, 84, 170, 121, 140, 214, 240, 234, 239, 91, 241, 257, 11, 119, 102, 35, 57, 169, 65, 1, 120, 226, 186, 42, 105, 132, 244, 17, 38, 133, 53, 164, 217, 128, 34, 28, 183, 114, 203, 163, 151, 202, 31, 232, 127, 185, 250, 260, 32, 167, 142, 236, 147, 29, 177, 216, 99, 82, 269, 175, 79, 197, 243, 208, 115, 148, 266, 72, 77, 25, 165, 81, 261, 174, 263, 39, 193, 88, 70, 87, 242, 211, 9, 195, 251, 192, 117, 47, 172, 413, 531, 485, 274, 430, 285, 426, 404, 451, 455, 349, 453, 308, 375, 416, 480, 425, 382, 304, 504, 524, 513, 435, 283, 320, 488, 496, 479, 462, 338, 315, 276, 385, 406, 481, 309, 465, 460, 407, 282, 386, 448, 280, 391, 359, 444, 392, 475, 388, 541, 374, 370, 469, 327, 303, 439, 322, 373, 478, 499, 542, 329, 333, 279, 281, 352, 467, 540, 432, 507, 311, 438, 394, 483, 535, 502, 442, 397, 427, 353, 332, 500, 321, 326, 343, 484, 519, 328, 509, 431, 544, 307, 390, 534, 510, 495, 325, 347, 299, 446, 525, 440, 277, 412, 454, 538, 273, 278, 414, 334, 408, 523, 395, 295, 341, 293, 511, 287, 443, 305, 543, 501, 476, 337, 456, 498, 393, 354, 331, 286, 489, 527, 345, 420, 433, 360, 396, 520, 532, 457, 409, 324, 468, 464, 389, 297, 367, 437, 379, 289, 336, 487, 401, 459, 378, 482, 508, 301, 346, 492, 497, 515, 362, 474, 422, 447, 366, 314, 450, 356, 316, 365, 291, 472, 471, 418, 423, 312, 364, 384, 526, 317, 342, 275, 494, 419, 514, 302, 387, 298, 449, 441], [218, 259, 225, 111, 178, 161, 89, 220, 55, 198, 265, 267, 210, 126, 97, 15, 8, 152, 59, 238, 45, 116, 247, 160, 63, 7, 92, 235, 206, 255, 144, 118, 103, 191, 73, 137, 230, 83, 64, 258, 101, 200, 272, 106, 27, 224, 189, 182, 219, 37, 231, 136, 20, 76, 318, 284, 402, 330, 357, 411, 503, 410, 372, 344, 323, 436, 505, 351, 533, 400, 398, 491, 376, 355, 415, 516, 363, 545, 521, 434, 539, 428, 288, 466, 518, 371, 537, 290, 335, 403, 493, 340, 350, 522, 528, 405, 429, 473, 296, 445, 377, 517, 424, 348, 477, 319, 463, 529], [201, 229, 134, 176, 5, 22, 199, 184, 12, 122, 215, 74, 44, 222, 125, 156, 155, 108, 18, 139, 256, 90, 71, 109, 205, 262, 264, 486, 310, 294, 383, 292, 369, 421, 361, 300, 461, 458, 339, 490, 512, 506, 399, 452, 380, 470, 306, 368, 313, 530, 417, 536, 381, 358])]\n",
      "Length of concatenated dataset: 1092\n",
      "Length of concatenated dataset: 1092\n",
      "Train Idx:\n",
      "[386, 463, 206, 38, 188, 361, 339, 272, 289, 437, 462, 124, 154, 526, 59, 252, 466, 491, 541, 235, 211, 420, 55, 171, 233, 205, 158, 385, 34, 367, 18, 506, 247, 529, 52, 74, 26, 173, 92, 167, 4, 435, 181, 246, 443, 5, 141, 301, 200, 135, 263, 122, 22, 68, 210, 20, 306, 336, 14, 170, 374, 281, 271, 220, 537, 64, 520, 250, 120, 81, 421, 500, 240, 308, 534, 528, 160, 536, 238, 497, 484, 392, 478, 465, 51, 195, 191, 116, 342, 395, 164, 106, 372, 63, 511, 284, 519, 445, 316, 93, 366, 332, 198, 145, 150, 39, 495, 439, 253, 344, 464, 2, 221, 514, 146, 241, 538, 114, 391, 176, 168, 515, 346, 513, 136, 424, 254, 476, 299, 457, 432, 255, 232, 133, 33, 88, 290, 44, 61, 199, 505, 525, 544, 340, 218, 302, 297, 73, 295, 35, 467, 29, 427, 446, 412, 309, 523, 217, 27, 469, 347, 156, 493, 409, 138, 212, 104, 414, 410, 416, 215, 521, 189, 214, 501, 204, 234, 259, 67, 24, 216, 300, 223, 129, 111, 166, 498, 470, 40, 274, 422, 79, 403, 468, 327, 13, 287, 326, 489, 251, 228, 415, 161, 83, 117, 25, 110, 149, 152, 16, 402, 331, 109, 417, 139, 237, 260, 352, 527, 317, 323, 477, 248, 389, 479, 19, 328, 296, 447, 269, 363, 226, 458, 3, 413, 125, 280, 286, 77, 184, 371, 461, 535, 275, 294, 517, 182, 32, 80, 307, 258, 11, 360, 440, 86, 266, 36, 193, 58, 41, 270, 411, 50, 209, 474, 473, 496, 123, 222, 419, 62, 449, 377, 130, 187, 23, 444, 43, 370, 394, 0, 383, 201, 405, 368, 522, 98, 387, 349, 304, 418, 292, 178, 369, 256, 94, 197, 95, 531, 163, 169, 69, 305, 48, 341, 373, 397, 207, 279, 509, 227, 148, 143, 334, 180, 356, 460, 131, 127, 47, 452, 262, 324, 203, 84, 426, 121, 539, 516, 530, 398, 384, 91, 82, 430, 267, 119, 358, 291, 57, 425, 487, 321, 257, 376, 31, 442, 42, 105, 388, 335, 273, 488, 53, 518, 128, 28, 183, 459, 510, 151, 244, 265, 288, 423, 147, 177, 99, 448, 431, 115, 72, 174, 87, 486, 314, 396, 472, 70, 277, 9, 359, 192, 1064, 1012, 998, 663, 1010, 712, 953, 551, 937, 791, 548, 866, 748, 628, 786, 790, 821, 572, 1052, 1049, 1007, 976, 685, 1070, 568, 693, 964, 982, 589, 799, 1041, 587, 794, 680, 635, 842, 737, 657, 851, 988, 898, 746, 569, 611, 1072, 1027, 930, 562, 672, 829, 640, 575, 800, 766, 967, 1063, 734, 854, 715, 844, 606, 760, 907, 947, 1087, 652, 694, 550, 793, 576, 843, 714, 816, 911, 1037, 879, 675, 1055, 987, 1019, 755, 642, 554, 653, 565, 647, 863, 874, 939, 1051, 658, 951, 968, 965, 709, 600, 940, 557, 660, 868, 1078, 833, 1089, 881, 553, 774, 1025, 776, 870, 736, 1033, 691, 855, 801, 1057, 1068, 960, 690, 607, 974, 798, 753, 761, 1043, 626, 952, 1003, 841, 1004, 700, 807, 999, 629, 838, 696, 639, 869, 991, 956, 641, 659, 605, 583, 689, 704, 586, 969, 941, 886, 759, 897, 782, 638, 932, 559, 958, 1077, 695, 972, 1036, 608, 625, 845, 739, 1001, 727, 1013, 989, 555, 996, 980, 1006, 853, 1085, 877, 950, 808, 552, 812, 803, 584, 697, 579, 1005, 817, 686, 1029, 740, 1061, 674, 1075, 1071, 957, 1031, 596, 708, 901, 1090, 914, 1032, 546, 856, 1038, 757, 718, 741, 977, 1008, 830, 669, 780, 585, 820, 1066, 702, 768, 822, 756, 890, 852, 763, 645, 835, 924, 713, 771, 650, 995, 556, 651, 614, 636, 1067, 919, 926, 1028, 779, 910, 918, 804, 811, 648, 850, 726, 889, 574, 588, 754, 566, 921, 1076, 630, 1044, 1073, 831, 1045, 617, 595, 725, 1069, 973, 662, 839, 622, 594, 1015, 809, 806, 692, 619, 992, 878, 610, 656, 598, 909, 1083, 955, 670, 633, 661, 1074, 834, 920, 848, 664, 558, 703, 883, 963, 673, 592, 781, 983, 681, 787, 899, 948, 707, 1056, 985, 723, 604, 710, 970, 871, 677, 698, 896, 993, 1023, 563, 934, 732, 743, 884, 752, 1080, 933, 891, 1021, 859, 788, 1079, 997, 716, 733, 593, 679, 847, 745, 618, 864, 624, 984, 620, 892, 942, 744, 935, 931, 929, 711, 665, 1040, 699, 946, 1016, 775, 975, 567, 827, 938, 796, 789, 783, 581, 1053, 1088, 875, 655, 925, 717, 784, 549, 836, 994, 867, 666, 701, 684, 671, 1002, 612, 720, 1054, 724, 767, 961, 646, 792, 765, 880]\n",
      "Val Idx:\n",
      "[85, 436, 96, 186, 134, 37, 450, 90, 502, 179, 140, 429, 66, 325, 285, 330, 196, 393, 137, 298, 407, 337, 503, 278, 230, 320, 175, 338, 162, 142, 451, 311, 261, 485, 113, 225, 242, 243, 15, 155, 380, 283, 17, 303, 475, 355, 353, 45, 532, 365, 456, 185, 406, 504, 512, 345, 438, 249, 433, 540, 224, 348, 108, 89, 401, 46, 102, 239, 21, 107, 315, 208, 103, 71, 75, 313, 78, 378, 357, 441, 379, 322, 310, 190, 236, 480, 219, 318, 381, 351, 157, 159, 276, 481, 492, 153, 268, 350, 1, 172, 12, 132, 390, 453, 434, 483, 245, 76, 229, 959, 1030, 887, 922, 876, 573, 917, 905, 865, 1039, 1062, 1058, 1020, 893, 1014, 772, 560, 903, 582, 637, 885, 1017, 826, 916, 1048, 615, 810, 928, 966, 1060, 616, 778, 872, 923, 764, 900, 936, 979, 1050, 735, 1091, 861, 832, 1035, 577, 873, 912, 578, 777, 986, 795, 621, 762, 785, 590, 602, 944, 742, 705, 631, 906, 580, 888, 846, 1082, 682, 773, 815, 1011, 1047, 915, 1046, 849, 1022, 688, 913, 721, 981, 954, 599, 564, 547, 758, 609, 769, 840, 818, 978, 728, 882, 654, 908, 747, 837, 927, 722, 824, 814, 862, 603, 632, 797, 1084, 949, 945, 613, 683, 601, 667]\n",
      "Test Idx:\n",
      "[312, 118, 400, 543, 202, 165, 508, 454, 264, 524, 455, 329, 375, 10, 482, 112, 282, 343, 293, 542, 65, 404, 126, 490, 545, 231, 213, 408, 194, 7, 507, 499, 494, 533, 471, 101, 97, 382, 54, 30, 364, 49, 100, 428, 362, 319, 399, 56, 144, 60, 6, 8, 354, 333, 823, 1024, 749, 813, 649, 1081, 1065, 1042, 561, 802, 904, 719, 770, 1000, 1026, 962, 805, 751, 943, 597, 729, 706, 902, 857, 591, 1018, 731, 571, 894, 738, 643, 1009, 750, 860, 895, 668, 1059, 644, 627, 687, 730, 623, 828, 990, 676, 858, 570, 678, 819, 971, 825, 634, 1086, 1034]\n",
      "[([386, 463, 206, 38, 188, 361, 339, 272, 289, 437, 462, 124, 154, 526, 59, 252, 466, 491, 541, 235, 211, 420, 55, 171, 233, 205, 158, 385, 34, 367, 18, 506, 247, 529, 52, 74, 26, 173, 92, 167, 4, 435, 181, 246, 443, 5, 141, 301, 200, 135, 263, 122, 22, 68, 210, 20, 306, 336, 14, 170, 374, 281, 271, 220, 537, 64, 520, 250, 120, 81, 421, 500, 240, 308, 534, 528, 160, 536, 238, 497, 484, 392, 478, 465, 51, 195, 191, 116, 342, 395, 164, 106, 372, 63, 511, 284, 519, 445, 316, 93, 366, 332, 198, 145, 150, 39, 495, 439, 253, 344, 464, 2, 221, 514, 146, 241, 538, 114, 391, 176, 168, 515, 346, 513, 136, 424, 254, 476, 299, 457, 432, 255, 232, 133, 33, 88, 290, 44, 61, 199, 505, 525, 544, 340, 218, 302, 297, 73, 295, 35, 467, 29, 427, 446, 412, 309, 523, 217, 27, 469, 347, 156, 493, 409, 138, 212, 104, 414, 410, 416, 215, 521, 189, 214, 501, 204, 234, 259, 67, 24, 216, 300, 223, 129, 111, 166, 498, 470, 40, 274, 422, 79, 403, 468, 327, 13, 287, 326, 489, 251, 228, 415, 161, 83, 117, 25, 110, 149, 152, 16, 402, 331, 109, 417, 139, 237, 260, 352, 527, 317, 323, 477, 248, 389, 479, 19, 328, 296, 447, 269, 363, 226, 458, 3, 413, 125, 280, 286, 77, 184, 371, 461, 535, 275, 294, 517, 182, 32, 80, 307, 258, 11, 360, 440, 86, 266, 36, 193, 58, 41, 270, 411, 50, 209, 474, 473, 496, 123, 222, 419, 62, 449, 377, 130, 187, 23, 444, 43, 370, 394, 0, 383, 201, 405, 368, 522, 98, 387, 349, 304, 418, 292, 178, 369, 256, 94, 197, 95, 531, 163, 169, 69, 305, 48, 341, 373, 397, 207, 279, 509, 227, 148, 143, 334, 180, 356, 460, 131, 127, 47, 452, 262, 324, 203, 84, 426, 121, 539, 516, 530, 398, 384, 91, 82, 430, 267, 119, 358, 291, 57, 425, 487, 321, 257, 376, 31, 442, 42, 105, 388, 335, 273, 488, 53, 518, 128, 28, 183, 459, 510, 151, 244, 265, 288, 423, 147, 177, 99, 448, 431, 115, 72, 174, 87, 486, 314, 396, 472, 70, 277, 9, 359, 192, 1064, 1012, 998, 663, 1010, 712, 953, 551, 937, 791, 548, 866, 748, 628, 786, 790, 821, 572, 1052, 1049, 1007, 976, 685, 1070, 568, 693, 964, 982, 589, 799, 1041, 587, 794, 680, 635, 842, 737, 657, 851, 988, 898, 746, 569, 611, 1072, 1027, 930, 562, 672, 829, 640, 575, 800, 766, 967, 1063, 734, 854, 715, 844, 606, 760, 907, 947, 1087, 652, 694, 550, 793, 576, 843, 714, 816, 911, 1037, 879, 675, 1055, 987, 1019, 755, 642, 554, 653, 565, 647, 863, 874, 939, 1051, 658, 951, 968, 965, 709, 600, 940, 557, 660, 868, 1078, 833, 1089, 881, 553, 774, 1025, 776, 870, 736, 1033, 691, 855, 801, 1057, 1068, 960, 690, 607, 974, 798, 753, 761, 1043, 626, 952, 1003, 841, 1004, 700, 807, 999, 629, 838, 696, 639, 869, 991, 956, 641, 659, 605, 583, 689, 704, 586, 969, 941, 886, 759, 897, 782, 638, 932, 559, 958, 1077, 695, 972, 1036, 608, 625, 845, 739, 1001, 727, 1013, 989, 555, 996, 980, 1006, 853, 1085, 877, 950, 808, 552, 812, 803, 584, 697, 579, 1005, 817, 686, 1029, 740, 1061, 674, 1075, 1071, 957, 1031, 596, 708, 901, 1090, 914, 1032, 546, 856, 1038, 757, 718, 741, 977, 1008, 830, 669, 780, 585, 820, 1066, 702, 768, 822, 756, 890, 852, 763, 645, 835, 924, 713, 771, 650, 995, 556, 651, 614, 636, 1067, 919, 926, 1028, 779, 910, 918, 804, 811, 648, 850, 726, 889, 574, 588, 754, 566, 921, 1076, 630, 1044, 1073, 831, 1045, 617, 595, 725, 1069, 973, 662, 839, 622, 594, 1015, 809, 806, 692, 619, 992, 878, 610, 656, 598, 909, 1083, 955, 670, 633, 661, 1074, 834, 920, 848, 664, 558, 703, 883, 963, 673, 592, 781, 983, 681, 787, 899, 948, 707, 1056, 985, 723, 604, 710, 970, 871, 677, 698, 896, 993, 1023, 563, 934, 732, 743, 884, 752, 1080, 933, 891, 1021, 859, 788, 1079, 997, 716, 733, 593, 679, 847, 745, 618, 864, 624, 984, 620, 892, 942, 744, 935, 931, 929, 711, 665, 1040, 699, 946, 1016, 775, 975, 567, 827, 938, 796, 789, 783, 581, 1053, 1088, 875, 655, 925, 717, 784, 549, 836, 994, 867, 666, 701, 684, 671, 1002, 612, 720, 1054, 724, 767, 961, 646, 792, 765, 880], [85, 436, 96, 186, 134, 37, 450, 90, 502, 179, 140, 429, 66, 325, 285, 330, 196, 393, 137, 298, 407, 337, 503, 278, 230, 320, 175, 338, 162, 142, 451, 311, 261, 485, 113, 225, 242, 243, 15, 155, 380, 283, 17, 303, 475, 355, 353, 45, 532, 365, 456, 185, 406, 504, 512, 345, 438, 249, 433, 540, 224, 348, 108, 89, 401, 46, 102, 239, 21, 107, 315, 208, 103, 71, 75, 313, 78, 378, 357, 441, 379, 322, 310, 190, 236, 480, 219, 318, 381, 351, 157, 159, 276, 481, 492, 153, 268, 350, 1, 172, 12, 132, 390, 453, 434, 483, 245, 76, 229, 959, 1030, 887, 922, 876, 573, 917, 905, 865, 1039, 1062, 1058, 1020, 893, 1014, 772, 560, 903, 582, 637, 885, 1017, 826, 916, 1048, 615, 810, 928, 966, 1060, 616, 778, 872, 923, 764, 900, 936, 979, 1050, 735, 1091, 861, 832, 1035, 577, 873, 912, 578, 777, 986, 795, 621, 762, 785, 590, 602, 944, 742, 705, 631, 906, 580, 888, 846, 1082, 682, 773, 815, 1011, 1047, 915, 1046, 849, 1022, 688, 913, 721, 981, 954, 599, 564, 547, 758, 609, 769, 840, 818, 978, 728, 882, 654, 908, 747, 837, 927, 722, 824, 814, 862, 603, 632, 797, 1084, 949, 945, 613, 683, 601, 667], [312, 118, 400, 543, 202, 165, 508, 454, 264, 524, 455, 329, 375, 10, 482, 112, 282, 343, 293, 542, 65, 404, 126, 490, 545, 231, 213, 408, 194, 7, 507, 499, 494, 533, 471, 101, 97, 382, 54, 30, 364, 49, 100, 428, 362, 319, 399, 56, 144, 60, 6, 8, 354, 333, 823, 1024, 749, 813, 649, 1081, 1065, 1042, 561, 802, 904, 719, 770, 1000, 1026, 962, 805, 751, 943, 597, 729, 706, 902, 857, 591, 1018, 731, 571, 894, 738, 643, 1009, 750, 860, 895, 668, 1059, 644, 627, 687, 730, 623, 828, 990, 676, 858, 570, 678, 819, 971, 825, 634, 1086, 1034])]\n",
      "Length of concatenated dataset: 546\n",
      "Length of concatenated dataset: 546\n",
      "Train Idx:\n",
      "[33, 190, 245, 129, 253, 21, 237, 146, 16, 204, 75, 173, 135, 209, 179, 4, 96, 188, 61, 67, 52, 66, 26, 110, 233, 124, 268, 40, 13, 107, 145, 166, 3, 159, 24, 30, 252, 213, 60, 56, 248, 157, 46, 19, 212, 153, 54, 227, 80, 51, 2, 196, 104, 150, 86, 10, 168, 58, 41, 14, 50, 194, 123, 62, 158, 187, 130, 246, 154, 43, 221, 138, 254, 181, 149, 112, 207, 98, 180, 93, 171, 162, 36, 113, 0, 94, 95, 223, 69, 49, 48, 85, 270, 141, 23, 249, 143, 78, 100, 131, 228, 271, 6, 68, 84, 170, 121, 140, 214, 240, 234, 239, 91, 241, 257, 11, 119, 102, 35, 57, 169, 65, 1, 120, 226, 186, 42, 105, 132, 244, 17, 38, 133, 53, 164, 217, 128, 34, 28, 183, 114, 203, 163, 151, 202, 31, 232, 127, 185, 250, 260, 32, 167, 142, 236, 147, 29, 177, 216, 99, 82, 269, 175, 79, 197, 243, 208, 115, 148, 266, 72, 77, 25, 165, 81, 261, 174, 263, 39, 193, 88, 70, 87, 242, 211, 9, 195, 251, 192, 117, 47, 172, 413, 531, 485, 274, 430, 285, 426, 404, 451, 455, 349, 453, 308, 375, 416, 480, 425, 382, 304, 504, 524, 513, 435, 283, 320, 488, 496, 479, 462, 338, 315, 276, 385, 406, 481, 309, 465, 460, 407, 282, 386, 448, 280, 391, 359, 444, 392, 475, 388, 541, 374, 370, 469, 327, 303, 439, 322, 373, 478, 499, 542, 329, 333, 279, 281, 352, 467, 540, 432, 507, 311, 438, 394, 483, 535, 502, 442, 397, 427, 353, 332, 500, 321, 326, 343, 484, 519, 328, 509, 431, 544, 307, 390, 534, 510, 495, 325, 347, 299, 446, 525, 440, 277, 412, 454, 538, 273, 278, 414, 334, 408, 523, 395, 295, 341, 293, 511, 287, 443, 305, 543, 501, 476, 337, 456, 498, 393, 354, 331, 286, 489, 527, 345, 420, 433, 360, 396, 520, 532, 457, 409, 324, 468, 464, 389, 297, 367, 437, 379, 289, 336, 487, 401, 459, 378, 482, 508, 301, 346, 492, 497, 515, 362, 474, 422, 447, 366, 314, 450, 356, 316, 365, 291, 472, 471, 418, 423, 312, 364, 384, 526, 317, 342, 275, 494, 419, 514, 302, 387, 298, 449, 441]\n",
      "Val Idx:\n",
      "[218, 259, 225, 111, 178, 161, 89, 220, 55, 198, 265, 267, 210, 126, 97, 15, 8, 152, 59, 238, 45, 116, 247, 160, 63, 7, 92, 235, 206, 255, 144, 118, 103, 191, 73, 137, 230, 83, 64, 258, 101, 200, 272, 106, 27, 224, 189, 182, 219, 37, 231, 136, 20, 76, 318, 284, 402, 330, 357, 411, 503, 410, 372, 344, 323, 436, 505, 351, 533, 400, 398, 491, 376, 355, 415, 516, 363, 545, 521, 434, 539, 428, 288, 466, 518, 371, 537, 290, 335, 403, 493, 340, 350, 522, 528, 405, 429, 473, 296, 445, 377, 517, 424, 348, 477, 319, 463, 529]\n",
      "Test Idx:\n",
      "[201, 229, 134, 176, 5, 22, 199, 184, 12, 122, 215, 74, 44, 222, 125, 156, 155, 108, 18, 139, 256, 90, 71, 109, 205, 262, 264, 486, 310, 294, 383, 292, 369, 421, 361, 300, 461, 458, 339, 490, 512, 506, 399, 452, 380, 470, 306, 368, 313, 530, 417, 536, 381, 358]\n",
      "[([33, 190, 245, 129, 253, 21, 237, 146, 16, 204, 75, 173, 135, 209, 179, 4, 96, 188, 61, 67, 52, 66, 26, 110, 233, 124, 268, 40, 13, 107, 145, 166, 3, 159, 24, 30, 252, 213, 60, 56, 248, 157, 46, 19, 212, 153, 54, 227, 80, 51, 2, 196, 104, 150, 86, 10, 168, 58, 41, 14, 50, 194, 123, 62, 158, 187, 130, 246, 154, 43, 221, 138, 254, 181, 149, 112, 207, 98, 180, 93, 171, 162, 36, 113, 0, 94, 95, 223, 69, 49, 48, 85, 270, 141, 23, 249, 143, 78, 100, 131, 228, 271, 6, 68, 84, 170, 121, 140, 214, 240, 234, 239, 91, 241, 257, 11, 119, 102, 35, 57, 169, 65, 1, 120, 226, 186, 42, 105, 132, 244, 17, 38, 133, 53, 164, 217, 128, 34, 28, 183, 114, 203, 163, 151, 202, 31, 232, 127, 185, 250, 260, 32, 167, 142, 236, 147, 29, 177, 216, 99, 82, 269, 175, 79, 197, 243, 208, 115, 148, 266, 72, 77, 25, 165, 81, 261, 174, 263, 39, 193, 88, 70, 87, 242, 211, 9, 195, 251, 192, 117, 47, 172, 413, 531, 485, 274, 430, 285, 426, 404, 451, 455, 349, 453, 308, 375, 416, 480, 425, 382, 304, 504, 524, 513, 435, 283, 320, 488, 496, 479, 462, 338, 315, 276, 385, 406, 481, 309, 465, 460, 407, 282, 386, 448, 280, 391, 359, 444, 392, 475, 388, 541, 374, 370, 469, 327, 303, 439, 322, 373, 478, 499, 542, 329, 333, 279, 281, 352, 467, 540, 432, 507, 311, 438, 394, 483, 535, 502, 442, 397, 427, 353, 332, 500, 321, 326, 343, 484, 519, 328, 509, 431, 544, 307, 390, 534, 510, 495, 325, 347, 299, 446, 525, 440, 277, 412, 454, 538, 273, 278, 414, 334, 408, 523, 395, 295, 341, 293, 511, 287, 443, 305, 543, 501, 476, 337, 456, 498, 393, 354, 331, 286, 489, 527, 345, 420, 433, 360, 396, 520, 532, 457, 409, 324, 468, 464, 389, 297, 367, 437, 379, 289, 336, 487, 401, 459, 378, 482, 508, 301, 346, 492, 497, 515, 362, 474, 422, 447, 366, 314, 450, 356, 316, 365, 291, 472, 471, 418, 423, 312, 364, 384, 526, 317, 342, 275, 494, 419, 514, 302, 387, 298, 449, 441], [218, 259, 225, 111, 178, 161, 89, 220, 55, 198, 265, 267, 210, 126, 97, 15, 8, 152, 59, 238, 45, 116, 247, 160, 63, 7, 92, 235, 206, 255, 144, 118, 103, 191, 73, 137, 230, 83, 64, 258, 101, 200, 272, 106, 27, 224, 189, 182, 219, 37, 231, 136, 20, 76, 318, 284, 402, 330, 357, 411, 503, 410, 372, 344, 323, 436, 505, 351, 533, 400, 398, 491, 376, 355, 415, 516, 363, 545, 521, 434, 539, 428, 288, 466, 518, 371, 537, 290, 335, 403, 493, 340, 350, 522, 528, 405, 429, 473, 296, 445, 377, 517, 424, 348, 477, 319, 463, 529], [201, 229, 134, 176, 5, 22, 199, 184, 12, 122, 215, 74, 44, 222, 125, 156, 155, 108, 18, 139, 256, 90, 71, 109, 205, 262, 264, 486, 310, 294, 383, 292, 369, 421, 361, 300, 461, 458, 339, 490, 512, 506, 399, 452, 380, 470, 306, 368, 313, 530, 417, 536, 381, 358])]\n",
      "SPLITS:\n",
      "- folds:\n",
      "1\n",
      "- splits per fold:\n",
      "3\n",
      "(F) Train validation splitted: 384 108\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 384/384 [01:29<00:00,  4.31it/s, batch_loss=0.469, loss=0.713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████| 108/108 [00:08<00:00, 12.38it/s, batch_loss=0.46, loss=0.703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/15: Validation average loss: 0.7033604642859211 + AUC SCORE = 0.5171467764060356 + AUC SCORE THRESH 0.6122448979591836 = 0.5740740740740741\n",
      "Saving the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 384/384 [01:28<00:00,  4.34it/s, batch_loss=0.554, loss=0.709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 108/108 [00:08<00:00, 12.40it/s, batch_loss=0.688, loss=0.725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2/15: Validation average loss: 0.7250772489717713 + AUC SCORE = 0.5006858710562414 + AUC SCORE THRESH 0.7755102040816326 = 0.5833333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 384/384 [01:28<00:00,  4.33it/s, batch_loss=0.873, loss=0.698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 108/108 [00:08<00:00, 12.32it/s, batch_loss=0.845, loss=0.715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3/15: Validation average loss: 0.7149383021449601 + AUC SCORE = 0.5006858710562414 + AUC SCORE THRESH 0.6938775510204082 = 0.5740740740740741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████| 384/384 [01:28<00:00,  4.32it/s, batch_loss=0.53, loss=0.683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 108/108 [00:08<00:00, 12.28it/s, batch_loss=0.851, loss=0.699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 4/15: Validation average loss: 0.6989641236486258 + AUC SCORE = 0.46810699588477367 + AUC SCORE THRESH 0.5714285714285714 = 0.5277777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 384/384 [01:29<00:00,  4.31it/s, batch_loss=0.593, loss=0.685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 108/108 [00:08<00:00, 12.31it/s, batch_loss=0.692, loss=0.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 5/15: Validation average loss: 0.6997826565746907 + AUC SCORE = 0.4619341563786008 + AUC SCORE THRESH 0.6122448979591836 = 0.5740740740740741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 384/384 [01:29<00:00,  4.31it/s, batch_loss=0.634, loss=0.671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 108/108 [00:08<00:00, 12.31it/s, batch_loss=0.629, loss=0.716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 6/15: Validation average loss: 0.715678444063222 + AUC SCORE = 0.448559670781893 + AUC SCORE THRESH 0.5306122448979591 = 0.5462962962962963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 384/384 [01:29<00:00,  4.31it/s, batch_loss=0.651, loss=0.665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 108/108 [00:08<00:00, 12.32it/s, batch_loss=0.584, loss=0.782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 7/15: Validation average loss: 0.7819199589667497 + AUC SCORE = 0.44890260631001366 + AUC SCORE THRESH 0.32653061224489793 = 0.5092592592592593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 384/384 [01:29<00:00,  4.31it/s, batch_loss=0.917, loss=0.657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████| 108/108 [00:08<00:00, 12.32it/s, batch_loss=3.38, loss=0.968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 8/15: Validation average loss: 0.9679068962663964 + AUC SCORE = 0.5216049382716049 + AUC SCORE THRESH 0.9183673469387754 = 0.5740740740740741\n",
      "Saving the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 384/384 [01:29<00:00,  4.31it/s, batch_loss=0.765, loss=0.702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████| 108/108 [00:08<00:00, 12.27it/s, batch_loss=0.767, loss=0.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 9/15: Validation average loss: 0.8396988351898337 + AUC SCORE = 0.49965706447187924 + AUC SCORE THRESH 0.8571428571428571 = 0.5740740740740741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 384/384 [01:29<00:00,  4.31it/s, batch_loss=0.793, loss=0.685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 108/108 [00:08<00:00, 12.30it/s, batch_loss=0.878, loss=0.951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10/15: Validation average loss: 0.9513975078071881 + AUC SCORE = 0.48079561042524005 + AUC SCORE THRESH 0.9387755102040816 = 0.5833333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 384/384 [01:29<00:00,  4.31it/s, batch_loss=0.504, loss=0.686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 108/108 [00:08<00:00, 12.29it/s, batch_loss=0.824, loss=0.869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 11/15: Validation average loss: 0.8690216263273248 + AUC SCORE = 0.4783950617283951 + AUC SCORE THRESH 0.8979591836734693 = 0.5833333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 384/384 [01:29<00:00,  4.31it/s, batch_loss=0.789, loss=0.668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 108/108 [00:08<00:00, 12.28it/s, batch_loss=0.849, loss=0.714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 12/15: Validation average loss: 0.7139648172866415 + AUC SCORE = 0.48731138545953356 + AUC SCORE THRESH 0.6122448979591836 = 0.5833333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████| 384/384 [01:29<00:00,  4.31it/s, batch_loss=1.06, loss=0.649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:08<00:00, 12.29it/s, batch_loss=0.00338, loss=0.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 13/15: Validation average loss: 0.9499226041220094 + AUC SCORE = 0.4591906721536351 + AUC SCORE THRESH 0.9387755102040816 = 0.5833333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 384/384 [01:29<00:00,  4.31it/s, batch_loss=0.515, loss=0.636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 108/108 [00:08<00:00, 12.28it/s, batch_loss=0.778, loss=0.848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 14/15: Validation average loss: 0.8481016096020876 + AUC SCORE = 0.46570644718792864 + AUC SCORE THRESH 0.8571428571428571 = 0.5833333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 384/384 [01:29<00:00,  4.31it/s, batch_loss=0.344, loss=0.622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 5.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 108/108 [00:08<00:00, 12.25it/s, batch_loss=0.416, loss=0.917]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 15/15: Validation average loss: 0.9166451604278 + AUC SCORE = 0.4646776406035665 + AUC SCORE THRESH 0.9183673469387754 = 0.5740740740740741\n",
      "0.5216049382716049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_folded_models(fold, mri_type, \"resnet10\", batch_size, epochs, train_origins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y, y_pred, y_prob, name):\n",
    "    auc = roc_auc_score(y, y_prob)\n",
    "    acc = [1 if yy == out else 0 for (yy,out) in zip(y,y_pred)].count(1)/len(y_pred)\n",
    "    total_0_count = y.count(0)\n",
    "    total_1_count = y.count(1)\n",
    "    total_1_pred_count = list(y_pred).count(1)\n",
    "    true_0 = [1 if yy == out and yy == 0 else 0 for (yy,out) in zip(y,y_pred)].count(1)\n",
    "    true_1 = [1 if yy == out and yy == 1 else 0 for (yy,out) in zip(y,y_pred)].count(1)\n",
    "    spec = true_0/total_0_count\n",
    "    sens = true_1/total_1_count\n",
    "    if total_1_pred_count != 0:\n",
    "        prec = true_1/total_1_pred_count\n",
    "    else:\n",
    "        prec = 0\n",
    "    print(f\"Prediction AUC: {auc:.4f}\")\n",
    "    print(f\"Prediction Accuracy: {acc:.4f}\")\n",
    "    print(f\"Prediction Specificity: {spec:.4f}\")\n",
    "    print(f\"Prediction Sensitivity: {sens:.4f}\")\n",
    "    print(f\"Prediction Precision: {prec:.4f}\")\n",
    "    return pd.DataFrame({\"model\": [name], \"AUC\": [auc], \"acc\": [acc], \"spec\": [spec], \"sens\": [sens], \"prec\": [prec]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(filepath, test_dl):\n",
    "    modelname = os.path.basename(filepath)\n",
    "    model = monai.networks.nets.resnet10(spatial_dims=3, n_input_channels=1, n_classes=1)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(f\"../../RSNA-BTC-Datasets/rsnaresnet10_output_weights/{filepath}\"))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred = [] #y_pred\n",
    "        preds = [] #y_prob\n",
    "        true_labels = [] #y\n",
    "        case_ids = []\n",
    "        epoch_iterator_test = tqdm(test_dl)\n",
    "        for step, batch in enumerate(epoch_iterator_test):\n",
    "            model.eval()\n",
    "            (img_ids, imgs, labels) = batch\n",
    "            images, targets = imgs[0].to(device), labels.to(device)\n",
    "            #images, targets = batch[\"image\"].to(device), batch[\"target\"].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            targets = targets  # .view(-1, 1)\n",
    "\n",
    "            preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "            true_labels.append(targets.cpu().numpy())\n",
    "            predicted = (outputs>0.5).int()\n",
    "            predicted = torch.reshape(predicted, (-1,))\n",
    "\n",
    "            y_pred.extend(predicted.tolist())\n",
    "            #case_ids.append(batch[\"case_id\"])\n",
    "            if img_ids[0][0][0].isnumeric():\n",
    "                img_ids_fixed = [img_id[:5] for img_id in img_ids[0]]\n",
    "            else:\n",
    "                img_ids_fixed = [img_id[:-4] for img_id in img_ids[0]]\n",
    "            case_ids.append(img_ids_fixed)\n",
    "    preds = np.vstack(preds).T[0].tolist()\n",
    "    true_labels = np.hstack(true_labels).tolist()\n",
    "    case_ids = np.hstack(case_ids).tolist()\n",
    "    auc_score = roc_auc_score(true_labels, preds)\n",
    "    preddf = get_metrics(true_labels, y_pred, preds, modelname)\n",
    "    return preddf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [00:04<00:00, 12.46it/s]\n",
      "/tmp/ipykernel_2843398/4121842239.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append(preddf, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction AUC: 0.3856\n",
      "Prediction Accuracy: 0.4333\n",
      "Prediction Specificity: 0.7000\n",
      "Prediction Sensitivity: 0.1667\n",
      "Prediction Precision: 0.3571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [00:04<00:00, 12.52it/s]\n",
      "/tmp/ipykernel_2843398/4121842239.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append(preddf, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction AUC: 0.5444\n",
      "Prediction Accuracy: 0.5000\n",
      "Prediction Specificity: 1.0000\n",
      "Prediction Sensitivity: 0.0000\n",
      "Prediction Precision: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [00:04<00:00, 12.40it/s]\n",
      "/tmp/ipykernel_2843398/4121842239.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append(preddf, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction AUC: 0.6856\n",
      "Prediction Accuracy: 0.5000\n",
      "Prediction Specificity: 1.0000\n",
      "Prediction Sensitivity: 0.0000\n",
      "Prediction Precision: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [00:04<00:00, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction AUC: 0.3156\n",
      "Prediction Accuracy: 0.3667\n",
      "Prediction Specificity: 0.0000\n",
      "Prediction Sensitivity: 0.7333\n",
      "Prediction Precision: 0.4231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_2843398/4121842239.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append(preddf, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "paths = [\n",
    "    \"resnet10_m_Aug09_17-26-02/3d-resnet10_m_T1wCE_fold0_0.49.pth\", #m\n",
    "    \"resnet10_n_Aug09_17-52-43/3d-resnet10_n_T1wCE_fold0_0.518.pth\", #n\n",
    "    \"resnet10_fn_Aug09_18-39-12/3d-resnet10_fn_T1wCE_fold0_0.577.pth\", #f\n",
    "    \"resnet10_f_Aug09_19-48-33/3d-resnet10_f_T1wCE_fold0_0.522.pth\" #fn\n",
    "]\n",
    "metrics = pd.DataFrame({\"model\": [], \"AUC\": [], \"acc\": [], \"spec\": [], \"sens\": [], \"prec\": []})\n",
    "for filepath in paths:\n",
    "    test_loader = m_test_loader\n",
    "    preddf = test_model(filepath, test_loader)\n",
    "    metrics = metrics.append(preddf, ignore_index=True)\n",
    "    \n",
    "metrics.to_csv(f\"tunisiaai_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 54/54 [00:04<00:00, 12.32it/s]\n",
      "/tmp/ipykernel_2843398/1426414694.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append(preddf, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction AUC: 0.6269\n",
      "Prediction Accuracy: 0.5741\n",
      "Prediction Specificity: 1.0000\n",
      "Prediction Sensitivity: 0.1481\n",
      "Prediction Precision: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 54/54 [00:04<00:00, 12.42it/s]\n",
      "/tmp/ipykernel_2843398/1426414694.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append(preddf, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction AUC: 0.2853\n",
      "Prediction Accuracy: 0.5000\n",
      "Prediction Specificity: 1.0000\n",
      "Prediction Sensitivity: 0.0000\n",
      "Prediction Precision: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 54/54 [00:04<00:00, 12.36it/s]\n",
      "/tmp/ipykernel_2843398/1426414694.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append(preddf, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction AUC: 0.3923\n",
      "Prediction Accuracy: 0.5000\n",
      "Prediction Specificity: 1.0000\n",
      "Prediction Sensitivity: 0.0000\n",
      "Prediction Precision: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 54/54 [00:04<00:00, 12.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction AUC: 0.6283\n",
      "Prediction Accuracy: 0.4815\n",
      "Prediction Specificity: 0.2593\n",
      "Prediction Sensitivity: 0.7037\n",
      "Prediction Precision: 0.4872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_2843398/1426414694.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics = metrics.append(preddf, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "paths = [\n",
    "    \"resnet10_m_Aug09_17-26-02/3d-resnet10_m_T1wCE_fold0_0.49.pth\", #m\n",
    "    \"resnet10_n_Aug09_17-52-43/3d-resnet10_n_T1wCE_fold0_0.518.pth\", #n\n",
    "    \"resnet10_fn_Aug09_18-39-12/3d-resnet10_fn_T1wCE_fold0_0.577.pth\", #f\n",
    "    \"resnet10_f_Aug09_19-48-33/3d-resnet10_f_T1wCE_fold0_0.522.pth\" #fn\n",
    "]\n",
    "metrics = pd.DataFrame({\"model\": [], \"AUC\": [], \"acc\": [], \"spec\": [], \"sens\": [], \"prec\": []})\n",
    "for filepath in paths:\n",
    "    test_loader = n_test_loader\n",
    "    preddf = test_model(filepath, test_loader)\n",
    "    metrics = metrics.append(preddf, ignore_index=True)\n",
    "    \n",
    "metrics.to_csv(f\"tunisiaai_metrics_n.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
